{"meta":{"title":"yanliang's blog","subtitle":"","description":"","author":"yanliang","url":"https://gyl-coder.top","root":"/"},"pages":[{"title":"404 Not Found","date":"2020-12-27T04:47:15.974Z","updated":"2020-12-27T04:47:15.974Z","comments":true,"path":"404.html","permalink":"https://gyl-coder.top/404.html","excerpt":"","text":"404 很抱歉，您访问的页面不存在 可能是输入地址有误或该地址已被删除"},{"title":"关于","date":"2020-12-27T04:47:15.978Z","updated":"2020-12-27T04:47:15.978Z","comments":true,"path":"about/index.html","permalink":"https://gyl-coder.top/about/index.html","excerpt":"","text":"Yanliang 🍂 しゃちく 🍀 个人简介 社畜一枚，坐标 SZ🌏，永远热爱，永远年轻相知🤞 爱好旅行/音乐/电影/阅读✨ 对一切新鲜的事物充满好奇🧐 目标是过简单的生活💪 对自己的要求是每天都要比昨天进步一点点👊 🍭 客观评价 🌌 博客简介 全站 HTTPS Hexo 框架 + Volatis 主题 博客中的部分图片源于网络，侵删 本博客文章采用 CC BY-NC-SA 4.0 协议 协议，转载请注明出处 🔑 技能学习 Java Mysql 算法 Spring Boot Kfaka …. 📆 建站历程 2020-10 优化 valine 评论系统 优化背景 侧边栏添加一言 &amp; 最新评论 2020-09 添加彩色标签 升级博客主题至4.0.0 侧边栏添加一言 &amp; 最新评论 2020-08 github&amp;coding双线部署 添加站点监控页面 添加围住小喵游戏&amp;自定义页脚&amp;RSS 2020-07 …… 2019-09 建站 👨‍✈️ 版权声明 博客内的所有原创内容（包括但不限于文章、图像等）除特别声明外均采用知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议，任何人都可以自由传播，但不得用于商用且必须署名并以相同方式分享。 本站部分内容转载于网络，有出处的已在文中署名作者并附加原文链接，出处已不可寻的皆已标注来源于网络，若您认为本博客有部分内容侵犯了您的版权，请电邮告知，我将认真处理。 赏 .single-reward { position: relative; width: 100%; margin: 30px auto; text-align: center; z-index: 999 } .single-reward .reward-open { position: relative; line-height: 22px; width: 35px; height: 35px; padding: 7px; color: #fff; text-align: center; display: inline-block; border-radius: 100%; background: #d34836; } .single-reward .reward-main { position: absolute; top: 45px; left: -156px; margin: 0; padding: 4px 0 0; width: 355px; background: 0 0; display: none; animation: main .4s } .reward-open:hover .reward-main { display: block } .single-reward .reward-row { margin: 0 auto; padding: 20px 15px 10px; background: #f5f5f5; display: inline-block; border-radius: 4px; } .single-reward .reward-row:before { content: \"\"; width: 0; height: 0; border-left: 13px solid transparent; border-right: 13px solid transparent; border-bottom: 13px solid #f5f5f5; position: absolute; top: -9px; left: -9px; right: 0; margin: 0 auto } .single-reward .reward-row li { list-style-type: none; padding: 0 12px; display: inline-block } .reward-row li img { width: 130px; max-width: 130px; border-radius: 3px; position: relative } .reward-row li::after { margin-top: 10px; display: block; font-size: 13px; color: #121212; } .alipay-code:after { content: \"支付宝\" } .wechat-code:after { content: \"微信\" } .md .single-reward ul li:before{ content: none }"},{"title":"树洞","date":"2020-11-21T13:51:21.000Z","updated":"2020-12-27T04:47:15.978Z","comments":true,"path":"bb/index.html","permalink":"https://gyl-coder.top/bb/index.html","excerpt":"","text":"👉🏼通过📱手机微信发出，主要记录碎片化思考和动态。📑"},{"title":"围住小猫","date":"2020-12-27T04:47:15.978Z","updated":"2020-12-27T04:47:15.978Z","comments":true,"path":"catchTheCat/index.html","permalink":"https://gyl-coder.top/catchTheCat/index.html","excerpt":"","text":"游戏规则：:bell: 点击小圆点，围住小猫。:bell: 你点击一次，小猫走一次。:bell: 直到你把小猫围住（赢），或者小猫走到边界并逃跑（输）。"},{"title":"所有分类","date":"2020-12-27T04:47:15.978Z","updated":"2020-12-27T04:47:15.978Z","comments":true,"path":"categories/index.html","permalink":"https://gyl-coder.top/categories/index.html","excerpt":"","text":""},{"title":"留言板","date":"2020-12-27T04:47:15.978Z","updated":"2020-12-27T04:47:15.978Z","comments":true,"path":"comment/index.html","permalink":"https://gyl-coder.top/comment/index.html","excerpt":"","text":"欢迎前来灌水。。。"},{"title":"","date":"2020-12-27T04:47:15.978Z","updated":"2020-12-27T04:47:15.978Z","comments":true,"path":"friends/index.html","permalink":"https://gyl-coder.top/friends/index.html","excerpt":"","text":"f r i e n d xaoxuu简约风格 果子小酱 LFhacks个人原创 小康博客一个收藏回忆与分享技术的地方！ 黑石博客人生在勤，不索何获。 欢迎和我交换友链~ 各位大佬想交换友链的话可以在下方留言，必须要提供名称、头像和链接哦~请先将本站添加到你滴友链中喔，谢谢~ 友链提交模板： 名称：【请填入名称，9字以内】 头像：【请输入地址，支持png、jpg、gif等常见格式】 地址：【请填入地址，不支持url.cn等短链形式，必须以https://协议标准开头】 网站缩略图：【请填入地址，不支持url.cn等短链形式，必须以https://协议标准开头】 ——————————————————————– 名称：yanliang 头像：https://cdn.jsdelivr.net/gh/gyl-coder/blogImgs/images/touxiang.webp 地址：https://gyl-coder.top 网站缩略图：https://cdn.jsdelivr.net/gh/gyl-coder/blogImgs/images/friends/yanliang.webp"},{"title":"我的相册","date":"2020-12-27T04:47:15.978Z","updated":"2020-12-27T04:47:15.978Z","comments":true,"path":"photos/index.html","permalink":"https://gyl-coder.top/photos/index.html","excerpt":"","text":"🎃我的相册 私のアルバム &nbsp;生活&nbsp;旅行&nbsp;二次元&nbsp;关于光影彼方为谁，无我有问每个人，都要在自己的世界中生活。或许没有轰轰烈烈的传奇故事，但就是那份平淡，才得以让我们疲惫的心得到休息。在这当中，不乏生活中的美。 九月露湿，待君之前有人说要么读书，要么旅行，身体和灵魂，总有一个要在路上；曾梦想仗剑走天涯，看一看世间的繁华。这是一个出走的过程，在短暂的时间里与自己对话，在时光的缝隙中遇见真正的自己，变得淡然与豁达，至于风景这只是附赠品。骚年，愿你走出半生，归来仍是少年。一辈子是场修行。短的是旅途，长的是人生。 Interesting收集一些有意思的图片 摄影是什么?摄影是光与影的结合，它将光线永远停留在了物体上，它将瞬间变为永恒。我不是一个专业的摄影者，纯粹只是个兴趣爱好，甚至谈不上摄影称之为拍照更合适，因为大多数时候我都是用手机拍的照片，弄这个相册于此处主要展示一些我自己拍的还有看到的有意思的一些图片。"},{"title":"树洞","date":"2020-12-27T04:47:15.986Z","updated":"2020-12-27T04:47:15.986Z","comments":false,"path":"shuoshuo/index.html","permalink":"https://gyl-coder.top/shuoshuo/index.html","excerpt":"","text":""},{"title":"所有标签","date":"2020-12-27T04:47:15.986Z","updated":"2020-12-27T04:47:15.986Z","comments":true,"path":"tags/index.html","permalink":"https://gyl-coder.top/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"接口 vs 抽象类的区别？","slug":"java/abstract_interface","date":"2020-09-27T00:00:00.000Z","updated":"2020-12-27T04:47:15.974Z","comments":true,"path":"java/abstract_interface/","link":"","permalink":"https://gyl-coder.top/java/abstract_interface/","excerpt":"在面向对象编程中，抽象类和接口是两个经常被用到的语法概念，是面向对象四大特性，以及很多设计模式、设计思想、设计原则编程实现的基础。 比如：我们可以使用接口来实现面向对象的抽象特性、多态特性和基于接口而非实现的设计原则，使用抽象类来实现面向对象的继承特性和模板设计模式等。","text":"在面向对象编程中，抽象类和接口是两个经常被用到的语法概念，是面向对象四大特性，以及很多设计模式、设计思想、设计原则编程实现的基础。 比如：我们可以使用接口来实现面向对象的抽象特性、多态特性和基于接口而非实现的设计原则，使用抽象类来实现面向对象的继承特性和模板设计模式等。 什么是抽象类和接口？ 区别在哪里？不同的编程语言对接口和抽象类的定义方式可能有些差别，但是差别并不大。本文使用 Java 语言。 抽象类下面我们通过一个例子来看一个典型的抽象类的使用场景。 Logger 是一个记录日志的抽象类，FileLogger 和 MessageQueueLogger 继承Logger，分别实现两种不同的日志记录方式： 记录日志到文件中 记录日志到消息队列中 FileLogger 和 MessageQueuLogger 两个子类复用了父类 Logger 中的name、enabled 以及 minPermittedLevel 属性和 log 方法，但是因为两个子类写日志的方式不同，他们又各自重写了父类中的doLog方法。 父类 import java.util.logging.Level; /** * 抽象父类 * @author yanliang * @date 9/27/2020 5:59 PM */ public abstract class Logger &#123; private String name; private boolean enabled; private Level minPermittedLevel; public Logger(String name, boolean enabled, Level minPermittedLevel) &#123; this.name = name; this.enabled = enabled; this.minPermittedLevel = minPermittedLevel; &#125; public void log(Level level, String message) &#123; boolean loggable = enabled &amp;&amp; (minPermittedLevel.intValue() &lt;= level.intValue()); if(!loggable) return; doLog(level, message); &#125; protected abstract void doLog(Level level, String message); &#125; FileLogger import java.io.FileWriter; import java.io.IOException; import java.io.Writer; import java.util.logging.Level; /** * 抽象类Logger的子类：输出日志到文件中 * @author yanliang * @date 9/28/2020 4:44 PM */ public class FileLogger extends Logger &#123; private Writer fileWriter; public FileLogger(String name, boolean enabled, Level minPermittedLevel, String filePath) throws IOException &#123; super(name, enabled, minPermittedLevel); this.fileWriter = new FileWriter(filePath); &#125; @Override protected void doLog(Level level, String message) &#123; // 格式化level 和 message，输出到日志文件 fileWriter.write(...); &#125; &#125; MessageQueuLogger import java.util.logging.Level; /** * 抽象类Logger的子类：输出日志到消息队列中 * @author yanliang * @date 9/28/2020 6:39 PM */ public class MessageQueueLogger extends Logger &#123; private MessageQueueClient messageQueueClient; public MessageQueueLogger(String name, boolean enabled, Level minPermittedLevel, MessageQueueClient messageQueueClient) &#123; super(name, enabled, minPermittedLevel); this.messageQueueClient = messageQueueClient; &#125; @Override protected void doLog(Level level, String message) &#123; // 格式化level 和 message，输出到消息队列中 messageQueueClient.send(...) &#125; &#125; 通过上面的例子，我们来看下抽象类有哪些特性。 抽象类不能被实例化，只能被继承。（new 一个抽象类，会报编译错误） 抽象类可以包含属性和方法。方法既可以包含实现，也可以不包含实现。不包含实现的方法叫做抽象方法 子类继承抽象类，必须实现抽象类中的所有抽象方法。 接口同样的，下面我们通过一个例子来看下接口的使用场景。 /** * 过滤器接口 * @author yanliang * @date 9/28/2020 6:46 PM */ public interface Filter &#123; void doFilter(RpcRequest req) throws RpcException; &#125; /** * 接口实现类：鉴权过滤器 * @author yanliang * @date 9/28/2020 6:48 PM */ public class AuthencationFilter implements Filter &#123; @Override public void doFilter(RpcRequest req) throws RpcException &#123; // 鉴权逻辑 &#125; &#125; /** * 接口实现类：限流过滤器 * @author yanliang * @date 9/28/2020 6:48 PM */ public class RateLimitFilter implements Filter&#123; @Override public void doFilter(RpcRequest req) throws RpcException &#123; // 限流逻辑 &#125; &#125; /** * 过滤器使用demo * @author yanliang * @date 9/28/2020 6:48 PM */ public class Application &#123; // 过滤器列表 private List&lt;Filter&gt; filters = new ArrayList&lt;&gt;(); filters.add(new AuthencationFilter()); filters.add(new RateLimitFilter()); public void handleRpcRequest(RpcRequest req) &#123; try &#123; for (Filter filter : filters) &#123; filter.doFilter(req); &#125; &#125; catch (RpcException e) &#123; // 处理过滤结果 &#125; // ... &#125; &#125; 上面的案例是一个典型的接口使用场景。通过Java中的 interface 关键字定义了一个Filter 接口，AuthencationFilter 和 RetaLimitFilter 是接口的两个实现类，分别实现了对Rpc请求的鉴权和限流的过滤功能。 下面我们来看下接口的特性： 接口不能包含属性（也就是成员变量） 接口只能生命方法，方法不能包含代码实现 类实现接口时，必须实现接口中生命的所有方法。 综上，从语法上对比，这两者有比较大的区别，比如抽象类中可以定义属性、方法的实现，而接口中不能定义属性，方法也不能包含实现等。 除了语法特性的不同外，从设计的角度，这两者也有较大区别。抽象类本质上就是类，只不过是一种特殊的类，这种类不能被实例化，只能被子类继承。属于is-a的关系。接口则是 has-a 的关系，表示具有某些功能。对于接口，有一个更形象的叫法：协议（contract） 抽象类和接口解决了什么问题？下面我们先来思考一个问题~ 抽象类的存在意义是为了解决代码复用的问题（多个子类可以继承抽象类中定义的属性哈方法，避免在子类中，重复编写相同的代码）。 那么，既然继承本身就能达到代码复用的目的，而且继承也不一定非要求是抽象类。我们不适用抽象类，貌似也可以实现继承和复用。从这个角度上讲，我们好像并不需要抽象类这种语法呀。那抽象类除了解决代码复用的问题，还有其他存在的意义吗？ 这里大家可以先思考一下哈~ 我们还是借用上面Logger的例子，首先对上面的案例实现做一些改造。在改造之后的实现中，Logger不再是抽象类，只是一个普通的父类，删除了Logger中的两个方法，新增了 isLoggable()方法。FileLogger 和 MessageQueueLogger 还是继承Logger父类已达到代码复用的目的。具体代码如下： /** * 父类：非抽象类，就是普通的类 * @author yanliang * @date 9/27/2020 5:59 PM */ public class Logger &#123; private String name; private boolean enabled; private Level minPermittedLevel; public Logger(String name, boolean enabled, Level minPermittedLevel) &#123; this.name = name; this.enabled = enabled; this.minPermittedLevel = minPermittedLevel; &#125; public boolean isLoggable(Level level) &#123; return enabled &amp;&amp; (minPermittedLevel.intValue() &lt;= level.intValue()); &#125; &#125; /** * 抽象类Logger的子类：输出日志到文件中 * @author yanliang * @date 9/28/2020 4:44 PM */ public class FileLogger extends Logger &#123; private Writer fileWriter; public FileLogger(String name, boolean enabled, Level minPermittedLevel, String filePath) throws IOException &#123; super(name, enabled, minPermittedLevel); this.fileWriter = new FileWriter(filePath); &#125; protected void log(Level level, String message) &#123; if (!isLoggable(level)) return ; // 格式化level 和 message，输出到日志文件 fileWriter.write(...); &#125; &#125; package com.yanliang.note.java.abstract_demo; import java.util.logging.Level; /** * 抽象类Logger的子类：输出日志到消息队列中 * @author yanliang * @date 9/28/2020 6:39 PM */ public class MessageQueueLogger extends Logger &#123; private MessageQueueClient messageQueueClient; public MessageQueueLogger(String name, boolean enabled, Level minPermittedLevel, MessageQueueClient messageQueueClient) &#123; super(name, enabled, minPermittedLevel); this.messageQueueClient = messageQueueClient; &#125; protected void log(Level level, String message) &#123; if (!isLoggable(level)) return ; // 格式化level 和 message，输出到消息队列中 messageQueueClient.send(...) &#125; &#125; 以上实现虽然达到了代码复用的目的（复用了父类中的属性），但是却无法使用多态的特性了。 像下面这样编写代码就会出现编译错误，因为Logger中并没有定义log（）方法。 Logger logger = new FileLogger(&quot;access-log&quot;, true, Level.WARN, &quot;/user/log&quot;); logger.log(Level.ERROR, &quot;This is a test log message.&quot;); 如果我们在父类中，定义一个空的log（）方法，让子类重写父类的log（）方法，实现自己的记录日志逻辑。使用这种方式是否能够解决上面的问题呢？ 大家可以先思考下~ 这个思路可以用使用，但是并不优雅，主要有一下几点原因： 在Logger中定义一个空的方法，会影响代码的可读性。如果不熟悉Logger背后的设计思想，又没有代码注释的话，在阅读Logger代码时就会感到疑惑（为什么这里会存在一个空的log（）方法） 当创建一个新的子类继承Logger父类时，有时可能会忘记重新实现log方法。之前是基于抽象类的设计思想，编译器会强制要求子类重写父类的log方法，否则就会报编译错误。 Logger可以被实例化，这也就意味着这个空的log方法有可能会被调用。这就增加了类被误用的风险。当然，这个问题 可以通过设置私有的构造函数的方式来解决，但是不如抽象类优雅。 抽象类更多是为了代码复用，而接口更侧重于解耦。接口是对行为的一种抽象，相当于一组协议或者契约（可类比API接口）。调用者只需要关心抽象的接口，不需要了解具体的实现，具体的实现代码对调用者透明。接口实现了约定和实现相分离，可以降低代码间的耦合，提高代码的可扩展性。 实际上，接口是一个比抽象类应用更加广泛、更加重要的知识点。比如，我们经常提到的 ”基于接口而非实现编程“ ,就是一条几乎天天会用到的，并且能极大的提高代码的灵活性、扩展性的设计思想。 如何模拟抽象类和接口在前面列举的例子中，我们使用Java的接口实现了Filter过滤器。不过，在 C++ 中只提供了抽象类，并没有提供接口，那从代码的角度上说，是不是就无法实现 Filter 的设计思路了呢？ 大家可以先思考下 🤔 ~ 我们先会议下接口的定义：接口中没有成员变量，只有方法声明，没有方法实现，实现接口的类必须实现接口中的所有方法。主要满足以上几点从设计的角度上来说，我们就可以把他叫做接口。 实际上，要满足接口的这些特性并不难。下面我们来看下实现： class Strategy &#123; public: -Strategy(); virtual void algorithm()=0; protected: Strategy(); &#125; 抽象类 Strategy 没有定义任何属性，并且所有的方法都声明为 virtual 类型（等同于Java中的abstract关键字），这样，所有的方法都不能有代码实现，并且所有继承了这个抽象类的子类，都要实现这些方法。从语法特性上看，这个抽象类就相当于一个接口。 处理用抽象类来模拟接口外，我们还可以用普通类来模拟接口。具体的Java实现如下所示： public class MockInterface &#123; protected MockInteface(); public void funcA() &#123; throw new MethodUnSupportedException(); &#125; &#125; 我们知道类中的方法必须包含实现，这个不符合接口的定义。但是，我们可以让类中的方法抛出 MethodUnSupportedException 异常，来模拟不包含实现的接口，并且强迫子类来继承这个父类的时候，都主动实现父类的方法，否则就会在运行时抛出异常。 那又如何避免这个类被实例化呢？ 实际上很简单，我们只需要将这个类的构造函数声明为 protected 访问权限就可以了。 如何决定该用抽象还是接口？上面的讲解可能偏理论，现在我们就从真实项目开发的角度来看下。在代码设计/编程时，什么时候该用接口？什么时候该用抽象类？ 实际上，判断的标准很简单。如果我们需要一种is-a关系，并且是为了解决代码复用的问题，就用抽象类。如果我们需要的是一种has-a关系，并且是为了解决抽象而非代码复用问题，我们就用接口。 从类的继承层次来看，抽象类是一种自下而上的设计思路，先有子类的代码复用，然后再抽象成上层的父类（也就是抽象类）。而接口则相反，它是一种自上而下的设计思路，我们在编程的时候，一般都是先设计接口，再去思考具体实现。 好了，你是否掌握了上面的内容呢。你可以通过一下几个维度来回顾自检一下： 抽象类和接口的语法特性 抽象类和接口存在的意义 抽象类和接口的应用场景有哪些","categories":[{"name":"Java","slug":"Java","permalink":"https://gyl-coder.top/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://gyl-coder.top/tags/Java/"},{"name":"接口","slug":"接口","permalink":"https://gyl-coder.top/tags/%E6%8E%A5%E5%8F%A3/"},{"name":"抽象","slug":"抽象","permalink":"https://gyl-coder.top/tags/%E6%8A%BD%E8%B1%A1/"}],"author":{"name":"yanliang","avatar":"https://cdn.jsdelivr.net/gh/gyl-coder/blogImgs/images/touxiang.webp","url":"https://gyl-coder.top"}},{"title":"Java 集合总结","slug":"java/collection/all","date":"2020-07-06T00:00:00.000Z","updated":"2020-12-27T04:47:15.974Z","comments":true,"path":"java/collection/all/","link":"","permalink":"https://gyl-coder.top/java/collection/all/","excerpt":"集合中用到的数据结构有以下几种： 数组： 最常用的数据结构之一。数组的特点是长度固定，可以用下标索引，并且所有的元素的类型都是一致的。使用时尽量把数组封装在一个类里，防止数据被错误的操作弄乱。 链表： 是一种由多个节点组成的数据结构，并且每个节点包含有数据以及指向下一个节点的引用，在双向链表里，还会有一个指向前一个节点的引用。例如，可以用单向链表和双向链表来实现堆栈和队列，因为链表的两端都是可以进行插入和删除的动作的。当然，也会有在链表的中间频繁插入和删除节点的场景。 树： 是一种由节点组成的数据结构，每个节点都包含数据元素，并且有一个或多个子节点，每个子节点指向一个父节点可以表示层级关系或者数据元素的顺序关系。如果树的每个子节点最多有两个叶子节点，那么这种树被称为二叉树。二叉树是一种非常常用的树形结构， 因为它的这种结构使得节点的插入和删除都非常高效。树的边表示从一个节点到另外一个节点的快捷路径。 堆栈： 只允许对最后插入的元素进行操作（也就是后进先出，Last In First Out – LIFO）。如果你移除了栈顶的元素，那么你可以操作倒数第二个元素，依次类推。这种后进先出的方式是通过仅有的peek(),push()和pop()这几个方法的强制性限制达到的。这种结构在很多场景下都非常实用，例如解析像(4+2)*3这样的数学表达式，把源码中的方法和异常按照他们出现的顺序放到堆栈中，检查你的代码看看小括号和花括号是不是匹配的，等等。 队列： 和堆栈有些相似，不同之处在于在队列里第一个插入的元素也是第一个被删除的元素（即是先进先出）。这种先进先出的结构是通过只提供peek()，offer()和poll()这几个方法来访问数据进行限制来达到的。例如，排队等待公交车，银行或者超市里的等待列队等等，都是可以用队列来表示。","text":"集合中用到的数据结构有以下几种： 数组： 最常用的数据结构之一。数组的特点是长度固定，可以用下标索引，并且所有的元素的类型都是一致的。使用时尽量把数组封装在一个类里，防止数据被错误的操作弄乱。 链表： 是一种由多个节点组成的数据结构，并且每个节点包含有数据以及指向下一个节点的引用，在双向链表里，还会有一个指向前一个节点的引用。例如，可以用单向链表和双向链表来实现堆栈和队列，因为链表的两端都是可以进行插入和删除的动作的。当然，也会有在链表的中间频繁插入和删除节点的场景。 树： 是一种由节点组成的数据结构，每个节点都包含数据元素，并且有一个或多个子节点，每个子节点指向一个父节点可以表示层级关系或者数据元素的顺序关系。如果树的每个子节点最多有两个叶子节点，那么这种树被称为二叉树。二叉树是一种非常常用的树形结构， 因为它的这种结构使得节点的插入和删除都非常高效。树的边表示从一个节点到另外一个节点的快捷路径。 堆栈： 只允许对最后插入的元素进行操作（也就是后进先出，Last In First Out – LIFO）。如果你移除了栈顶的元素，那么你可以操作倒数第二个元素，依次类推。这种后进先出的方式是通过仅有的peek(),push()和pop()这几个方法的强制性限制达到的。这种结构在很多场景下都非常实用，例如解析像(4+2)*3这样的数学表达式，把源码中的方法和异常按照他们出现的顺序放到堆栈中，检查你的代码看看小括号和花括号是不是匹配的，等等。 队列： 和堆栈有些相似，不同之处在于在队列里第一个插入的元素也是第一个被删除的元素（即是先进先出）。这种先进先出的结构是通过只提供peek()，offer()和poll()这几个方法来访问数据进行限制来达到的。例如，排队等待公交车，银行或者超市里的等待列队等等，都是可以用队列来表示。 概览Java 容器主要包括 Collection 和 Map 两种，Collection存储着对象的集合，而Map存储着键值对（两个对象）的映射表。 Collection Set TreeSet（有序，唯一）：基于红黑树实现，支持有序性操作，例如根据一个范围查找元素的操作。但是查找效率不如 HashSet，HashSet 查找的时间复杂度为 O(1)，TreeSet 则为 O(logN)。 HashSet（无序，唯一）：基于Hashmap实现，支持快速查找，但不支持有序性操作。并且失去了元素的插入顺序信息，也就是说使用 Iterator 遍历 HashSet 得到的结果是不确定的。 LinkedHashSet：LinkedHashSet 是 HashSet 的子类，并且其内部是通过 LinkedHashMap 来实现的。具有 HashSet 的查找效率，并且内部使用双向链表维护元素的插入顺序。 List ArrayList：基于动态数组 Object[] 实现，支持随机访问。 Vector：和 ArrayList 类似，但它是线程安全的。 LinkedList：基于双向链表实现 (JDK1.6 之前为循环链表，JDK1.7 取消了循环)，只能顺序访问，但是可以快速地在链表中间插入和删除元素。不仅如此，LinkedList 还可以用作栈、队列和双向队列。 Queue LinkedList：可以用它来实现双向队列。 PriorityQueue：基于堆结构实现，可以用它来实现优先队列。 Map TreeMap：基于红黑树实现。（自平衡的排序二叉树） HashMap：JDK1.8 之前 HashMap 由数组+链表组成的，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在的（“拉链法”解决冲突）。JDK1.8 以后在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为 8）（将链表转换成红黑树前会判断，如果当前数组的长度小于 64，那么会选择先进行数组扩容，而不是转换为红黑树）时，将链表转化为红黑树，以减少搜索时间 HashTable：和 HashMap 类似，但它是线程安全的，这意味着同一时刻多个线程同时写入 HashTable 不会导致数据不一致。它是遗留类，不应该去使用它，而是使用 ConcurrentHashMap 来支持线程安全，ConcurrentHashMap 的效率会更高，因为 ConcurrentHashMap 引入了分段锁。 LinkedHashMap：LinkedHashMap 继承自 HashMap，所以它的底层仍然是基于拉链式散列结构即由数组和链表或红黑树组成。另外，LinkedHashMap 在上面结构的基础上，增加了一条双向链表来维护元素的顺序，顺序为插入顺序或者最近最少使用（LRU）顺序。 迭代器 Iterator Collection 继承了 Iterable 接口，其中的 iterator() 方法能够产生一个 Iterator 对象，通过这个对象就可以迭代遍历 Collection 中的元素。从 JDK 1.5 之后可以使用 foreach 方法来遍历实现了 Iterable 接口的聚合对象。 public interface Iterator&lt;E&gt; &#123; //集合中是否还有元素 boolean hasNext(); //获得集合中的下一个元素 E next(); ...... &#125; Iterator 对象称为迭代器（设计模式的一种），迭代器可以对集合进行遍历，但每一个集合内部的数据结构可能是不尽相同的，所以每一个集合存和取都很可能是不一样的，虽然我们可以人为地在每一个类中定义 hasNext() 和 next() 方法，但这样做会让整个集合体系过于臃肿。于是就有了迭代器。 迭代器是将这样的方法抽取出接口，然后在每个类的内部，定义自己迭代方式，这样做就规定了整个集合体系的遍历方式都是 hasNext()和next()方法，使用者不用管怎么实现的，会用即可。迭代器的定义为：提供一种方法访问一个容器对象中各个元素，而又不需要暴露该对象的内部细节。 迭代器的作用Iterator 主要用于遍历集合，他的特点是安全。因为它可以确保，当前遍历的集合元素被更改时，抛出 ConcurrentModificationException 异常。 如何使用Map&lt;Integer, String&gt; map = new HashMap(); map.put(1, &quot;Java&quot;); map.put(2, &quot;C++&quot;); map.put(3, &quot;PHP&quot;); Iterator&lt;Map.Entry&lt;Integer, String&gt;&gt; iterator = map.entrySet().iterator(); while (iterator.hasNext()) &#123; Map.Entry&lt;Integer, String&gt; entry = iterator.next(); System.out.println(entry.getKey() + entry.getValue()); &#125; ListArrayList查看专题文章 ArrayList扩容查看专题文章 Vector查看专题文章 LinkedList查看专题文章 Arraylist 和 Vector 的区别? ArrayList 是 List 的主要实现类，底层使用 Object[ ]存储，适用于频繁的查找工作，线程不安全 ； Vector 是 List 的古老实现类，底层使用 Object[ ]存储，线程安全的。 Arraylist 与 LinkedList 区别? 是否保证线程安全： ArrayList 和 LinkedList 都是不同步的，也就是不保证线程安全； 底层数据结构： Arraylist 底层使用的是 Object 数组；LinkedList 底层使用的是 双向链表 数据结构（JDK1.6 之前为循环链表，JDK1.7 取消了循环。注意双向链表和双向循环链表的区别，下面有介绍到！） 插入和删除是否受元素位置的影响： ① ArrayList 采用数组存储，所以插入和删除元素的时间复杂度受元素位置的影响。 比如：执行add(E e)方法的时候， ArrayList 会默认在将指定的元素追加到此列表的末尾，这种情况时间复杂度就是 O(1)。但是如果要在指定位置 i 插入和删除元素的话（add(int index, E element)）时间复杂度就为 O(n-i)。因为在进行上述操作的时候集合中第 i 和第 i 个元素之后的(n-i)个元素都要执行向后位/向前移一位的操作。 ② LinkedList 采用链表存储，所以对于add(E e)方法的插入，删除元素时间复杂度不受元素位置的影响，近似 O(1)，如果是要在指定位置i插入和删除元素的话（(add(int index, E element)） 时间复杂度近似为o(n))因为需要先移动到指定位置再插入。 是否支持快速随机访问： LinkedList 不支持高效的随机元素访问，而 ArrayList 支持。快速随机访问就是通过元素的序号快速获取元素对象(对应于get(int index)方法)。 内存空间占用： ArrayList 的空 间浪费主要体现在在 list 列表的结尾会预留一定的容量空间，而 LinkedList 的空间花费则体现在它的每一个元素都需要消耗比 ArrayList 更多的空间（因为要存放直接后继和直接前驱以及数据）。 MapMap与List、Set接口不同，它是由一系列键值对组成的集合，提供了key到Value的映射。同时它也没有继承Collection。在Map中它保证了key与value之间的一一对应关系。也就是说一个key对应一个value，所以它不能存在相同的key值，当然value值可以相同。key可以为空，但是只允许出现一个null。它的主要实现类有HashMap、HashTable、LinkedHashMap、TreeMap。 HashMapHashMap 是 Map 的一个实现类，它代表的是一种键值对的数据存储形式。大多数情况下可以直接定位到它的值，因而具有很快的访问速度，但遍历顺序却是不确定的。HashMap最多只允许一条记录的键为null，允许多条记录的值为null。遇到key为null的时候，调用putForNullKey方法进行处理，而对value没有处理。不保证有序(比如插入的顺序)、也不保证序不随时间变化。jdk 8 之前，其内部是由数组+链表来实现的，而 jdk 8 对于链表长度超过 8 的链表将转储为红黑树。HashMap非线程安全，即任一时刻可以有多个线程同时写HashMap，可能会导致数据的不一致。如果需要满足线程安全，可以用 Collections的synchronizedMap方法使HashMap具有线程安全的能力，或者使用ConcurrentHashMap。hash数组的默认大小是16，而且大小一定是2的指数 查看专题文章 HashTableHashtable和HashMap一样也是散列表，存储元素也是键值对，底层实现是一个Entry数组+链表。Hashtable继承于Dictionary类（Dictionary类声明了操作键值对的接口方法），实现Map接口（定义键值对接口）。HashTable是线程安全的，它的大部分类都被synchronized关键字修饰。key和value都不可为null。hash数组默认大小是11，扩充方式是old*2+1 LinkedHashMapLinkedHashMap继承自HashMap实现了Map接口。基本实现同HashMap一样（底层基于数组+链表+红黑树实现），不同之处在于LinkedHashMap保证了迭代的有序性。其内部维护了一个双向链表，解决了 HashMap不能随时保持遍历顺序和插入顺序一致的问题。除此之外，LinkedHashMap对访问顺序也提供了相关支持。在一些场景下，该特性很有用，比如缓存。在实现上，LinkedHashMap很多方法直接继承自HashMap，仅为维护双向链表覆写了部分方法。默认情况下，LinkedHashMap的迭代顺序是按照插入节点的顺序。也可以通过改变accessOrder参数的值，使得其遍历顺序按照访问顺序输出。 查看专题文章 TreeMapTreeMap集合是基于红黑树（Red-Black tree）的 NavigableMap实现。该集合最重要的特点就是可排序，该映射根据其键的自然顺序进行排序，或者根据创建映射时提供的 Comparator 进行排序，具体取决于使用的构造方法。 SetSet接口继承了Collection接口。Set集合中不能包含重复的元素，每个元素必须是唯一的。你只需将元素加入set中，重复的元素会自动移除。有三种常见的Set实现——HashSet, TreeSet和LinkedHashSet。如果你需要一个访问快速的Set，你应该使用HashSet；当你需要一个排序的Set，你应该使用TreeSet；当你需要记录下插入时的顺序时，你应该使用LinedHashSet。 HashSetHashSet是是基于 HashMap 实现的，底层采用 HashMap 来保存元素,所以它不保证set 的迭代顺序；特别是它不保证该顺序恒久不变。add()、remove()以及contains()等方法都是复杂度为O(1)的方法。由于HashMap中key不可重复，所以HashSet元素不可重复。可以存储null元素，是线程不安全的。 TreeSetTreeSet是一个有序集，基于TreeMap实现，是线程不安全的。TreeSet底层采用TreeMap存储，构造器启动时新建TreeMap。TreeSet存储元素实际为TreeMap存储的键值对为&lt;key,PRESENT&gt;的key;，PRESENT为固定对象：private static final Object PRESENT = new Object().TreeSet支持两种两种排序方式，通过不同构造器调用实现 自然排序： public TreeSet() &#123; // 新建TreeMap，自然排序 this(new TreeMap&lt;E,Object&gt;()); &#125; Comparator排序： public TreeSet(Comparator&lt;? super E&gt; comparator) &#123; // 新建TreeMap，传入自定义比较器comparator this(new TreeMap&lt;&gt;(comparator)); &#125; TreeSet支持正向/反向迭代器遍历和foreach遍历 // 顺序TreeSet：迭代器实现 Iterator iter = set.iterator(); while (iter.hasNext()) &#123; System.out.println(iter.next()); &#125; // 顺序遍历TreeSet：foreach实现 for (Integer i : set) &#123; System.out.println(i); &#125; // 逆序遍历TreeSet：反向迭代器实现 Iterator iter1 = set.descendingIterator(); while (iter1.hasNext()) &#123; System.out.println(iter1.next()); &#125; LinkedHashSetLinkedHashSet介于HashSet和TreeSet之间。哈希表和链接列表实现。基本方法的复杂度为O(1)。LinkedHashSet 是 Set 的一个具体实现，其维护着一个运行于所有条目的双重链接列表。此链接列表定义了迭代顺序，该迭代顺序可为插入顺序或是访问顺序。LinkedHashSet 继承于 HashSet，并且其内部是通过 LinkedHashMap 来实现的。有点类似于我们之前说的LinkedHashMap 其内部是基于 Hashmap 实现的一样。如果我们需要迭代的顺序为插入顺序或者访问顺序，那么 LinkedHashSet 是需要你首先考虑的。LinkedHashSet 底层使用 LinkedHashMap 来保存所有元素，因为继承于 HashSet，所有的方法操作上又与 HashSet 相同，因此 LinkedHashSet 的实现上非常简单，只提供了四个构造方法，并通过传递一个标识参数，调用父类的构造器，底层构造一个 LinkedHashMap 来实现，在相关操作上与父类 HashSet 的操作相同，直接调用父类 HashSet 的方法即可。 package java.util; public class LinkedHashSet&lt;E&gt; extends HashSet&lt;E&gt; implements Set&lt;E&gt;, Cloneable, java.io.Serializable &#123; private static final long serialVersionUID = -2851667679971038690L; /** * 构造一个带有指定初始容量和加载因子的空链表哈希set。 * * 底层会调用父类的构造方法，构造一个有指定初始容量和加载因子的LinkedHashMap实例。 * @param initialCapacity 初始容量。 * @param loadFactor 加载因子。 */ public LinkedHashSet(int initialCapacity, float loadFactor) &#123; super(initialCapacity, loadFactor, true); &#125; /** * 构造一个指定初始容量和默认加载因子0.75的新链表哈希set。 * * 底层会调用父类的构造方法，构造一个指定初始容量和默认加载因子0.75的LinkedHashMap实例。 * @param initialCapacity 初始容量。 */ public LinkedHashSet(int initialCapacity) &#123; super(initialCapacity, .75f, true); &#125; /** * 构造一个默认初始容量16和加载因子0.75的新链表哈希set。 * * 底层会调用父类的构造方法，构造一个默认初始容量16和加载因子0.75的LinkedHashMap实例。 */ public LinkedHashSet() &#123; super(16, .75f, true); &#125; /** * 构造一个与指定collection中的元素相同的新链表哈希set。 * * 底层会调用父类的构造方法，构造一个足以包含指定collection * 中所有元素的初始容量和加载因子为0.75的LinkedHashMap实例。 * @param c 其中的元素将存放在此set中的collection。 */ public LinkedHashSet(Collection&lt;? extends E&gt; c) &#123; super(Math.max(2*c.size(), 11), .75f, true); addAll(c); &#125; @Override public Spliterator&lt;E&gt; spliterator() &#123; return Spliterators.spliterator(this, Spliterator.DISTINCT | Spliterator.ORDERED); &#125; &#125; 通过观察HashMap的源码我们可以发现:Hash Map的前三个构造函数，即访问权限为public类型的构造函数均是以HashMap作为实现。而以LinkedHashMap作为实现的构造函数的访问权限是默认访问权限，即包内访问权限。 即：在java编程中，通过new创建的HashSet对象均是以HashMap作为实现基础。只有在jdk中java.util包内的源代码才可能创建以LinkedHashMap作为实现的HashSet(LinkedHashSet就是通过封装一个以LinkedHashMap为实现的HashSet来实现的)。只有包含三个参数的构造函数才是采用的LinkedHashMap作为实现。 无序性和不可重复性的含义是什么1、什么是无序性？无序性不等于随机性 ，无序性是指存储的数据在底层数组中并非按照数组索引的顺序添加 ，而是根据数据的哈希值决定的。2、什么是不可重复性？不可重复性是指添加的元素按照 equals()判断时 ，返回 false，需要同时重写 equals()方法和 HashCode()方法。 比较 HashSet、LinkedHashSet 和 TreeSet 三者的异同HashSet 是 Set 接口的主要实现类 ，HashSet 的底层是 HashMap，线程不安全的，可以存储 null 值；LinkedHashSet 是 HashSet 的子类，能够按照添加的顺序遍历；TreeSet 底层使用红黑树，能够按照添加元素的顺序进行遍历，排序的方式有自然排序和定制排序。 Set中的元素不能重复，如何实现？在Java的Set体系中，根据实现方式不同主要分为两大类。HashSet和TreeSet。 TreeSet 是二叉树实现的,Treeset中的数据是自动排好序的，不允许放入null值 2、HashSet 是哈希表实现的,HashSet中的数据是无序的，可以放入null，但只能放入一个null，两者中的值都不能重复，就如数据库中唯一约束 在HashSet中，基本的操作都是有HashMap底层实现的，因为HashSet底层是用HashMap存储数据的。当向HashSet中添加元素的时候，首先计算元素的hashcode值，然后通过扰动计算和按位与的方式计算出这个元素的存储位置，如果这个位置位空，就将元素添加进去；如果不为空，则用equals方法比较元素是否相等，相等就不添加，否则找一个空位添加。 TreeSet的底层是TreeMap的keySet()，而TreeMap是基于红黑树实现的，红黑树是一种平衡二叉查找树，它能保证任何一个节点的左右子树的高度差不会超过较矮的那棵的一倍。 TreeMap是按key排序的，元素在插入TreeSet时compareTo()方法要被调用，所以TreeSet中的元素要实现Comparable接口。TreeSet作为一种Set，它不允许出现重复元素。TreeSet是用compareTo()来判断重复元素的。 Set大多都用的Map接口的实现类来实现的（HashSet基于HashMap实现，TreeSet基于TreeMap实现，LinkedHashSet基于LinkedHashMap实现）在HashMap中通过如下实现来保证key值唯一 // 1. 如果key 相等 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; // 2. 修改对应的value if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; 添加元素的时候，如果key(也对应的Set集合的元素)相等，那么则修改value值。而在Set集合中，value值仅仅是一个Object对象罢了(该对象对Set本身而言是无用的)。也就是说：Set集合如果添加的元素相同时，是根本没有插入的(仅修改了一个无用的value值)。从源码(HashMap)中也看出来，==和equals()方法都有使用！ comparable 和 Comparator 的区别 comparable 接口实际上是出自java.lang包 它有一个 compareTo(Object obj)方法用来排序 comparator接口实际上是出自 java.util 包它有一个compare(Object obj1, Object obj2)方法用来排序 一般我们需要对一个集合使用自定义排序时，我们就要重写compareTo()方法或compare()方法，当我们需要对某一个集合实现两种排序方式，比如一个 song 对象中的歌名和歌手名分别采用一种排序方法的话，我们可以重写compareTo()方法和使用自制的Comparator方法或者以两个 Comparator 来实现歌名排序和歌星名排序，第二种代表我们只能使用两个参数版的 Collections.sort(). Comparator 定制排序 ArrayList&lt;Integer&gt; arrayList = new ArrayList&lt;Integer&gt;(); arrayList.add(-1); arrayList.add(3); arrayList.add(3); arrayList.add(-5); arrayList.add(7); arrayList.add(4); arrayList.add(-9); arrayList.add(-7); System.out.println(&quot;原始数组:&quot;); System.out.println(arrayList); // void reverse(List list)：反转 Collections.reverse(arrayList); System.out.println(&quot;Collections.reverse(arrayList):&quot;); System.out.println(arrayList); // void sort(List list),按自然排序的升序排序 Collections.sort(arrayList); System.out.println(&quot;Collections.sort(arrayList):&quot;); System.out.println(arrayList); // 定制排序的用法 Collections.sort(arrayList, new Comparator&lt;Integer&gt;() &#123; @Override public int compare(Integer o1, Integer o2) &#123; return o2.compareTo(o1); &#125; &#125;); System.out.println(&quot;定制排序后：&quot;); System.out.println(arrayList); OutPut : 原始数组: [-1, 3, 3, -5, 7, 4, -9, -7] Collections.reverse(arrayList): [-7, -9, 4, 7, -5, 3, 3, -1] Collections.sort(arrayList): [-9, -7, -5, -1, 3, 3, 4, 7] 定制排序后： [7, 4, 3, 3, -1, -5, -7, -9] 重写 compareTo（）方法实现按年龄排序// person对象没有实现Comparable接口，所以必须实现，这样才不会出错，才可以使treemap中的数据按顺序排列 // 前面一个例子的String类已经默认实现了Comparable接口，详细可以查看String类的API文档，另外其他 // 像Integer类等都已经实现了Comparable接口，所以不需要另外实现了 public class Person implements Comparable&lt;Person&gt; &#123; private String name; private int age; public Person(String name, int age) &#123; super(); this.name = name; this.age = age; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; /** * T重写compareTo方法实现按年龄来排序 */ @Override public int compareTo(Person o) &#123; if (this.age &gt; o.getAge()) &#123; return 1; &#125; if (this.age &lt; o.getAge()) &#123; return -1; &#125; return 0; &#125; &#125; public static void main(String[] args) &#123; TreeMap&lt;Person, String&gt; pdata = new TreeMap&lt;Person, String&gt;(); pdata.put(new Person(&quot;张三&quot;, 30), &quot;zhangsan&quot;); pdata.put(new Person(&quot;李四&quot;, 20), &quot;lisi&quot;); pdata.put(new Person(&quot;王五&quot;, 10), &quot;wangwu&quot;); pdata.put(new Person(&quot;小红&quot;, 5), &quot;xiaohong&quot;); // 得到key的值的同时得到key所对应的值 Set&lt;Person&gt; keys = pdata.keySet(); for (Person key : keys) &#123; System.out.println(key.getAge() + &quot;-&quot; + key.getName()); &#125; &#125; OutPut : 5-小红 10-王五 20-李四 30-张三 其他说说List、Set、Map的区别List 存储元素有序，可重复。Set 存储元素无序，不可重复。Map 使用键值对（kye-value）存储，类似于数学上的函数 y=f(x)，“x”代表 key，”y”代表 value，Key 是无序的、不可重复的，value 是无序的、可重复的，每个键最多映射到一个值。 如何选用集合?主要根据集合的特点来选用，比如我们需要根据键值获取到元素值时就选用 Map 接口下的集合，需要排序时选择 TreeMap,不需要排序时就选择 HashMap,需要保证线程安全就选用 ConcurrentHashMap。 当我们只需要存放元素值时，就选择实现Collection 接口的集合，需要保证元素唯一时选择实现 Set 接口的集合比如 TreeSet 或 HashSet，不需要就选择实现 List 接口的比如 ArrayList 或 LinkedList，然后再根据实现这些接口的集合的特点来选用。 为什么要使用集合？当我们需要保存一组类型相同的数据的时候，我们应该是用一个容器来保存，这个容器就是数组，但是，使用数组存储对象具有一定的弊端， 因为我们在实际开发中，存储的数据的类型是多种多样的，于是，就出现了“集合”，集合同样也是用来存储多个数据的。 数组的缺点是一旦声明之后，长度就不可变了；同时，声明数组时的数据类型也决定了该数组存储的数据的类型；而且，数组存储的数据是有序的、可重复的，特点单一。 但是集合提高了数据存储的灵活性，Java 集合不仅可以用来存储不同类型不同数量的对象，还可以保存具有映射关系的数据 有哪些集合是线程不安全的？怎么解决呢？我们常用的 Arraylist ,LinkedList,Hashmap,HashSet,TreeSet,TreeMap，PriorityQueue 都不是线程安全的。解决办法很简单，可以使用线程安全的集合来代替。如果你要使用线程安全的集合的话， java.util.concurrent 包中提供了很多并发容器供你使用： ConcurrentHashMap: 可以看作是线程安全的 HashMap CopyOnWriteArrayList:可以看作是线程安全的 ArrayList，在读多写少的场合性能非常好，远远好于 Vector. ConcurrentLinkedQueue:高效的并发队列，使用链表实现。可以看做一个线程安全的 LinkedList，这是一个非阻塞队列。 BlockingQueue: 这是一个接口，JDK 内部通过链表、数组等方式实现了这个接口。表示阻塞队列，非常适合用于作为数据共享的通道。 ConcurrentSkipListMap :跳表的实现。这是一个Map，使用跳表的数据结构进行快速查找。 fail-fastfail-fast 机制是java集合(Collection)中的一种错误机制。当多个线程对同一个集合的内容进行操作时，就可能会产生fail-fast事件。例如：当某一个线程A通过iterator去遍历某集合的过程中，若该集合的内容被其他线程所改变了；那么线程A访问集合时，就会抛出ConcurrentModificationException异常，产生fail-fast事件。快速失败（fail—fast）在用迭代器遍历一个集合对象时，如果遍历过程中对集合对象的内容进行了修改（增加、删除、修改），则会抛出Concurrent Modification Exception。原理：迭代器在遍历时直接访问集合中的内容，并且在遍历过程中使用一个 modCount 变量。集合在被遍历期间如果内容发生变化，就会改变modCount的值。每当迭代器使用hashNext()/next()遍历下一个元素之前，都会检测modCount变量是否为expectedmodCount值，是的话就返回遍历；否则抛出异常，终止遍历。注意：这里异常的抛出条件是检测到 modCount！=expectedmodCount 这个条件。如果集合发生变化时修改modCount值刚好又设置为了expectedmodCount值，则异常不会抛出。因此，不能依赖于这个异常是否抛出而进行并发操作的编程，这个异常只建议用于检测并发修改的bug。场景：java.util包下的集合类都是快速失败的，不能在多线程下发生并发修改（迭代过程中被修改）。安全失败（fail—safe）采用安全失败机制的集合容器，在遍历时不是直接在集合内容上访问的，而是先复制原有集合内容，在拷贝的集合上进行遍历。原理：由于迭代时是对原集合的拷贝进行遍历，所以在遍历过程中对原集合所作的修改并不能被迭代器检测到，所以不会触发Concurrent Modification Exception。缺点：基于拷贝内容的优点是避免了Concurrent Modification Exception，但同样地，迭代器并不能访问到修改后的内容，即：迭代器遍历的是开始遍历那一刻拿到的集合拷贝，在遍历期间原集合发生的修改迭代器是不知道的。场景：java.util.concurrent包下的容器都是安全失败，可以在多线程下并发使用，并发修改。 Vector和ArrayList相同点：这两个类都实现了List接口，他们都是有序的集合（储存有序），底层都用数组实现。可以通过索引来获取某个元素。允许元素重复和出现null值。ArrayList和Vector的迭代器实现都是fail-fast的。不同点：vector是线程同步的，所以它也是线程安全的，而arraylist是线程异步的，是不安全的。如果不考虑到线程的安全因素，一般用arraylist效率比较高。扩容时，arraylist扩容1.5倍，vector扩容2倍（或者扩容指定的大小）ArrayList 和Vector是采用数组方式存储数据，此数组元素数大于实际存储的数据以便增加和插入元素，都允许直接序号索引元素，但是插入数据要设计到数组元素移动等内存操作，所以索引数据快插入数据慢，Vector由于使用了synchronized方法（线程安全）所以性能上比ArrayList要差，LinkedList使用双向链表实现存储，按序号索引数据需要进行向前或向后遍历，但是插入数据时只需要记录本项的前后项即可，所以插入数度较快！ Aarraylist和LinkedlistArrayList是基于数组实现的，LinkedList基于双向链表实现的。ArrayList它支持以下标位置进行索引出对应的元素(随机访问)，而LinkedList则需要遍历整个链表来获取对应的元素。因此一般来说ArrayList的访问速度是要比LinkedList要快的ArrayList由于是数组，对于删除和修改而言消耗是比较大(复制和移动数组实现)，LinkedList是双向链表删除和修改只需要修改对应的指针即可，消耗是很小的。因此一般来说LinkedList的增删速度是要比ArrayList要快的LinkedList比ArrayList消耗更多的内存，因为LinkedList中的每个节点存储了前后节点的引用。对于增加/删除元素操作如果增删都是在末尾来操作（每次调用的都是remove()和add()），此时ArrayList就不需要移动和复制数组来进行操作了。如果数据量有百万级的时，速度是会比LinkedList要快的。如果删除操作的位置是在中间。由于LinkedList的消耗主要是在遍历上，ArrayList的消耗主要是在移动和复制上(底层调用的是arraycopy()方法，是native方法)。LinkedList的遍历速度是要慢于ArrayList的复制移动速度的如果数据量有百万级的时，还是ArrayList要快。 哪些集合类提供对元素的随机访问？ArrayList、HashMap、TreeMap和HashTable类提供对元素的随机访问。 Enumeration和Iterator接口的区别Enumeration的速度是Iterator的两倍，也使用更少的内存。Enumeration是非常基础的，也满足了基础的需要。但是，与Enumeration相比，Iterator更加安全，因为当一个集合正在被遍历的时候，它会阻止其它线程去修改集合。Iterator的方法名比Enumeration更科学Iterator有fail-fast机制，比Enumeration更安全Iterator能够删除元素，Enumeration并不能删除元素 Iterater和ListIterator之间有什么区别？我们可以使用Iterator来遍历Set和List集合，而ListIterator只能遍历List。Iterator只可以向前遍历，而LIstIterator可以双向遍历。ListIterator从Iterator接口继承，然后添加了一些额外的功能，比如添加一个元素、替换一个元素、获取前面或后面元素的索引位置。 Java中HashMap的key值要是为类对象则该类需要满足什么条件？需要同时重写该类的hashCode()方法和它的equals()方法。从源码可以得知，在插入元素的时候是先算出该对象的hashCode。如果hashcode相等话的。那么表明该对象是存储在同一个位置上的。如果调用equals()方法，两个key相同，则替换元素如果调用equals()方法，两个key不相同，则说明该hashCode仅仅是碰巧相同，此时是散列冲突，将新增的元素放在桶子上重写了equals()方法，就要重写hashCode()的方法。因为equals()认定了这两个对象相同，而同一个对象调用hashCode()方法时，是应该返回相同的值的！ HashSet与HashMapHashSet 实现了 Set 接口，它不允许集合中有重复的值，当我们提到 HashSet 时，第一件事情就是在将对象存储在 HashSet 之前，要先确保对象重写 equals()和 hashCode()方法，这样才能比较对象的值是否相等，以确保set中没有储存相等的对象。如果我们没有重写这两个方法，将会使用这个方法的默认实现。public boolean add(Object o)方法用来在 Set 中添加元素，当元素值重复时则会立即返回 false，如果成功添加的话会返回 true。HashMap 实现了 Map 接口，Map 接口对键值对进行映射。Map 中不允许重复的键。Map 接口有两个基本的实现，HashMap 和 TreeMap。TreeMap 保存了对象的排列次序，而 HashMap 则不能。HashMap 允许键和值为 null。HashMap 是非 synchronized 的，但 collection 框架提供方法能保证 HashMap synchronized，这样多个线程同时访问 HashMap 时，能保证只有一个线程更改 Map。public Object put(Object Key,Object value)方法用来将元素添加到 map 中。 HashMap HashSet HashMap实现了Map接口 HashSet实现了Set接口 HashMap储存键值对 HashSet仅仅存储对象 使用put()方法将元素放入map中 使用add()方法将元素放入set中 HashMap中使用键对象来计算hashcode值 HashSet使用成员对象来计算hashcode值，对于两个对象来说hashcode可能相同，所以equals()方法用来判断对象的相等性，如果两个对象不同的话，那么返回false hashtable与hashmap相同点：储存结构和实现基本相同，都是是实现的Map接口不同点：HashTable是同步的，HashMap是非同步的，需要同步的时候可以ConcurrentHashMap方法HashMap允许为null，HashTable不允许为null继承不同，HashMap继承的是AbstractMap，HashTable继承的是DictionaryHashMap提供对key的Set进行遍历，因此它是fail-fast的，但HashTable提供对key的Enumeration进行遍历，它不支持fail-fast。HashTable是一个遗留类，如果需要保证线程安全推荐使用CocurrentHashMap HashMap与TreeMapHashMap通过hashcode对其内容进行快速查找，而TreeMap中所有的元素都保持着某种固定的顺序，如果你需要得到一个有序的结果你就应该使用TreeMap（HashMap中元素的排列顺序是不固定的）。HashMap中元素的排列顺序是不固定的）。在Map 中插入、删除和定位元素，HashMap 是最好的选择。但如果您要按自然顺序或自定义顺序遍历键，那么TreeMap会更好。使用HashMap要求添加的键类明确定义了hashCode()和 equals()的实现。 这个TreeMap没有调优选项，因为该树总处于平衡状态。 集合框架中的泛型有什么优点？Java1.5引入了泛型，所有的集合接口和实现都大量地使用它。泛型允许我们为集合提供一个可以容纳的对象类型，因此，如果你添加其它类型的任何元素，它会在编译时报错。这避免了在运行时出现ClassCastException，因为你将会在编译时得到报错信息。泛型也使得代码整洁，我们不需要使用显式转换和instanceOf操作符。它也给运行时带来好处，因为不会产生类型检查的字节码指令。 comparable 和 comparator的不同之处？comparable接口实际上是出自java.lang包它有一个 compareTo(Object obj)方法来将objects排序comparator接口实际上是出自 java.util 包它有一个compare(Object obj1, Object obj2)方法来将objects排序 如何保证一个集合线程安全？Vector, Hashtable, Properties 和 Stack 都是同步的类，所以它们都线程安全的，可以被使用在多线程环境中使用Collections.synchronizedList(list)) 方法，可以保证list类是线程安全的使用java.util.Collections.synchronizedSet()方法可以保证set类是线程安全的。 TreeMap和TreeSet在排序时如何比较元素？Collections工具类中的sort()方法如何比较元素？TreeSet要求存放的对象所属的类必须实现Comparable接口，该接口提供了比较元素的compareTo()方法，当插入元素时会回调该方法比较元素的大小。TreeMap要求存放的键值对映射的键必须实现Comparable接口从而根据键对元素进行排序。Collections工具类的sort方法有两种重载的形式，第一种要求传入的待排序容器中存放的对象比较实现Comparable接口以实现元素的比较；第二种不强制性的要求容器中的元素必须可比较，但是要求传入第二个参数，参数是Comparator接口的子类型（需要重写compare方法实现元素的比较），相当于一个临时定义的排序规则，其实就是通过接口注入比较元素大小的算法，也是对回调模式的应用（Java中对函数式编程的支持）。 什么是Java优先级队列？Java PriorityQueue是一个数据结构，它是Java集合框架的一部分。 它是一个队列的实现，其中元素的顺序将根据每个元素的优先级来决定。 实例化PriorityQueue时，可以在构造函数中提供比较器。 该比较器将决定PriorityQueue集合实例中元素的排序顺序。 Java hashCode（）和equals（）方法。equals（）方法用于确定两个Java对象的相等性。 当我们有一个自定义类时，我们需要重写equals（）方法并提供一个实现，以便它可以用来找到它的两个实例之间的相等性。 通过Java规范，equals（）和hashCode（）之间有一个契约。 它说，“如果两个对象相等，即obj1.equals（obj2）为true，那么obj1.hashCode（）和obj2.hashCode（）必须返回相同的整数”无论何时我们选择重写equals（），我们都必须重写hashCode（）方法。 hashCode（）用于计算位置存储区和key。 Enumeration和Iterator区别函数接口不同 Enumeration只有2个函数接口。通过Enumeration，我们只能读取集合的数据，而不能对数据进行修改。 Iterator只有3个函数接口。Iterator除了能读取集合的数据之外，也能数据进行删除操作。 Iterator支持fail-fast机制，而Enumeration不支持。 Enumeration 是JDK 1.0添加的接口。使用到它的函数包括Vector、Hashtable等类，这些类都是JDK 1.0中加入的，Enumeration存在的目的就是为它们提供遍历接口。Enumeration本身并没有支持同步，而在Vector、Hashtable实现Enumeration时，添加了同步。 而Iterator 是JDK 1.2才添加的接口，它也是为了HashMap、ArrayList等集合提供遍历接口。Iterator是支持fail-fast机制的：当多个线程对同一个集合的内容进行操作时，就可能会产生fail-fast事件。 注意：Enumeration迭代器只能遍历Vector、Hashtable这种古老的集合，因此通常不要使用它，除非在某些极端情况下，不得不使用Enumeration，否则都应该选择Iterator迭代器。 HashMap 和 ConcurrentHashMap 的区别？ConcurrentHashMap和HashMap的实现方式不一样，虽然都是使用桶数组实现的，但是还是有区别，ConcurrentHashMap对桶数组进行了分段，而HashMap并没有。 ConcurrentHashMap在每一个分段上都用锁进行了保护。HashMap没有锁机制。所以，前者线程安全的，后者不是线程安全的。 HashMap中hash方法的原理https://hollischuang.github.io/toBeTopJavaer/#/basics/java-basic/hash-in-hashmap 为什么HashMap的默认容量设置成16https://hollischuang.github.io/toBeTopJavaer/#/basics/java-basic/hashmap-default-capacity 为什么HashMap的默认负载因子设置成0.75https://hollischuang.github.io/toBeTopJavaer/#/basics/java-basic/hashmap-default-loadfactor Java 8中stream相关用法https://hollischuang.github.io/toBeTopJavaer/#/basics/java-basic/stream Apache集合处理工具类的使用https://hollischuang.github.io/toBeTopJavaer/#/basics/java-basic/apache-collections","categories":[{"name":"Java","slug":"Java","permalink":"https://gyl-coder.top/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://gyl-coder.top/tags/Java/"},{"name":"集合","slug":"集合","permalink":"https://gyl-coder.top/tags/%E9%9B%86%E5%90%88/"}],"author":{"name":"yanliang","avatar":"https://cdn.jsdelivr.net/gh/gyl-coder/blogImgs/images/touxiang.webp","url":"https://gyl-coder.top"}},{"title":"LinkedHashMap 底层实现原理分析","slug":"java/collection/LinkedHashMap","date":"2020-07-05T00:00:00.000Z","updated":"2020-12-27T04:47:15.974Z","comments":true,"path":"java/collection/LinkedHashMap/","link":"","permalink":"https://gyl-coder.top/java/collection/LinkedHashMap/","excerpt":"LinkedHashMap继承自HashMap实现了Map接口。基本实现同HashMap一样，不同之处在于LinkedHashMap保证了迭代的有序性。其内部维护了一个双向链表，解决了 HashMap不能随时保持遍历顺序和插入顺序一致的问题。除此之外，LinkedHashMap对访问顺序也提供了相关支持。在一些场景下，该特性很有用，比如缓存。 在实现上，LinkedHashMap很多方法直接继承自HashMap，仅为维护双向链表覆写了部分方法。所以，要看懂 LinkedHashMap 的源码，需要先看懂 HashMap 的源码。 默认情况下，LinkedHashMap的迭代顺序是按照插入节点的顺序。也可以通过改变accessOrder参数的值，使得其遍历顺序按照访问顺序输出。 这里我们只讨论LinkedHashMap和HashMap的不同之处，LinkedHashMap的其他操作和特性具体请参考HashMap 我们先来看下两者的区别：","text":"LinkedHashMap继承自HashMap实现了Map接口。基本实现同HashMap一样，不同之处在于LinkedHashMap保证了迭代的有序性。其内部维护了一个双向链表，解决了 HashMap不能随时保持遍历顺序和插入顺序一致的问题。除此之外，LinkedHashMap对访问顺序也提供了相关支持。在一些场景下，该特性很有用，比如缓存。 在实现上，LinkedHashMap很多方法直接继承自HashMap，仅为维护双向链表覆写了部分方法。所以，要看懂 LinkedHashMap 的源码，需要先看懂 HashMap 的源码。 默认情况下，LinkedHashMap的迭代顺序是按照插入节点的顺序。也可以通过改变accessOrder参数的值，使得其遍历顺序按照访问顺序输出。 这里我们只讨论LinkedHashMap和HashMap的不同之处，LinkedHashMap的其他操作和特性具体请参考HashMap 我们先来看下两者的区别： import java.util.HashMap; import java.util.Iterator; import java.util.LinkedHashMap; import java.util.Map; public class Test04 &#123; public static void main(String[] args) &#123; Map&lt;String, String&gt; map = new LinkedHashMap&lt;String, String&gt;(); map.put(&quot;ahdjkf&quot;, &quot;1&quot;); map.put(&quot;ifjdj&quot;, &quot;2&quot;); map.put(&quot;giafdja&quot;, &quot;3&quot;); map.put(&quot;agad&quot;, &quot;4&quot;); map.put(&quot;ahdjkge&quot;, &quot;5&quot;); map.put(&quot;iegnj&quot;, &quot;6&quot;); System.out.println(&quot;LinkedHashMap的迭代顺序(accessOrder=false)：&quot;); Iterator iterator = map.entrySet().iterator(); while (iterator.hasNext()) &#123; Map.Entry entry = (Map.Entry) iterator.next(); System.out.println(entry.getKey() + &quot;=&quot; + entry.getValue()); &#125; Map&lt;String, String&gt; map1 = new LinkedHashMap&lt;String, String&gt;(16,0.75f,true); map1.put(&quot;ahdjkf&quot;, &quot;1&quot;); map1.put(&quot;ifjdj&quot;, &quot;2&quot;); map1.put(&quot;giafdja&quot;, &quot;3&quot;); map1.put(&quot;agad&quot;, &quot;4&quot;); map1.put(&quot;ahdjkge&quot;, &quot;5&quot;); map1.put(&quot;iegnj&quot;, &quot;6&quot;); map1.get(&quot;ahdjkf&quot;); map1.get(&quot;ifjdj&quot;); System.out.println(&quot;LinkedHashMap的迭代顺序(accessOrder=true)：&quot;); Iterator iterator1 = map1.entrySet().iterator(); while (iterator1.hasNext()) &#123; Map.Entry entry = (Map.Entry) iterator1.next(); System.out.println(entry.getKey() + &quot;=&quot; + entry.getValue()); &#125; Map&lt;String, String&gt; map2 = new HashMap&lt;&gt;(); map2.put(&quot;ahdjkf&quot;, &quot;1&quot;); map2.put(&quot;ifjdj&quot;, &quot;2&quot;); map2.put(&quot;giafdja&quot;, &quot;3&quot;); map2.put(&quot;agad&quot;, &quot;4&quot;); map2.put(&quot;ahdjkge&quot;, &quot;5&quot;); map2.put(&quot;iegnj&quot;, &quot;6&quot;); System.out.println(&quot;HashMap的迭代顺序：&quot;); Iterator iterator2 = map2.entrySet().iterator(); while (iterator2.hasNext()) &#123; Map.Entry aMap = (Map.Entry) iterator2.next(); System.out.println(aMap.getKey() + &quot;=&quot; + aMap.getValue()); &#125; &#125; &#125; Output： LinkedHashMap的迭代顺序(accessOrder=false)： ahdjkf=1 ifjdj=2 giafdja=3 agad=4 ahdjkge=5 iegnj=6 LinkedHashMap的迭代顺序(accessOrder=true)： giafdja=3 agad=4 ahdjkge=5 iegnj=6 ahdjkf=1 ifjdj=2 HashMap的迭代顺序： iegnj=6 giafdja=3 ifjdj=2 agad=4 ahdjkf=1 ahdjkge=5 可以看到 LinkedHashMap在每次插入数据，访问、修改数据时都会调整链表的节点顺序。以决定迭代时输出的顺序。 下面我们来看LinkedHashMap具体是怎么实现的： LinkedHashMap继承了HashMap，内部静态类Entry继承了HashMap的Entry，但是LinkedHashMap.Entry多了两个字段：before和after，before表示在本节点之前添加到LinkedHashMap的那个节点，after表示在本节点之后添加到LinkedHashMap的那个节点，这里的之前和之后指时间上的先后顺序。 static class Entry&lt;K,V&gt; extends HashMap.Node&lt;K,V&gt; &#123; Entry&lt;K,V&gt; before, after; Entry(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; super(hash, key, value, next); &#125; &#125; 同时类里有两个成员变量head和tail,分别指向内部双向链表的表头、表尾。 //双向链表的头结点 transient LinkedHashMap.Entry&lt;K,V&gt; head; //双向链表的尾节点 transient LinkedHashMap.Entry&lt;K,V&gt; tail; 我们通过两张图来看下LinkedHashMap的存储结构 图片来自：coolblog 将LinkedHashMap的accessOrder字段设置为true后，每次访问哈希表中的节点都将该节点移到链表的末尾，表示该节点是最新访问的节点。即循环双向链表的头部存放的是最久访问的节点或最先插入的节点，尾部为最近访问的或最近插入的节点。 由于增加了一个accessOrder属性，LinkedHashMap相对HashMap来说增加了一个构造方法用来控制迭代顺序。 final boolean accessOrder; public LinkedHashMap() &#123; super(); accessOrder = false; &#125; //指定初始化时的容量， public LinkedHashMap(int initialCapacity) &#123; super(initialCapacity); accessOrder = false; &#125; //指定初始化时的容量，和扩容的加载因子 public LinkedHashMap(int initialCapacity, float loadFactor) &#123; super(initialCapacity, loadFactor); accessOrder = false; &#125; //指定初始化时的容量，和扩容的加载因子，以及迭代输出节点的顺序 public LinkedHashMap(int initialCapacity, float loadFactor, boolean accessOrder) &#123; super(initialCapacity, loadFactor); this.accessOrder = accessOrder; &#125; //利用另一个Map 来构建 public LinkedHashMap(Map&lt;? extends K, ? extends V&gt; m) &#123; super(); accessOrder = false; //该方法上文分析过，批量插入一个map中的所有数据到 本集合中。 putMapEntries(m, false); &#125; 添加元素LinkedHashMap在添加元素的时候，依旧使用的是HashMap中的put方法。不同的是LinkedHashMap重写了newNode()方法在每次构建新节点时，通过linkNodeLast(p);将新节点链接在内部双向链表的尾部。 //将新增的节点，连接在链表的尾部 private void linkNodeLast(LinkedHashMap.Entry&lt;K,V&gt; p) &#123; LinkedHashMap.Entry&lt;K,V&gt; last = tail; tail = p; //如果集合之前是空的 if (last == null) head = p; else &#123;//将新节点连接在链表的尾部 p.before = last; last.after = p; &#125; &#125; 删除元素LinkedHashMap并没有重写HashMap的remove()方法，但是他重写了afterNodeRemoval()方法，这个方法的作用是在删除一个节点时，同步将该节点从双向链表中删除。该方法将会在remove中被回调。 //在删除节点e时，同步将e从双向链表上删除 void afterNodeRemoval(Node&lt;K,V&gt; e) &#123; // unlink LinkedHashMap.Entry&lt;K,V&gt; p = (LinkedHashMap.Entry&lt;K,V&gt;)e, b = p.before, a = p.after; //将待删除节点 p 的前置后置节点都置空 p.before = p.after = null; //如果前置节点是null，则说明现在的头结点应该是后置节点a if (b == null) head = a; else//否则将前置节点b的后置节点指向a b.after = a; //同理如果后置节点时null ，则尾节点应是b if (a == null) tail = b; else//否则更新后置节点a的前置节点为b a.before = b; &#125; 删除过程总的来说可以分为三步： 根据 hash 定位到桶位置 遍历链表或调用红黑树相关的删除方法 回调afterNodeRemoval，从 LinkedHashMap 维护的双链表中移除要删除的节点 更新元素// 清除节点时要将头尾节点一起清除 public void clear() &#123; super.clear(); head = tail = null; &#125; 查找元素LinkedHashMap重写了get()和getOrDefault()方法默认情况下，LinkedHashMap是按插入顺序维护链表。不过如果我们在初始化 LinkedHashMap时，指定 accessOrder参数为 true，即可让它按访问顺序维护链表。访问顺序的原理是，当我们调用get/getOrDefault/replace等方法时，会将这些方法访问的节点移动到链表的尾部。 public V get(Object key) &#123; Node&lt;K,V&gt; e; if ((e = getNode(hash(key), key)) == null) return null; if (accessOrder) // 回调afterNodeAccess(Node&lt;K,V&gt; e) afterNodeAccess(e); // 将节点e移至双向链表的尾部（保证迭代顺序） return e.value; &#125; public V getOrDefault(Object key, V defaultValue) &#123; Node&lt;K,V&gt; e; if ((e = getNode(hash(key), key)) == null) return defaultValue; if (accessOrder) afterNodeAccess(e); // 作用同上 return e.value; &#125; void afterNodeAccess(Node&lt;K,V&gt; e) &#123; // move node to last LinkedHashMap.Entry&lt;K,V&gt; last;//原尾节点 //如果accessOrder 是true ，且原尾节点不等于e if (accessOrder &amp;&amp; (last = tail) != e) &#123; //节点e强转成双向链表节点p LinkedHashMap.Entry&lt;K,V&gt; p = (LinkedHashMap.Entry&lt;K,V&gt;)e, b = p.before, a = p.after; //p现在是尾节点， 后置节点一定是null p.after = null; //如果p的前置节点是null，则p以前是头结点，所以更新现在的头结点是p的后置节点a if (b == null) head = a; else//否则更新p的前直接点b的后置节点为 a b.after = a; //如果p的后置节点不是null，则更新后置节点a的前置节点为b if (a != null) a.before = b; else//如果原本p的后置节点是null，则p就是尾节点。 此时 更新last的引用为 p的前置节点b last = b; if (last == null) //原本尾节点是null 则，链表中就一个节点 head = p; else &#123;//否则 更新 当前节点p的前置节点为 原尾节点last， last的后置节点是p p.before = last; last.after = p; &#125; //尾节点的引用赋值成p tail = p; //修改modCount。 ++modCount; &#125; &#125; // 因为LinkedHashMap中维护了一个双向链表所以相对于HashMap中的双重循环遍历这个方法要优化很多 LinkedHashMap public boolean containsValue(Object value) &#123; for (LinkedHashMap.Entry&lt;K,V&gt; e = head; e != null; e = e.after) &#123; // 通过双向链表来遍历 V v = e.value; if (v == value || (value != null &amp;&amp; value.equals(v))) return true; &#125; return false; &#125; HashMap public boolean containsValue(Object value) &#123; Node&lt;K,V&gt;[] tab; V v; if ((tab = table) != null &amp;&amp; size &gt; 0) &#123; for (int i = 0; i &lt; tab.length; ++i) &#123; for (Node&lt;K,V&gt; e = tab[i]; e != null; e = e.next) &#123; if ((v = e.value) == value || (value != null &amp;&amp; value.equals(v))) return true; &#125; &#125; &#125; return false; &#125; 其他方法LinkedHashMap还有一个比较神奇的存在。 void afterNodeInsertion(boolean evict) &#123; // possibly remove eldest LinkedHashMap.Entry&lt;K,V&gt; first; // 根据条件判断是否移除最近最少被访问的节点 if (evict &amp;&amp; (first = head) != null &amp;&amp; removeEldestEntry(first)) &#123; K key = first.key; removeNode(hash(key), key, null, false, true); &#125; &#125; // 移除最近最少被访问条件之一，通过覆盖此方法可实现不同策略的缓存 protected boolean removeEldestEntry(Map.Entry&lt;K,V&gt; eldest) &#123; return false; &#125; 上面的方法一般不会被执行，但是当我们基于 LinkedHashMap 实现缓存时，通过覆写removeEldestEntry方法可以实现自定义策略的 LRU 缓存。比如我们可以根据节点数量判断是否移除最近最少被访问的节点，或者根据节点的存活时间判断是否移除该节点等。 迭代器public Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet() &#123; Set&lt;Map.Entry&lt;K,V&gt;&gt; es; //返回LinkedEntrySet return (es = entrySet) == null ? (entrySet = new LinkedEntrySet()) : es; &#125; final class LinkedEntrySet extends AbstractSet&lt;Map.Entry&lt;K,V&gt;&gt; &#123; public final Iterator&lt;Map.Entry&lt;K,V&gt;&gt; iterator() &#123; return new LinkedEntryIterator(); &#125; &#125; final class LinkedEntryIterator extends LinkedHashIterator implements Iterator&lt;Map.Entry&lt;K,V&gt;&gt; &#123; public final Map.Entry&lt;K,V&gt; next() &#123; return nextNode(); &#125; &#125; abstract class LinkedHashIterator &#123; //下一个节点 LinkedHashMap.Entry&lt;K,V&gt; next; //当前节点 LinkedHashMap.Entry&lt;K,V&gt; current; int expectedModCount; LinkedHashIterator() &#123; //初始化时，next 为 LinkedHashMap内部维护的双向链表的扁头 next = head; //记录当前modCount，以满足fail-fast expectedModCount = modCount; //当前节点为null current = null; &#125; //判断是否还有next public final boolean hasNext() &#123; //就是判断next是否为null，默认next是head 表头 return next != null; &#125; //nextNode() 就是迭代器里的next()方法 。 //该方法的实现可以看出，迭代LinkedHashMap，就是从内部维护的双链表的表头开始循环输出。 final LinkedHashMap.Entry&lt;K,V&gt; nextNode() &#123; //记录要返回的e。 LinkedHashMap.Entry&lt;K,V&gt; e = next; //判断fail-fast if (modCount != expectedModCount) throw new ConcurrentModificationException(); //如果要返回的节点是null，异常 if (e == null) throw new NoSuchElementException(); //更新当前节点为e current = e; //更新下一个节点是e的后置节点 next = e.after; //返回e return e; &#125; //删除方法 最终还是调用了HashMap的removeNode方法 public final void remove() &#123; Node&lt;K,V&gt; p = current; if (p == null) throw new IllegalStateException(); if (modCount != expectedModCount) throw new ConcurrentModificationException(); current = null; K key = p.key; removeNode(hash(key), key, null, false, false); expectedModCount = modCount; &#125; &#125; 该方法的实现可以看出，迭代LinkedHashMap，就是从内部维护的双链表的表头开始循环输出。而双链表节点的顺序在LinkedHashMap的增、删、改、查时都会更新。以满足按照插入顺序输出，还是访问顺序输出。 总结在日常开发中LinkedHashMap 的使用频率没有HashMap高，但它也个重要的实现。在 Java 集合框架中，HashMap、LinkedHashMap 和 TreeMap 三个映射类基于不同的数据结构，并实现了不同的功能。HashMap 底层基于拉链式的散列结构，并在 JDK 1.8 中引入红黑树优化过长链表的问题。基于这样结构，HashMap 可提供高效的增删改查操作。LinkedHashMap 在其之上，通过维护一条双向链表，实现了散列数据结构的有序遍历。TreeMap 底层基于红黑树实现，利用红黑树的性质，实现了键值对排序功能。具体实现我们下次分析。","categories":[{"name":"Java","slug":"Java","permalink":"https://gyl-coder.top/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://gyl-coder.top/tags/Java/"},{"name":"集合","slug":"集合","permalink":"https://gyl-coder.top/tags/%E9%9B%86%E5%90%88/"},{"name":"LinkedHashMap","slug":"LinkedHashMap","permalink":"https://gyl-coder.top/tags/LinkedHashMap/"}],"author":{"name":"yanliang","avatar":"https://cdn.jsdelivr.net/gh/gyl-coder/blogImgs/images/touxiang.webp","url":"https://gyl-coder.top"}},{"title":"LinkedList 底层实现原理分析","slug":"java/collection/LinkedList","date":"2020-07-04T00:00:00.000Z","updated":"2020-12-27T04:47:15.974Z","comments":true,"path":"java/collection/LinkedList/","link":"","permalink":"https://gyl-coder.top/java/collection/LinkedList/","excerpt":"在Java.util包下 继承自AbstractSequentialList 实现 List 接口，能对它进行队列操作。 实现 Deque 接口，即能将LinkedList当作双端队列使用。 实现了Cloneable接口，即覆盖了函数clone()，能克隆。 实现java.io.Serializable接口，这意味着LinkedList支持序列化，能通过序列化去传输。 允许包含null值 迭代器可以快速报错 非线程安全的，如果在多线程中使用（修改），需要在外部作同步处理。 LinkedList是一种可以在任何位置进行高效地插入和移除操作的有序序列，它是基于双向链表实现的。内部有三个变量，size表示链表中元素的个数， first指向链表头部，last指向链表尾部。 结构图如下图所示","text":"在Java.util包下 继承自AbstractSequentialList 实现 List 接口，能对它进行队列操作。 实现 Deque 接口，即能将LinkedList当作双端队列使用。 实现了Cloneable接口，即覆盖了函数clone()，能克隆。 实现java.io.Serializable接口，这意味着LinkedList支持序列化，能通过序列化去传输。 允许包含null值 迭代器可以快速报错 非线程安全的，如果在多线程中使用（修改），需要在外部作同步处理。 LinkedList是一种可以在任何位置进行高效地插入和移除操作的有序序列，它是基于双向链表实现的。内部有三个变量，size表示链表中元素的个数， first指向链表头部，last指向链表尾部。 结构图如下图所示 下面是LinkedList中Node节点的定义，Node类是LinkedList的静态内部类。 private static class Node&lt;E&gt; &#123; E item; // 当前节点所存数据 Node&lt;E&gt; next; // 当前节点的下一个节点 Node&lt;E&gt; prev; // 当前节点的前一个节点 Node(Node&lt;E&gt; prev, E element, Node&lt;E&gt; next) &#123; this.item = element; this.next = next; this.prev = prev; &#125; &#125; 构造方法（Construction method）LinkedList提供了两种种方式的构造器，构造一个空列表、以及构造一个包含指定collection的元素的列表，这些元素按照该collection的迭代器返回的顺序排列的。 public LinkedList() &#123; &#125; public LinkedList(Collection&lt;? extends E&gt; c) &#123; this(); addAll(c); // 调用addAll方法，构建一个包含指定集合c的列表 &#125; 添加元素因为LinkedList即实现了List接口，又实现了Deque接口，所以LinkedList既可以添加将元素添加到尾部，也可以将元素添加到指定索引位置，还可以添加添加整个集合；另外既可以在头部添加，又可以在尾部添加。 //添加元素作为第一个元素 public void addFirst(E e) &#123; linkFirst(e); &#125; //店家元素作为最后一个元素 public void addLast(E e) &#123; linkLast(e); &#125; //使用对应参数作为第一个节点，内部使用 private void linkFirst(E e) &#123; final Node&lt;E&gt; f = first;//得到首节点 final Node&lt;E&gt; newNode = new Node&lt;&gt;(null, e, f);//创建一个节点 first = newNode; //更新首节点 if (f == null) last = newNode; //如果之前首节点为空(size==0)，那么尾节点就是首节点 else f.prev = newNode; //如果之前首节点不为空，之前的首节点的前一个节点为当前首节点 size++; //长度+1 modCount++; //修改次数+1 &#125; //使用对应参数作为尾节点 void linkLast(E e) &#123; final Node&lt;E&gt; l = last; //得到尾节点 final Node&lt;E&gt; newNode = new Node&lt;&gt;(l, e, null);//使用参数创建一个节点 last = newNode; //设置尾节点 if (l == null) first = newNode; //如果之前尾节点为空(size==0)，首节点即尾节点 else l.next = newNode; //如果之前尾节点不为空，之前的尾节点的后一个就是当前的尾节点 size++; modCount++; &#125; //在非空节点succ之前插入元素E。 void linkBefore(E e, Node&lt;E&gt; succ) &#123; final Node&lt;E&gt; pred = succ.prev;//获取前一个节点 final Node&lt;E&gt; newNode = new Node&lt;&gt;(pred, e, succ);//使用参数创建新的节点 succ.prev = newNode;//当前节点指向新的节点 if (pred == null) first = newNode;//如果前一个节点为null，新的节点就是首节点 else pred.next = newNode;//如果存在前节点，那么前节点的向后指向新节点 size++; modCount++; &#125; //添加指定集合的元素到列表，默认从最后开始添加 public boolean addAll(Collection&lt;? extends E&gt; c) &#123; return addAll(size, c);//size表示最后一个位置 &#125; /* 从指定位置（而不是下标！下标即索引从0开始，位置可以看做从1开始，其实也是0）后面添加指定集合的元素到列表中，只要有至少一次添加就会返回true index换成position应该会更好理解，所以也就是从索引为index(position)的元素的前面索引为index-1的后面添加！ 当然位置可以为0啊，为0的时候就是从位置0(虽然它不存在)后面开始添加嘛，所以理所当然就是添加到第一个位置（位置1的前面）的前面 比如列表：0 1 2 3，如果此处index=4(实际索引为3)，就是在元素3后面添加；如果index=3(实际索引为2)，就在元素2后面添加。 */ public boolean addAll(int index, Collection&lt;? extends E&gt; c) &#123; checkPositionIndex(index); //检查索引是否正确（0&lt;=index&lt;=size） Object[] a = c.toArray(); //得到元素数组 int numNew = a.length; //得到元素个数 if (numNew == 0) //若没有元素要添加，直接返回false return false; Node&lt;E&gt; pred, succ; if (index == size) &#123; //如果是在末尾开始添加，当前节点后一个节点初始化为null，前一个节点为尾节点 succ = null; //这里可以看做node(index)，不过index=size了（index最大只能是size-1），所以这里的succ只能=null，也方便后面判断 pred = last; &#125; else &#123; //如果不是从末尾开始添加，当前位置的节点为指定位置的节点，前一个节点为要添加的节点的前一个节点 succ = node(index); //添加好元素后(整个新加的)的后一个节点 pred = succ.prev; &#125; //遍历数组并添加到列表中 for (Object o : a) &#123; @SuppressWarnings(&quot;unchecked&quot;) E e = (E) o; Node&lt;E&gt; newNode = new Node&lt;&gt;(pred, e, null);//创建一个节点，向前指向上面得到的前节点 if (pred == null) first = newNode; //若当前节点为null，则新加的节点为首节点 else pred.next = newNode;//如果存在前节点，前节点会向后指向新加的节点 pred = newNode; //新加的节点成为前一个节点 &#125; if (succ == null) &#123; //pred.next = null //加上这句也可以更好的理解 last = pred; //如果是从最后开始添加的，则最后添加的节点成为尾节点 &#125; else &#123; pred.next = succ; //如果不是从最后开始添加的，则最后添加的节点向后指向之前得到的后续第一个节点 succ.prev = pred; //当前，后续的第一个节点也应改为向前指向最后一个添加的节点 &#125; size += numNew; modCount++; return true; &#125; //将指定的元素(E element)插入到列表的指定位置(index) public void add(int index, E element) &#123; checkPositionIndex(index); //index &gt;= 0 &amp;&amp; index &lt;= size if (index == size) linkLast(element); //尾插入 else linkBefore(element, node(index)); //中间插入 &#125; linkBefore的添加步骤： 创建newNode节点，将newNode的后继指针指向succ，前驱指针指向pred 将succ的前驱指针指向newNode 根据pred是否为null，进行不同操作。 如果pred为null，说明该节点插入在头节点之前，要重置first头节点 如果pred不为null，那么直接将pred的后继指针指向newNode即可 addAll的添加步骤： 检查index索引范围 得到集合数据 得到插入位置的前驱和后继节点 遍历数据，将数据插入到指定位置 删除元素同样的LinkedList也提供了很多方法来删除元素 // 删除首节点并返回删除前首节点的值，内部使用 (f == first &amp;&amp; f != null) private E unlinkFirst(Node&lt;E&gt; f) &#123; final E element = f.item; // 获取首节点的值 final Node&lt;E&gt; next = f.next; // 获取首节点的后一个节点 f.item = null; f.next = null; // help GC first = next; // 更新首节点 if (next == null) //如果不存在下一个节点，则首尾都为null last = null; else next.prev = null; //如果存在下一个节点，那它的前指针为null size--; modCount++; return element; &#125; // 删除尾节点，并返回尾节点的元素 （assert l == last &amp;&amp; l != null） private E unlinkLast(Node&lt;E&gt; l) &#123; final E element = l.item;//获取尾节点的值 final Node&lt;E&gt; prev = l.prev;//获取尾节点前一个节点 l.item = null; l.prev = null; // help GC last = prev; //前一个节点成为新的尾节点 if (prev == null) first = null; //如果前一个节点不存在，则首尾都为null else prev.next = null;//如果前一个节点存在，先后指向null size--; modCount++; return element; &#125; // 删除指定节点x并返回节点的值（x != null） E unlink(Node&lt;E&gt; x) &#123; //获取当前值和前后节点 final E element = x.item; final Node&lt;E&gt; next = x.next; final Node&lt;E&gt; prev = x.prev; if (prev == null) &#123; first = next; //如果前一个节点为空(如当前节点为首节点)，后一个节点成为新的首节点 &#125; else &#123; prev.next = next;//如果前一个节点不为空，那么他先后指向当前的下一个节点 x.prev = null; //help GC &#125; if (next == null) &#123; last = prev; //如果后一个节点为空(如当前节点为尾节点)，当前节点前一个成为新的尾节点 &#125; else &#123; next.prev = prev;//如果后一个节点不为空，后一个节点向前指向当前的前一个节点 x.next = null; //help GC &#125; x.item = null; //help GC size--; modCount++; return element; &#125; //删除第一个元素并返回删除的元素 public E removeFirst() &#123; final Node&lt;E&gt; f = first;//得到第一个节点 if (f == null) //如果为空，抛出异常 throw new NoSuchElementException(); return unlinkFirst(f); &#125; //删除最后一个元素并返回删除的值 public E removeLast() &#123; final Node&lt;E&gt; l = last;//得到最后一个节点 if (l == null) //如果为空，抛出异常 throw new NoSuchElementException(); return unlinkLast(l); &#125; 序列化方法private static final long serialVersionUID = 876323262645176354L; //序列化：将linkedList的“大小，所有的元素值”都写入到输出流中 private void writeObject(java.io.ObjectOutputStream s) throws java.io.IOException &#123; s.defaultWriteObject(); s.writeInt(size); for (Node&lt;E&gt; x = first; x != null; x = x.next) s.writeObject(x.item); &#125; //反序列化：先将LinkedList的“大小”读出，然后将“所有的元素值”读出 @SuppressWarnings(&quot;unchecked&quot;) private void readObject(java.io.ObjectInputStream s) throws java.io.IOException, ClassNotFoundException &#123; s.defaultReadObject(); int size = s.readInt(); for (int i = 0; i &lt; size; i++) linkLast((E)s.readObject()); //以尾插入的方式 &#125; 队列操作//提供普通队列和双向队列的功能，当然，也可以实现栈，FIFO，FILO //出队（从前端），获得第一个元素，不存在会返回null，不会删除元素（节点） public E peek() &#123; final Node&lt;E&gt; f = first; return (f == null) ? null : f.item; &#125; //出队（从前端），不删除元素，若为null会抛出异常而不是返回null public E element() &#123; return getFirst(); &#125; //出队（从前端），如果不存在会返回null，存在的话会返回值并移除这个元素（节点） public E poll() &#123; final Node&lt;E&gt; f = first; return (f == null) ? null : unlinkFirst(f); &#125; //出队（从前端），如果不存在会抛出异常而不是返回null，存在的话会返回值并移除这个元素（节点） public E remove() &#123; return removeFirst(); &#125; //入队（从后端），始终返回true public boolean offer(E e) &#123; return add(e); &#125; //入队（从前端），始终返回true public boolean offerFirst(E e) &#123; addFirst(e); return true; &#125; //入队（从后端），始终返回true public boolean offerLast(E e) &#123; addLast(e);//linkLast(e) return true; &#125; //出队（从前端），获得第一个元素，不存在会返回null，不会删除元素（节点） public E peekFirst() &#123; final Node&lt;E&gt; f = first; return (f == null) ? null : f.item; &#125; //出队（从后端），获得最后一个元素，不存在会返回null，不会删除元素（节点） public E peekLast() &#123; final Node&lt;E&gt; l = last; return (l == null) ? null : l.item; &#125; //出队（从前端），获得第一个元素，不存在会返回null，会删除元素（节点） public E pollFirst() &#123; final Node&lt;E&gt; f = first; return (f == null) ? null : unlinkFirst(f); &#125; //出队（从后端），获得最后一个元素，不存在会返回null，会删除元素（节点） public E pollLast() &#123; final Node&lt;E&gt; l = last; return (l == null) ? null : unlinkLast(l); &#125; //入栈，从前面添加 public void push(E e) &#123; addFirst(e); &#125; //出栈，返回栈顶元素，从前面移除（会删除） public E pop() &#123; return removeFirst(); &#125; 迭代器//返回迭代器 public Iterator&lt;E&gt; descendingIterator() &#123; return new DescendingIterator(); &#125; //迭代器 private class DescendingIterator implements Iterator&lt;E&gt; &#123; private final ListItr itr = new ListItr(size()); public boolean hasNext() &#123; return itr.hasPrevious(); &#125; public E next() &#123; return itr.previous(); &#125; public void remove() &#123; itr.remove(); &#125; &#125; public ListIterator&lt;E&gt; listIterator(int index) &#123; checkPositionIndex(index); return new ListItr(index); &#125; private class ListItr implements ListIterator&lt;E&gt; &#123; private Node&lt;E&gt; lastReturned; private Node&lt;E&gt; next; private int nextIndex; private int expectedModCount = modCount;//保存当前modCount，确保fail-fast机制 ListItr(int index) &#123; next = (index == size) ? null : node(index);//得到当前索引指向的next节点 nextIndex = index; &#125; public boolean hasNext() &#123; // 判断后面是否还有元素 return nextIndex &lt; size; &#125; public E next() &#123; //获取下一个节点 checkForComodification(); if (!hasNext()) throw new NoSuchElementException(); lastReturned = next; next = next.next; nextIndex++; return lastReturned.item; &#125; public boolean hasPrevious() &#123; return nextIndex &gt; 0; &#125; //获取前一个节点，将next节点向前移 public E previous() &#123; checkForComodification(); if (!hasPrevious()) throw new NoSuchElementException(); lastReturned = next = (next == null) ? last : next.prev; nextIndex--; return lastReturned.item; &#125; public int nextIndex() &#123; return nextIndex; &#125; public int previousIndex() &#123; return nextIndex - 1; &#125; public void remove() &#123; checkForComodification(); if (lastReturned == null) throw new IllegalStateException(); Node&lt;E&gt; lastNext = lastReturned.next; unlink(lastReturned); if (next == lastReturned) next = lastNext; else nextIndex--; lastReturned = null; expectedModCount++; &#125; public void set(E e) &#123; if (lastReturned == null) throw new IllegalStateException(); checkForComodification(); lastReturned.item = e; &#125; public void add(E e) &#123; checkForComodification(); lastReturned = null; if (next == null) linkLast(e); else linkBefore(e, next); nextIndex++; expectedModCount++; &#125; public void forEachRemaining(Consumer&lt;? super E&gt; action) &#123; Objects.requireNonNull(action); while (modCount == expectedModCount &amp;&amp; nextIndex &lt; size) &#123; action.accept(next.item); lastReturned = next; next = next.next; nextIndex++; &#125; checkForComodification(); &#125; final void checkForComodification() &#123; if (modCount != expectedModCount) throw new ConcurrentModificationException(); &#125; &#125; 在ListIterator的构造器中，得到了当前位置的节点，就是变量next。next()方法返回当前节点的值并将next指向其后继节点，previous()方法返回当前节点的前一个节点的值并将next节点指向其前驱节点。 由于Node是一个双向节点，所以这用了一个节点就可以实现从前向后迭代和从后向前迭代。另外在ListIterator初始时，exceptedModCount保存了当前的modCount，如果在迭代期间，有操作改变了链表的底层结构，那么再操作迭代器的方法时将会抛出ConcurrentModificationException。 其他方法//获取第一个元素 public E getFirst() &#123; final Node&lt;E&gt; f = first;//得到首节点 if (f == null) //如果为空，抛出异常 throw new NoSuchElementException(); return f.item; &#125; //获取最后一个元素 public E getLast() &#123; final Node&lt;E&gt; l = last;//得到尾节点 if (l == null) //如果为空，抛出异常 throw new NoSuchElementException(); return l.item; &#125; //检查是否包含某个元素，返回bool public boolean contains(Object o) &#123; return indexOf(o) != -1;//返回指定元素的索引位置，不存在就返回-1，然后比较返回bool值 &#125; //返回列表长度 public int size() &#123; return size; &#125; //清空表 public void clear() &#123; // help GC for (Node&lt;E&gt; x = first; x != null; ) &#123; Node&lt;E&gt; next = x.next; x.item = null; x.next = null; x.prev = null; x = next; &#125; first = last = null; size = 0; modCount++; &#125; //获取指定索引的节点的值 public E get(int index) &#123; checkElementIndex(index); return node(index).item; &#125; //修改指定索引的值并返回之前的值 public E set(int index, E element) &#123; checkElementIndex(index); // 检查下标是否合法 Node&lt;E&gt; x = node(index); E oldVal = x.item; x.item = element; return oldVal; &#125; //获取指定位置的节点 Node&lt;E&gt; node(int index) &#123; if (index &lt; (size &gt;&gt; 1)) &#123;//如果位置索引小于列表长度的一半(或一半减一)，从前面开始遍历； Node&lt;E&gt; x = first;//index==0时不会循环，直接返回first for (int i = 0; i &lt; index; i++) x = x.next; return x; &#125; else &#123; // 否则，从后面开始遍历 Node&lt;E&gt; x = last; for (int i = size - 1; i &gt; index; i--) x = x.prev; return x; &#125; &#125; //获取指定元素从first开始的索引位置，不存在就返回-1 //这里不能按条件双向找了，所以通常根据索引获得元素的速度比通过元素获得索引的速度快 public int indexOf(Object o) &#123; int index = 0; if (o == null) &#123; for (Node&lt;E&gt; x = first; x != null; x = x.next) &#123; if (x.item == null) return index; index++; &#125; &#125; else &#123; for (Node&lt;E&gt; x = first; x != null; x = x.next) &#123; if (o.equals(x.item)) return index; index++; &#125; &#125; return -1; &#125; //获取指定元素从first开始最后出现的索引，不存在就返回-1 //但实际查找是从last开始的 public int lastIndexOf(Object o) &#123; int index = size; if (o == null) &#123; for (Node&lt;E&gt; x = last; x != null; x = x.prev) &#123; index--; if (x.item == null) return index; &#125; &#125; else &#123; for (Node&lt;E&gt; x = last; x != null; x = x.prev) &#123; index--; if (o.equals(x.item)) return index; &#125; &#125; return -1; &#125; //返回此 LinkedList实例的浅拷贝 public Object clone() &#123; LinkedList&lt;E&gt; clone = superClone(); clone.first = clone.last = null; clone.size = 0; clone.modCount = 0; for (Node&lt;E&gt; x = first; x != null; x = x.next) clone.add(x.item); return clone; &#125; //返回一个包含LinkedList中所有元素值的数组 public Object[] toArray() &#123; Object[] result = new Object[size]; int i = 0; for (Node&lt;E&gt; x = first; x != null; x = x.next) result[i++] = x.item; return result; &#125; //如果给定的参数数组长度足够，则将ArrayList中所有元素按序存放于参数数组中，并返回 //如果给定的参数数组长度小于LinkedList的长度，则返回一个新分配的、长度等于LinkedList长度的、包含LinkedList中所有元素的新数组 @SuppressWarnings(&quot;unchecked&quot;) public &lt;T&gt; T[] toArray(T[] a) &#123; if (a.length &lt; size) a = (T[])java.lang.reflect.Array.newInstance( a.getClass().getComponentType(), size); int i = 0; Object[] result = a; for (Node&lt;E&gt; x = first; x != null; x = x.next) result[i++] = x.item; if (a.length &gt; size) a[size] = null; return a; &#125;","categories":[{"name":"Java","slug":"Java","permalink":"https://gyl-coder.top/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://gyl-coder.top/tags/Java/"},{"name":"集合","slug":"集合","permalink":"https://gyl-coder.top/tags/%E9%9B%86%E5%90%88/"},{"name":"LinkedList","slug":"LinkedList","permalink":"https://gyl-coder.top/tags/LinkedList/"}],"author":{"name":"yanliang","avatar":"https://cdn.jsdelivr.net/gh/gyl-coder/blogImgs/images/touxiang.webp","url":"https://gyl-coder.top"}},{"title":"Vector 底层实现原理分析","slug":"java/collection/Vector","date":"2020-07-03T00:00:00.000Z","updated":"2020-12-27T04:47:15.974Z","comments":true,"path":"java/collection/Vector/","link":"","permalink":"https://gyl-coder.top/java/collection/Vector/","excerpt":"Vector，一个可变长的数组，底层实现与 ArrayList 大同小异，但Vector是同步的（线程安全），Vector的很多方法之前都加了关键字synchronized，所以是线程安全的。 由于Vector的实现和ArrayList的实现大同小异，这里就不再逐一分析Vector中的方法，主要分析一下和ArrayList不同的方法。 首先我们还是来看以下Vector中定义的变量","text":"Vector，一个可变长的数组，底层实现与 ArrayList 大同小异，但Vector是同步的（线程安全），Vector的很多方法之前都加了关键字synchronized，所以是线程安全的。 由于Vector的实现和ArrayList的实现大同小异，这里就不再逐一分析Vector中的方法，主要分析一下和ArrayList不同的方法。 首先我们还是来看以下Vector中定义的变量 public class Vector&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable &#123; protected Object[] elementData; // 存储Vector中元素 protected int elementCount; // Vector中的元素个数 protected int capacityIncrement; // Vector的增长系数 private static final long serialVersionUID = -2767605614048989439L; // Vector的序列版本号 private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8; // 能够分配元素数量的最大值 &#125; 构造方法Vector有四个构造方法，其内部有两个重要的参数，一个是elementCount代表当前元素个数，一个是capacityIncrement代表当列表元素满了之后增加的容量。如果不设置capacityIncrement，那么Vector容量扩展时默认将扩展两倍，在ArrayList源码分析中，我们可以知道ArrayList在扩容时默认将扩展1.5倍。ector初始时容量为10，而ArrayList初始容量为0。 // Vector构造函数。默认容量是10。 public Vector() &#123; this(10); &#125; // 指定Vector容量大小的构造函数 public Vector(int initialCapacity) &#123; this(initialCapacity, 0); &#125; // 指定Vector&quot;容量大小&quot;和&quot;增长系数&quot;的构造函数 public Vector(int initialCapacity, int capacityIncrement) &#123; super(); if (initialCapacity &lt; 0) throw new IllegalArgumentException(&quot;Illegal Capacity: &quot;+ initialCapacity); // 新建一个数组，数组容量是initialCapacity this.elementData = new Object[initialCapacity]; // 设置容量增长系数 this.capacityIncrement = capacityIncrement; &#125; // 指定集合的Vector构造函数。 public Vector(Collection&lt;? extends E&gt; c) &#123; // 获取“集合(c)”的数组，并将其赋值给elementData elementData = c.toArray(); // 设置数组长度 elementCount = elementData.length; // c.toArray might (incorrectly) not return Object[] (see 6260652) if (elementData.getClass() != Object[].class) elementData = Arrays.copyOf(elementData, elementCount, Object[].class); &#125; Vector中的构造器和ArrayList中的基本相同，只不过Vector中多了一个可以自定义增长系数的构造器public Vector(int initialCapacity, int capacityIncrement) 扩容机制Vector中的添加元素和ArrayList中的大同小异，这里不再具体分析，这里只分析下Vector的扩容机制 /** * 增加vector容量 * 如果vector当前容量小于至少需要的容量，它的容量将增加。 * 新的容量将在旧的容量的基础上加上capacityIncrement，除非capacityIncrement小于等于0，在这种情况下，容量将会增加一倍。 * 增加后，如果新的容量还是小于至少需要的容量，那就将容量扩容至至少需要的容量。 */ public synchronized void ensureCapacity(int minCapacity) &#123; if (minCapacity &gt; 0) &#123; modCount++; ensureCapacityHelper(minCapacity); &#125; &#125; /** * ensureCapacity()方法的unsynchronized实现。 * ensureCapacity()是同步的，它可以调用本方法来扩容，而不用承受同步带来的消耗 */ private void ensureCapacityHelper(int minCapacity) &#123; // 如果至少需要的容量 &gt; 数组缓冲区当前的长度，就进行扩容 if (minCapacity - elementData.length &gt; 0) grow(minCapacity); &#125; /** * 分派给arrays的最大容量 * 为什么要减去8呢？ * 因为某些VM会在数组中保留一些头字，尝试分配这个最大存储容量，可能会导致array容量大于VM的limit，最终导致OutOfMemoryError。 */ private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8; /** * 扩容，保证vector至少能存储minCapacity个元素。 * 首次扩容时，newCapacity = oldCapacity + ((capacityIncrement &gt; 0) ?capacityIncrement : oldCapacity);即如果capacityIncrement&gt;0，就加capacityIncrement，如果不是就增加一倍。 * 如果第一次扩容后，容量还是小于minCapacity，就直接将容量增为minCapacity。 */ private void grow(int minCapacity) &#123; // 获取当前数组的容量 int oldCapacity = elementData.length; //计算目标容量，如果指定了每次扩展的量，直接增加，如果没有就直接翻倍 int newCapacity = oldCapacity + ((capacityIncrement &gt; 0) ? capacityIncrement : oldCapacity); //如果自动扩容的容量无法满足用户指定的容量，则直接扩容到用户指定的容量 if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; ///如果扩容后的容量大于临界值，则进行大容量分配 if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); elementData = Arrays.copyOf(elementData, newCapacity); &#125; // 进行大容量分配 private static int hugeCapacity(int minCapacity) &#123; //数据溢出，抛出异常 if (minCapacity &lt; 0) // overflow throw new OutOfMemoryError(); //如果想要的容量大于MAX_ARRAY_SIZE，则分配Integer.MAX_VALUE，否则分配MAX_ARRAY_SIZE return (minCapacity &gt; MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE; &#125; 其他方法Vector中添加一个枚举方法 // 返回一个枚举类型的对象 public Enumeration&lt;E&gt; elements() &#123; return new Enumeration&lt;E&gt;() &#123; int count = 0; public boolean hasMoreElements() &#123; // 判断后面是否还有数据 return count &lt; elementCount; &#125; public E nextElement() &#123; // 返回一个数据 synchronized (Vector.this) &#123; if (count &lt; elementCount) &#123; return elementData(count++); &#125; &#125; throw new NoSuchElementException(&quot;Vector Enumeration&quot;); &#125; &#125;; &#125; 其他类同方法请参考ArrayList源码分析Vector与ArrayList的最大区别就是Vector是线程安全的，而ArrayList不是线程安全的。另外区别还有： ArrayList不可以设置扩展的容量，默认1.5倍；Vector可以设置扩展的容量，如果没有设置，默认2倍 ArrayList的无参构造方法中初始容量为0，而Vector的无参构造方法中初始容量为10。 下面我们再来分析下Vector的子类Stack方法。 StackStack类代表最先进先出（LIFO）堆栈的对象。 它扩展了Vector五个操作，允许一个vector被视为堆栈。五个方法分别是： push() 添加元素到堆栈的顶部 pop() 删除堆栈顶部元素 peek() 查看堆栈顶部元素 empty() 判断堆栈是否为空 search() 返回元素所在位置 package java.util; public class Stack&lt;E&gt; extends Vector&lt;E&gt; &#123; /** * Creates an empty Stack. */ public Stack() &#123; &#125; public E push(E item) &#123; addElement(item); // 调用vector中的方法在栈顶添加一个元素 return item; &#125; public synchronized E pop() &#123; E obj; int len = size(); obj = peek(); removeElementAt(len - 1); // 调用vector中的方法删除栈顶的元素 return obj; &#125; public synchronized E peek() &#123; int len = size(); // 返回栈顶元素 if (len == 0) throw new EmptyStackException(); return elementAt(len - 1); &#125; public boolean empty() &#123; return size() == 0; &#125; public synchronized int search(Object o) &#123; int i = lastIndexOf(o); // 因为LIFO 所以选择从后往前遍历 if (i &gt;= 0) &#123; return size() - i; &#125; return -1; &#125; /** use serialVersionUID from JDK 1.0.2 for interoperability */ private static final long serialVersionUID = 1224463164541339165L; &#125;","categories":[{"name":"Java","slug":"Java","permalink":"https://gyl-coder.top/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://gyl-coder.top/tags/Java/"},{"name":"集合","slug":"集合","permalink":"https://gyl-coder.top/tags/%E9%9B%86%E5%90%88/"},{"name":"Vector","slug":"Vector","permalink":"https://gyl-coder.top/tags/Vector/"}],"author":{"name":"yanliang","avatar":"https://cdn.jsdelivr.net/gh/gyl-coder/blogImgs/images/touxiang.webp","url":"https://gyl-coder.top"}},{"title":"ArrayList动态扩容机制","slug":"java/collection/ArrayList_grow","date":"2020-07-02T00:00:00.000Z","updated":"2020-12-27T04:47:15.974Z","comments":true,"path":"java/collection/ArrayList_grow/","link":"","permalink":"https://gyl-coder.top/java/collection/ArrayList_grow/","excerpt":"我们通过一个具体的例子看一下ArrayList的扩容效果先看一下ArrayList的初始容量 ArrayList&lt;Integer&gt; array = new ArrayList&lt;&gt;(); Integer capacity = getCapacity(array); int size = array.size(); System.out.println(&quot;容量：&quot;+capacity); System.out.println(&quot;大小：&quot;+size); 容量：0 大小：0 getCapacity（）方法是用来获取集合容量的，ArrayList通过一个elementData对象数组储存数据，也就是说ArrayList的容量就是该数组的长度。所以我们只要得到了elementData数组就可以知道ArrayList的实际容量。","text":"我们通过一个具体的例子看一下ArrayList的扩容效果先看一下ArrayList的初始容量 ArrayList&lt;Integer&gt; array = new ArrayList&lt;&gt;(); Integer capacity = getCapacity(array); int size = array.size(); System.out.println(&quot;容量：&quot;+capacity); System.out.println(&quot;大小：&quot;+size); 容量：0 大小：0 getCapacity（）方法是用来获取集合容量的，ArrayList通过一个elementData对象数组储存数据，也就是说ArrayList的容量就是该数组的长度。所以我们只要得到了elementData数组就可以知道ArrayList的实际容量。 由于elementData是私有的无法直接得到，但是我们可以通过反射的方式获取。 代码如下： public static Integer getCapacity(ArrayList list) &#123; Integer length = null; Class c = ((Object)list).getClass(); Field f; try &#123; f = c.getDeclaredField(&quot;elementData&quot;); f.setAccessible(true); Object[] o = (Object[]) f.get(list); length = o.length; &#125; catch (NoSuchFieldException ex) &#123; Logger.getLogger(CollectionDemo.class.getName()).log(Level.SEVERE, null, ex); &#125; catch (SecurityException ex) &#123; Logger.getLogger(CollectionDemo.class.getName()).log(Level.SEVERE, null, ex); &#125; catch (IllegalArgumentException ex) &#123; Logger.getLogger(CollectionDemo.class.getName()).log(Level.SEVERE, null, ex); &#125; catch (IllegalAccessException ex) &#123; Logger.getLogger(CollectionDemo.class.getName()).log(Level.SEVERE, null, ex); &#125; return length; &#125; 接下来，我们向ArrayList中添加一个元素 ArrayList&lt;Integer&gt; array = new ArrayList&lt;&gt;(); array.add(1); Integer capacity = getCapacity(array); int size = array.size(); System.out.println(&quot;容量：&quot;+capacity); System.out.println(&quot;大小：&quot;+size); 容量：10 大小：1 向ArrayList中添加11个元素 ArrayList&lt;Integer&gt; array = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 11; i ++) &#123; array.add(i); &#125; Integer capacity = getCapacity(array); int size = array.size(); System.out.println(&quot;容量：&quot;+capacity); System.out.println(&quot;大小：&quot;+size); 容量：15 大小：11 我们发现，当向array中添加11个元素之后，array的容量扩大到原来的1.5倍。 Why does it expansion 1.5 times? 具体为什么，下面我们看一下源码： /** * Appends the specified element to the end of this list. * * @param e element to be appended to this list * @return &lt;tt&gt;true&lt;/tt&gt; (as specified by &#123;@link Collection#add&#125;) */ public boolean add(E e) &#123; ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true; &#125; add方法是通过在array的尾部追加元素的方法，添加数据的。其中，调用ensureCapacityInternal方法用来判断是否需要扩容. private void ensureCapacityInternal(int minCapacity) &#123; if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123; minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); &#125; ensureExplicitCapacity(minCapacity); &#125; private void ensureExplicitCapacity(int minCapacity) &#123; modCount++; // overflow-conscious code if (minCapacity - elementData.length &gt; 0) grow(minCapacity); &#125; 参数传的是当前需要的最小容量，方法首先确认当前ArrayList实例是否为空，如果为空则比较所需容量和默认容量，取其较大值作为所需最小容量值。然后执行ensureExplicitCapacity进一步确定容量，以及是否需要扩容。当所需最小容量大于当前elementData数组长度时，要进行扩容操作。modCount是fail fast机制，不了解也不影响，如果想了解可以看这里,如果minCapacity的值大于添加数据之前的大小，就调用grow方法，进行扩容，否则什么也不做。 发生扩容的条件根据传入的最小需要容量minCapacity来和数组的容量长度对比，若minCapactity大于或等于数组容量，则需要进行扩容。(如果实际存储数组是空数组，则最小需要容量就是默认容量)以上只是真实容量和所需容量的比较，其目的是计算出array的最终容量。真正实现扩容的方法是grow方法。下面具体来了解扩容机制的增长规则 /** * Increases the capacity to ensure that it can hold at least the * number of elements specified by the minimum capacity argument. * * @param minCapacity the desired minimum capacity */ private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity); &#125; 这里传过来的minCapacity的值是array的size+1 添加一个元素，首先计算当前的array所需最小的容量大小，判断是否需要扩容等。 当需要扩容时： 得到当前的ArrayList的容量(oldCapacity)。 计算除扩容后的新容量(newCapacity)，其值(oldCapacity + (oldCapacity &gt;&gt;1))约是oldCapacity 的1.5倍。 这里采用的是移位运算。为什么采用这种方法呢？应该是出于效率的考虑。 当newCapacity小于所需最小容量，那么将所需最小容量赋值给newCapacity。 newCapacity大于ArrayList的所允许的最大容量,处理。进行数据的复制，完成向ArrayList实例添加元素操作。 每次array的size到达当前的容量最大值后，再插入数据就会造成扩容。","categories":[{"name":"Java","slug":"Java","permalink":"https://gyl-coder.top/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://gyl-coder.top/tags/Java/"},{"name":"集合","slug":"集合","permalink":"https://gyl-coder.top/tags/%E9%9B%86%E5%90%88/"},{"name":"ArrayList","slug":"ArrayList","permalink":"https://gyl-coder.top/tags/ArrayList/"}],"author":{"name":"yanliang","avatar":"https://cdn.jsdelivr.net/gh/gyl-coder/blogImgs/images/touxiang.webp","url":"https://gyl-coder.top"}},{"title":"ArrayList 底层实现原理分析","slug":"java/collection/ArrayList","date":"2020-07-01T00:00:00.000Z","updated":"2020-12-27T04:47:15.974Z","comments":true,"path":"java/collection/ArrayList/","link":"","permalink":"https://gyl-coder.top/java/collection/ArrayList/","excerpt":"ArrayList是List接口的 可变数组的实现。实现了所有可选列表操作，并允许包括 null 在内的所有元素。除了实现 List接口外，此类还提供一些方法来操作内部用来存储列表的数组的大小。ArrayList继承自 AbstractList，这是一个抽象类对一些基础的list操作做了一些封装.实现了RandomAccess 标记接口，表明可以实现快速随机访问.实现了Cloneable接口的实现表示该容器具有Clone函数操作，Serializable是序列化。 每个ArrayList实例都有一个容量，该容量是指用来存储列表元素的数组的大小。它总是至少等于列表的大小。随着向ArrayList中不断添加元素，其容量也自动增长。自动增长会带来数据向新数组的重新拷贝，因此，如果可预知数据量的大小，就可在构造ArrayList实例时指定其容量。 在添加大量元素前，应用程序也可以使用ensureCapacity操作来增加ArrayList实例的容量，这可以减少递增式再分配的数量。 注意，此实现不是同步的。如果多个线程同时访问一个ArrayList实例，而其中至少一个线程从结构上修改了列表，那么它必须保持外部同步。 ArrayList这个数据结构比较简单，总体来说，ArrayList底层结构是数组，他的很多方法都是从数组上面演变而来的。 下面我们先来看一下ArrayList中的一些初始值","text":"ArrayList是List接口的 可变数组的实现。实现了所有可选列表操作，并允许包括 null 在内的所有元素。除了实现 List接口外，此类还提供一些方法来操作内部用来存储列表的数组的大小。ArrayList继承自 AbstractList，这是一个抽象类对一些基础的list操作做了一些封装.实现了RandomAccess 标记接口，表明可以实现快速随机访问.实现了Cloneable接口的实现表示该容器具有Clone函数操作，Serializable是序列化。 每个ArrayList实例都有一个容量，该容量是指用来存储列表元素的数组的大小。它总是至少等于列表的大小。随着向ArrayList中不断添加元素，其容量也自动增长。自动增长会带来数据向新数组的重新拷贝，因此，如果可预知数据量的大小，就可在构造ArrayList实例时指定其容量。 在添加大量元素前，应用程序也可以使用ensureCapacity操作来增加ArrayList实例的容量，这可以减少递增式再分配的数量。 注意，此实现不是同步的。如果多个线程同时访问一个ArrayList实例，而其中至少一个线程从结构上修改了列表，那么它必须保持外部同步。 ArrayList这个数据结构比较简单，总体来说，ArrayList底层结构是数组，他的很多方法都是从数组上面演变而来的。 下面我们先来看一下ArrayList中的一些初始值 //通过ArrayList实现的接口可知，其支持随机访问，能被克隆，支持序列化 public class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable &#123; //序列版本号 private static final long serialVersionUID = 8683452581122892189L; //默认初始容量 private static final int DEFAULT_CAPACITY = 10; //空实例的共享空数组实例 private static final Object[] EMPTY_ELEMENTDATA = &#123;&#125;; //被用于默认大小的空实例的共享数组实例。 //与EMPTY_ELEMENTDATA的区别是：当我们向数组中添加第一个元素时，知道数组该扩充多少。 private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = &#123;&#125;; /** * Object[]类型的数组，保存了添加到ArrayList中的元素。ArrayList的容量是该Object[]类型数组的长度 * 当第一个元素被添加时，任何空ArrayList中的elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA将会被 * 扩充到DEFAULT_CAPACITY（默认容量）。 */ transient Object[] elementData; //没有被私有化是为了简化内部类访问 // ArrayList的大小（指其所含的元素个数） private int size; // 记录被修改的次数 protected transient int modCount = 0; // 数组的最大值 private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8 &#125; elementData 是”Object[] 类型的数组”，它保存了添加到ArrayList中的元素。实际上，elementData是个动态数组，我们能通过构造函数 ArrayList(intinitialCapacity)来执行它的初始容量为initialCapacity；如果通过不含参数的构造函数ArrayList()来创建ArrayList，则elementData的容量默认是10。elementData数组的大小会根据ArrayList容量的增长而动态的增长。 构造函数ArrayList提供了三种方式的构造器。可以构造一个默认初始容量为10的空列表、构造一个指定初始容量的空列表以及构造一个包含指定collection的元素的列表。 这些元素按照该collection的迭代器返回的顺序排列的。 // 构造一个指定初始容量的空列表 public ArrayList(int initialCapacity) &#123; if (initialCapacity &gt; 0) &#123; this.elementData = new Object[initialCapacity]; &#125; else if (initialCapacity == 0) &#123; this.elementData = EMPTY_ELEMENTDATA; &#125; else &#123; // 如果给定的初始容量为负值 throw new IllegalArgumentException(&quot;Illegal Capacity: &quot;+ initialCapacity); &#125; &#125; // 构造一个默认初始容量为10的空列表 public ArrayList() &#123; //这里并没有初始化，jdk 1.8之后是在进行add操作后初始化 this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA; &#125; // 构造一个包含指定collection的元素的列表，这些元素按照该collection的迭代器返回的顺序排列的 public ArrayList(Collection&lt;? extends E&gt; c) &#123; elementData = c.toArray(); if ((size = elementData.length) != 0) &#123; // c.toArray()可能不会正确地返回一个 Object[]数组，那么使用Arrays.copyOf()方法 if (elementData.getClass() != Object[].class) elementData = Arrays.copyOf(elementData, size, Object[].class); &#125; else &#123; // 如果指定的collection为空 this.elementData = EMPTY_ELEMENTDATA; &#125; &#125; 使用无参构造器，默认初始容量为什么是10？ 初始时：this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA = {}; size = 0; 向数组中添加第一个元素时，add(E e)方法中调用了ensureCapacityInternal(size + 1)方法，即ensureCapacityInternal(1)； 在ensureCapacityInternal(int minCapacity)方法中，minCapacity=DEFAULT_CAPACITY=10，然后再调用ensureExplicitCapacity(minCapacity)方法，即ensureExplicitCapacity(10)； 在ensureExplicitCapacity(minCapacity)方法中调用grow(minCapacity)方法，即grow(10)，此处为真正具体的数组扩容的算法，在此方法中，通过elementData = Arrays.copyOf(elementData, 10)具体实现了elementData数组初始容量为10的构造。 添加元素// 在数组末尾加上一个元素 public boolean add(E e) &#123; ensureCapacityInternal(size + 1); // 进行扩容检查 elementData[size++] = e; return true; &#125; // 在数组的指定位置添加元素 public void add(int index, E element) &#123; rangeCheckForAdd(index); // 检查index是否越界 ensureCapacityInternal(size + 1); // 进行扩容检查 // 对数据进行复制操作，空出index位置，并插入element，将源数组中从index位置开始后的size-index个元素统一后移一位 System.arraycopy(elementData, index, elementData, index + 1, size - index); elementData[index] = element; size++; // 元素个数加1 &#125; // 按照指定collection集合的迭代器所返回的元素顺序，将该collection中的所有元素添加到列表的尾部 public boolean addAll(Collection&lt;? extends E&gt; c) &#123; Object[] a = c.toArray(); // 将collection转换为数组类型 int numNew = a.length; // collection中的元素个数 ensureCapacityInternal(size + numNew); // 进行扩容检查 System.arraycopy(a, 0, elementData, size, numNew); // 将数组a[0,...,numNew-1]复制到数组elementData[size,...,size+numNew-1] size += numNew; return numNew != 0; &#125; // 按照指定collection集合的迭代器所返回的元素顺序，将该collection中的所有元素添加到列表的指定位置 public boolean addAll(int index, Collection&lt;? extends E&gt; c) &#123; rangeCheckForAdd(index); // 检查index是否越界 Object[] a = c.toArray(); int numNew = a.length; ensureCapacityInternal(size + numNew); // 进行扩容检查 // 将数组elementData[index,...,index+numMoved-1]复制到elementData[index+numMoved,...,index+2*numMoved-1] //将源数组中从index位置开始的后numMoved个元素统一后移numNew位 int numMoved = size - index; if (numMoved &gt; 0) System.arraycopy(elementData, index, elementData, index + numNew, numMoved); // 将数组a[0,...,numNew-1]复制到数组elementData[index,...,index+numNew-1]完成数据的插入 System.arraycopy(a, 0, elementData, index, numNew); size += numNew; return numNew != 0; &#125; 扩容相关// 用于自定义设置ArrayList的容量 public void ensureCapacity(int minCapacity) &#123; int minExpand = (elementData != DEFAULTCAPACITY_EMPTY_ELEMENTDATA) ? 0 : DEFAULT_CAPACITY; if (minCapacity &gt; minExpand) &#123; ensureExplicitCapacity(minCapacity); &#125; &#125; // 进行扩容检查 private void ensureCapacityInternal(int minCapacity) &#123; //第一次add操作初始化，如果为空ArrayList，那么初始化容量为10 if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123; minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); &#125; //判断是否需要扩容 ensureExplicitCapacity(minCapacity); &#125; //判断是否需要扩容 private void ensureExplicitCapacity(int minCapacity) &#123; //modCount这个参数运用到了 fail-fast 机制 modCount++; if (minCapacity - elementData.length &gt; 0) grow(minCapacity); // 扩容 &#125; // 扩容 private void grow(int minCapacity) &#123; int oldCapacity = elementData.length; //newCapacity为以前的1.5倍 int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; //判断容量是否到达long int 最大临界值 if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // 对数组进行复制处理 elementData = Arrays.copyOf(elementData, newCapacity); &#125; // 检查是否超过最大容量 0x7fffffff ，是否抛出异常 private static int hugeCapacity(int minCapacity) &#123; if (minCapacity &lt; 0) // overflow throw new OutOfMemoryError(); return (minCapacity &gt; MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE; &#125; 删除元素// 删除指定位置的元素 public E remove(int index) &#123; rangeCheck(index); //数组越界检查 modCount++; E oldValue = elementData(index); int numMoved = size - index - 1; //计算数组需要复制的数量 if (numMoved &gt; 0) //将index后的数据都向前移一位 System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; //help GC return oldValue; &#125; // 删除指定内容的元素（只删除第一个匹配成功的） public boolean remove(Object o) &#123; if (o == null) &#123; for (int index = 0; index &lt; size; index++) if (elementData[index] == null) &#123; fastRemove(index); return true; &#125; &#125; else &#123; for (int index = 0; index &lt; size; index++) if (o.equals(elementData[index])) &#123; fastRemove(index); return true; &#125; &#125; return false; &#125; //找到对应的元素后，删除。删除元素后的元素都向前移动一位 private void fastRemove(int index) &#123; modCount++; int numMoved = size - index - 1; if (numMoved &gt; 0) // 将index后面的元素整体向前移动一位 System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // help GC &#125; //清空ArrayList，将全部的元素设为null public void clear() &#123; modCount++; for (int i = 0; i &lt; size; i++) // help GC elementData[i] = null; size = 0; &#125; //删除ArrayList中从fromIndex到toIndex（区间--左闭右开）之间所有的元素 protected void removeRange(int fromIndex, int toIndex) &#123; modCount++; int numMoved = size - toIndex; //需向前移动的元素的个数 System.arraycopy(elementData, toIndex, elementData, fromIndex, numMoved); // help GC int newSize = size - (toIndex-fromIndex); for (int i = newSize; i &lt; size; i++) &#123; elementData[i] = null; &#125; size = newSize; &#125; //删除ArrayList中包含在指定容器c中的所有元素 public boolean removeAll(Collection&lt;?&gt; c) &#123; Objects.requireNonNull(c); //检查指定的对象c是否为空 return batchRemove(c, false); &#125; //移除ArrayList中不包含在指定容器c中的所有元素，与removeAll(Collection&lt;?&gt; c)正好相反 public boolean retainAll(Collection&lt;?&gt; c) &#123; Objects.requireNonNull(c); //检查指定的对象c是否为空 return batchRemove(c, true); &#125; // 根据complement的值删除元素 private boolean batchRemove(Collection&lt;?&gt; c, boolean complement) &#123; final Object[] elementData = this.elementData; int r = 0, w = 0; //读写双指针 w是重新存元素时的索引，r是原来的索引 boolean modified = false; try &#123; //遍历数组，并检查这个集合是否包含对应的值，移动要保留的值到数组前面，w最后值为要保留的元素的数量 //简单点：若保留，就将相同元素移动到前段；若删除，就将不同元素移动到前段 for (; r &lt; size; r++) if (c.contains(elementData[r]) == complement) //判断指定容器c中是否含有elementData[r]元素 elementData[w++] = elementData[r]; &#125;finally &#123;//确保异常抛出前的部分可以完成期望的操作，而未被遍历的部分会被接到后面 //r!=size表示可能出错了：c.contains(elementData[r])抛出异常 if (r != size) &#123; System.arraycopy(elementData, r,elementData, w,size - r); w += size - r; &#125; //如果w==size：表示全部元素都保留了，所以也就没有删除操作发生，所以会返回false；反之，返回true，并更改数组 //而w!=size的时候，即使try块抛出异常，也能正确处理异常抛出前的操作，因为w始终为要保留的前段部分的长度，数组也不会因此乱序 if (w != size) &#123; for (int i = w; i &lt; size; i++) elementData[i] = null; modCount += size - w;//改变的次数 size = w; //新的大小为保留的元素的个数 modified = true; &#125; &#125; return modified; &#125; removeAll和retainAll方法：实现删除或保留ArrayList中包含Collection c中的的元素。 这两个方法都用到batchRemove方法（boolean complement使得batchRemove方法得到了重用） 下面以removeAll为例，分析batchRemove(c, false)a. 遍历elementDatab. 如果集合c中包含elementData的元素e，即c.contains(elementData[r])为true，if不成立，if结束；如果c不包含elementData的元素e，则if成立，将此元素e赋值给elementData[w++] （即elementData保留了c中没有的元素，也就是删除了c中存在的所有元素。）c. 执行finallyd. finally是不管try中结果如何都会执行的。if(r!=size)，则将elementData未参加比较的元素arraycopy到elementData后面；新索引w加上刚arraycopy的数目；if (w != size)，此时w还不等于size，则将w后的元素移除.只有执行了if (w != size)（事实上只要c中含有elementData的元素，w肯定不等于size），才令modified = true，才说明remove成功，返回true，否则返回false。 ArrayList中还有一个用于节约数组内存空间，缩小容量的方法 // 因为容量常常会大于实际元素的数量。内存紧张时，可以调用该方法删除预留的位置，调整容量为元素实际数量。 // 如果确定不会再有元素添加进来时也可以调用该方法来节约空间 public void trimToSize() &#123; modCount++; // length是数组长度，size表示数组内元素个数 // size&lt;length那么就说明数组内有空元素，进行缩小容量操作 if (size &lt; elementData.length) &#123; elementData = (size == 0) ? EMPTY_ELEMENTDATA : Arrays.copyOf(elementData, size); &#125; &#125; 去掉预留元素的位置。返回一个新数组，新数组不含null，数组的size和elementData.length相等，以节省空间。此函数可避免size很小但elementData.length很大的情况。 ArrayList会每次增长会预申请多一点空间，1.5倍，这样就会出现当size() = 10的时候，ArrayList已经申请了15空间， trimToSize就是删除多余的5，只留10。 或许有人会有疑问：调用Arrays.copyOf复制size长度的元素到elementData，而且由源码看应该是从0复制到size处，那么如果我之前调用过add(int index, E element)呢？比如，list={1，2，3，null，null，4，null，null}，如果调用trimToSize返回的应该是list={1，2，3，null}（因为size=4）。其实上面这种情况不会发生的，因为调用add(int index, E element)时，会检查index的合法性，所以list的元素肯定是相邻的，而不会出现上述这种中间出现null的情况。 修改元素// 将指定位置的元素改为指定的值 public E set(int index, E element) &#123; rangeCheck(index); // 检查index是否越界 E oldValue = elementData(index); elementData[index] = element; return oldValue; &#125; 查找元素//判断ArrayList中是否包含Object(o) public boolean contains(Object o) &#123; return indexOf(o) &gt;= 0; &#125; //返回一个值在数组首次出现的位置，会根据是否为null使用不同方式判断。不存在就返回-1。时间复杂度为O(N) public int indexOf(Object o) &#123; if (o == null) &#123; for (int i = 0; i &lt; size; i++) if (elementData[i]==null) return i; &#125; else &#123; for (int i = 0; i &lt; size; i++) if (o.equals(elementData[i])) return i; &#125; return -1; &#125; //返回一个值在数组最后一次出现的位置，不存在就返回-1。时间复杂度为O(N) public int lastIndexOf(Object o) &#123; if (o == null) &#123; for (int i = size-1; i &gt;= 0; i--) if (elementData[i]==null) return i; &#125; else &#123; for (int i = size-1; i &gt;= 0; i--) if (o.equals(elementData[i])) return i; &#125; return -1; &#125; //返回指定位置的值，因为是数组，所以速度特别快 @SuppressWarnings(&quot;unchecked&quot;) E elementData(int index) &#123; return (E) elementData[index]; &#125; //返回指定位置的值，但是会检查这个位置数否超出数组长度 public E get(int index) &#123; rangeCheck(index); return elementData(index); //实质上return (E) elementData[index] &#125; 序列化//保存数组实例的状态到一个流（即它序列化）。写入过程数组被更改会抛出异常 private void writeObject(java.io.ObjectOutputStream s) throws java.io.IOException&#123; int expectedModCount = modCount; s.defaultWriteObject(); //执行默认的反序列化/序列化过程。将当前类的非静态和非瞬态字段写入此流 // 写入大小 s.writeInt(size); // 按顺序写入所有元素 for (int i=0; i&lt;size; i++) &#123; s.writeObject(elementData[i]); &#125; if (modCount != expectedModCount) &#123; throw new ConcurrentModificationException(); &#125; &#125; //上面是写，这个就是读了。 private void readObject(java.io.ObjectInputStream s) throws java.io.IOException, ClassNotFoundException &#123; elementData = EMPTY_ELEMENTDATA; // 执行默认的序列化/反序列化过程 s.defaultReadObject(); // 读入数组长度 s.readInt(); if (size &gt; 0) &#123; ensureCapacityInternal(size); Object[] a = elementData; //读入所有元素 for (int i=0; i&lt;size; i++) &#123; a[i] = s.readObject(); &#125; &#125; &#125; 为什么要自定义序列化、反序列化机制呢？ 保存元素的数组 elementData 使用 transient 修饰，该关键字声明数组默认不会被序列化。 transient Object[] elementData; // non-private to simplify nested class access 由于ArrayList实质上是一个动态数组，往往数组中会有空余的空间，如果采用默认的序列化机制，那些空余的空间会作为null写入本地文件或者在网络中传输，耗费了不必要的资源。所以，ArrayList使用自定义序列化机制，仅写入索引为【0，size）的有效元素以节省资源 序列化时需要使用 ObjectOutputStream 的 writeObject() 将对象转换为字节流并输出。而 writeObject() 方法在传入的对象存在 writeObject() 的时候会去反射调用该对象的 writeObject() 来实现序列化。反序列化使用的是 ObjectInputStream 的 readObject() 方法，原理类似。 ArrayList list = new ArrayList(); ObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream(file)); oos.writeObject(list); 迭代器//返回ListIterator，开始位置为指定参数 public ListIterator&lt;E&gt; listIterator(int index) &#123; if (index &lt; 0 || index &gt; size) throw new IndexOutOfBoundsException(&quot;Index: &quot;+index); return new ListItr(index); &#125; //返回ListIterator，开始位置为0 public ListIterator&lt;E&gt; listIterator() &#123; return new ListItr(0); &#125; //返回普通迭代器 public Iterator&lt;E&gt; iterator() &#123; return new Itr(); &#125; //通用的迭代器实现 private class Itr implements Iterator&lt;E&gt; &#123; int cursor; //游标，下一个元素的索引，默认初始化为0 int lastRet = -1; //上次访问的元素的位置 int expectedModCount = modCount;//迭代过程不允许修改数组，否则就抛出异常 //是否还有下一个 public boolean hasNext() &#123; return cursor != size; &#125; //下一个元素 @SuppressWarnings(&quot;unchecked&quot;) public E next() &#123; checkForComodification();//检查数组是否被修改 int i = cursor; if (i &gt;= size) throw new NoSuchElementException(); Object[] elementData = ArrayList.this.elementData; if (i &gt;= elementData.length) throw new ConcurrentModificationException(); cursor = i + 1; //向后移动游标 return (E) elementData[lastRet = i]; //设置访问的位置并返回这个值 &#125; //删除元素 public void remove() &#123; if (lastRet &lt; 0) throw new IllegalStateException(); checkForComodification();//检查数组是否被修改 try &#123; ArrayList.this.remove(lastRet); cursor = lastRet; lastRet = -1; expectedModCount = modCount; &#125; catch (IndexOutOfBoundsException ex) &#123; throw new ConcurrentModificationException(); &#125; &#125; @Override @SuppressWarnings(&quot;unchecked&quot;) public void forEachRemaining(Consumer&lt;? super E&gt; consumer) &#123; Objects.requireNonNull(consumer); final int size = ArrayList.this.size; int i = cursor; if (i &gt;= size) &#123; return; &#125; final Object[] elementData = ArrayList.this.elementData; if (i &gt;= elementData.length) &#123; throw new ConcurrentModificationException(); &#125; while (i != size &amp;&amp; modCount == expectedModCount) &#123; consumer.accept((E) elementData[i++]); &#125; cursor = i; lastRet = i - 1; checkForComodification(); &#125; //检查数组是否被修改 final void checkForComodification() &#123; if (modCount != expectedModCount) throw new ConcurrentModificationException(); &#125; &#125; //ListIterator迭代器实现 private class ListItr extends Itr implements ListIterator&lt;E&gt; &#123; ListItr(int index) &#123; super(); cursor = index; &#125; public boolean hasPrevious() &#123; return cursor != 0; &#125; public int nextIndex() &#123; return cursor; &#125; public int previousIndex() &#123; return cursor - 1; &#125; @SuppressWarnings(&quot;unchecked&quot;) public E previous() &#123; checkForComodification(); int i = cursor - 1; if (i &lt; 0) throw new NoSuchElementException(); Object[] elementData = ArrayList.this.elementData; if (i &gt;= elementData.length) throw new ConcurrentModificationException(); cursor = i; return (E) elementData[lastRet = i]; &#125; public void set(E e) &#123; if (lastRet &lt; 0) throw new IllegalStateException(); checkForComodification(); try &#123; ArrayList.this.set(lastRet, e); &#125; catch (IndexOutOfBoundsException ex) &#123; throw new ConcurrentModificationException(); &#125; &#125; public void add(E e) &#123; checkForComodification(); try &#123; int i = cursor; ArrayList.this.add(i, e); cursor = i + 1; lastRet = -1; expectedModCount = modCount; &#125; catch (IndexOutOfBoundsException ex) &#123; throw new ConcurrentModificationException(); &#125; &#125; &#125; Iterator与ListIterator的区别： Iterator可以应用于所有的集合，Set、List和Map和这些集合的子类型。而ListIterator只能用于List及其子类型； Iterator只能实现顺序向后遍历，ListIterator可实现顺序向后遍历和逆向（顺序向前）遍历； Iterator只能实现remove操作，ListIterator可以实现remove操作，add操作，set操作。 其他方法//返回ArrayList的大小（元素个数） public int size() &#123; return size; &#125; //判断ArrayList是否为空 public boolean isEmpty() &#123; return size == 0; &#125; //返回此 ArrayList实例的浅拷贝（元素本身没有被复制，复制过程数组发生改变会抛出异常） public Object clone() &#123; try &#123; ArrayList&lt;?&gt; v = (ArrayList&lt;?&gt;) super.clone(); v.elementData = Arrays.copyOf(elementData, size); v.modCount = 0; return v; &#125; catch (CloneNotSupportedException e) &#123; throw new InternalError(e); &#125; &#125; /* 浅克隆就是我们所看到的Arrays.copyOf， System.arraycopy，数组是新的，但是里面N个元素全是引用的旧的。 浅拷贝(影子克隆):只复制基本类型。 深拷贝(深度克隆):基本类+对象。 */ //返回一个包含ArrayList中所有元素的数组 public Object[] toArray() &#123; return Arrays.copyOf(elementData, size); &#125; // 返回一个数组，使用运行时确定类型，该数组包含在这个列表中的所有元素（从第一到最后一个元素） // 返回的数组容量由参数和本数组中较大值确定 @SuppressWarnings(&quot;unchecked&quot;) public &lt;T&gt; T[] toArray(T[] a) &#123; if (a.length &lt; size) // Make a new array of a&#39;s runtime type, but my contents: return (T[]) Arrays.copyOf(elementData, size, a.getClass()); System.arraycopy(elementData, 0, a, 0, size); if (a.length &gt; size) a[size] = null; return a; &#125; ArrayList相关问题Integer.MAX_VALUE - 8 这里为什么要减去8？主要是考虑到不同的JVM,有的VM会在加入一些数据头,当扩容后的容量大于MAX_ARRAY_SIZE,我们会将最小需要容量和MAX_ARRAY_SIZE做比较,如果比它大, 只能取Integer.MAX_VALUE,否则是Integer.MAX_VALUE -8。这个是从jdk1.7开始才有的 jdk1.8的无参构造函数和之前版本的构造函数有什么区别?jdk1.6 public ArrayList() &#123; this(10); &#125; jdk1.7 public ArrayList() &#123; super(); this.elementData = EMPTY_ELEMENTDATA; &#125; jdk1.8 public ArrayList() &#123; this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA; &#125; 对比下可以看出：jdk1.6的无参构造方法（默认构造方法）构造的ArrayList的底层数组elementData大小（容量）默认为10；从1.7开始，无参构造方法构造的ArrayList的底层数组elementData大小默认为0。 java集合类在jdk1.7版本基本上都有一种改动：懒初始化。懒初始化指的是默认构造方法构造的集合类，占据尽可能少的内存空间（对于ArrayList来说，使用空数组来占据尽量少的空间，不使用null是为了避免null判断），在第一次进行包含有添加语义的操作时，才进行真正的初始化工作。 1.7开始的ArrayList，默认构造方法构造的实例，底层数组是空数组，容量为0，在进行第一次add/addAll等操作时才会真正给底层数组赋非empty的值。如果add/addAll添加的元素小于10，则把elementData数组扩容为10个元素大小，否则使用刚好合适的大小（例如，第一次addAll添加6个，那么扩容为10个，第一次添加大于10个的，比如24个，扩容为24个，刚好合适） 1.8版本，默认构造的实例这个行为没有改变，只是用的数组名字变了。 jdk1.6中扩容算法的缺陷（由于jdk1.7和jdk1.8在扩容算法方面差别不大，所以下面没有严格区分） jdk1.6 public void ensureCapacity(int minCapacity) &#123; modCount++; int oldCapacity = elementData.length; if (minCapacity &gt; oldCapacity) &#123; Object oldData[] = elementData; int newCapacity = (oldCapacity * 3)/2 + 1; if (newCapacity &lt; minCapacity) newCapacity = minCapacity; // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity); &#125; &#125; 从上面的代码可以看出jdk1.6的ensureCapacity方法只是简单进行了逻辑上的操作，没有过多考虑int型溢出的问题，从1.7开始对这个进行了完善。 而且没考虑入参minCapacity可能因为int溢出变为负数。这个方法可以外部手动调用，手动扩容传入负数这个肯定是应该拦截掉的。但是自动扩容会因为int溢出产生负数，碰到这种情况时应该特殊处理，而不是什么都不做，等着后面抛出一个ArrayIndexOutOfBoundsException。 还有就是下面这句代码会造成过早溢出 int newCapacity = (oldCapacity * 3)/2 + 1; 虽然上面这行代码和1.7开始的oldCapacity + (oldCapacity &gt;&gt; 1) 差不多，都是相当于1.5倍，但实际上是有区别的。 这里主要有两个区别 第一个区别是jdk1.6的乘除运算的数学结果比后面一个大1比如oldCapacity=10，1.6的算法得到16，1.7开始的算法得到15，这个影响不大； 第二个区别就是两者在数字比较大时运算结果不一样，比如oldCapacity=10^9，这个数和Integer.MAX_VALUE位数一样，用1.6的算法得到的会是错误的-647483647，用1.7的则是正确的1500000000，这时候明明可以1.5倍扩容，但是jdk1.6却用的是按需扩容。 ensureCapacity（称之为手动，是因为此方法是public的，可以外部手动调用）。在1.6版本是只有这个手动的方法，内部自动操作也是调用这个方法，1.7开始进行了区分，并且进一步改进了扩容操作。 从1.7开始将内部扩容和外部可以调用的扩容方法分开了，通过源码可以看出：外部调用的手动扩容方法ensureCapacity要多一个判断条件 minCapacity &gt; minExpand，这个判断条件拦截掉负数的minCapacity，这样调用内部扩容ensureCapacityInternal方法时，minCapacity一定是正数；内部扩容方法直接就用minCapacity - elementData.length &gt; 0判断，此条件可以检测出int型溢出，碰到溢出最后会抛出一个OOM错误。jdk1.7用OOM，这比jdk1.6用ArrayIndexOutOfBoundsException更好，因为此时数组大小超出了虚拟机对数组的限制，虚拟机无法处理这种情况了，抛出一个ERROR是合理的。 使用这行代码 newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); 这行代码不仅仅是使用位运算加快执行速度，上面说了，这种做法才是对的，是真正的1.5倍。不仅仅因为那一个大小的差别，更重要的是避免过早出现int溢出的情况，保证了内部自动扩容会尽量按规定的策略执行。同时整个扩容处理流程中多增加了几处if判断，对各种情况处理更加完善。 为什么ArrayList自动容量扩充选择扩充1.5倍？这种算法构造出来的新的数组长度的增量都会比上一次大( 而且是越来越大) ，避免频繁newInstance 的情况。 为什么ArrayList 不适合频繁插入和删除操作？由上面分析的增加删除方法可以看出在ArrayList中经常会调用 System.arraycopy 这个效率很低的操作来复制数组，所以导致ArrayList在插入和删除操作中效率不高。 RandomAccess接口public interface RandomAccess &#123; &#125; 查看源码我们发现实际上 RandomAccess 接口中什么都没有定义。所以，在我看来 RandomAccess 接口不过是一个标识罢了。标识什么？ 标识实现这个接口的类具有随机访问功能。 在 binarySearch（) 方法中，它要判断传入的 list 是否 RamdomAccess 的实例，如果是，调用indexedBinarySearch()方法，如果不是，那么调用iteratorBinarySearch()方法 public static &lt;T&gt; int binarySearch(List&lt;? extends Comparable&lt;? super T&gt;&gt; list, T key) &#123; if (list instanceof RandomAccess || list.size()&lt;BINARYSEARCH_THRESHOLD) return Collections.indexedBinarySearch(list, key); else return Collections.iteratorBinarySearch(list, key); &#125; ArrayList 实现了 RandomAccess 接口， 而 LinkedList 没有实现。为什么呢？我觉得还是和底层数据结构有关！ArrayList 底层是数组，而 LinkedList 底层是链表。数组天然支持随机访问，时间复杂度为 O(1)，所以称为快速随机访问。链表需要遍历到特定位置才能访问特定位置的元素，时间复杂度为 O(n)，所以不支持快速随机访问。，ArrayList 实现了 RandomAccess 接口，就表明了他具有快速随机访问功能。 RandomAccess 接口只是标识，并不是说 ArrayList 实现 RandomAccess 接口才具有快速随机访问功能的！","categories":[{"name":"Java","slug":"Java","permalink":"https://gyl-coder.top/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://gyl-coder.top/tags/Java/"},{"name":"集合","slug":"集合","permalink":"https://gyl-coder.top/tags/%E9%9B%86%E5%90%88/"},{"name":"ArrayList","slug":"ArrayList","permalink":"https://gyl-coder.top/tags/ArrayList/"}],"author":{"name":"yanliang","avatar":"https://cdn.jsdelivr.net/gh/gyl-coder/blogImgs/images/touxiang.webp","url":"https://gyl-coder.top"}},{"title":"手写实现IOC 和 AOP","slug":"spring/custom-ioc-aop","date":"2020-04-03T00:00:00.000Z","updated":"2020-12-27T04:47:15.978Z","comments":true,"path":"spring/custom-ioc-aop/","link":"","permalink":"https://gyl-coder.top/spring/custom-ioc-aop/","excerpt":"通过上一篇 IOC &amp; AOP 详解 我们了解了 IOC 和 AOP 这两个思想，下面我们先不去考虑Spring是如何实现这两个思想的，先通过一个 银行转账 的案例，分析一下该案例在代码层面存在什么问题？分析之后使用我们已有的知识来解决这些问题（痛点）。 其实这个过程就是在一步步分析并手动实现 IOC 和 AOP 。","text":"通过上一篇 IOC &amp; AOP 详解 我们了解了 IOC 和 AOP 这两个思想，下面我们先不去考虑Spring是如何实现这两个思想的，先通过一个 银行转账 的案例，分析一下该案例在代码层面存在什么问题？分析之后使用我们已有的知识来解决这些问题（痛点）。 其实这个过程就是在一步步分析并手动实现 IOC 和 AOP 。 案例介绍银行转账：账户A向账户B转账（账户A减钱，账户B加钱）。为了简单起见，在前端页面中写死了两个账户。每次只需要输入转账金额，进行转账操作，验证功能即可。 案例表结构name varcher 255 用户名 money int 255 账户金额 cardNo varcher 255 银行卡号 案例代码调用关系 核心代码@WebServlet(name=&quot;transferServlet&quot;,urlPatterns = &quot;/transferServlet&quot;) public class TransferServlet extends HttpServlet &#123; // 1. 实例化service层对象 private TransferService transferService = new TransferServiceImpl(); @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; doPost(req,resp); &#125; @Override protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; // 设置请求体的字符编码 req.setCharacterEncoding(&quot;UTF-8&quot;); String fromCardNo = req.getParameter(&quot;fromCardNo&quot;); String toCardNo = req.getParameter(&quot;toCardNo&quot;); String moneyStr = req.getParameter(&quot;money&quot;); int money = Integer.parseInt(moneyStr); Result result = new Result(); try &#123; // 2. 调用service层方法 transferService.transfer(fromCardNo,toCardNo,money); result.setStatus(&quot;200&quot;); &#125; catch (Exception e) &#123; e.printStackTrace(); result.setStatus(&quot;201&quot;); result.setMessage(e.toString()); &#125; // 响应 resp.setContentType(&quot;application/json;charset=utf-8&quot;); resp.getWriter().print(JsonUtils.object2Json(result)); &#125; &#125; public interface TransferService &#123; void transfer(String fromCardNo, String toCardNo, int money) throws Exception; &#125; public class TransferServiceImpl implements TransferService &#123; private AccountDao accountDao = new JdbcAccountDaoImpl(); @Override public void transfer(String fromCardNo, String toCardNo, int money) throws Exception &#123; Account from = accountDao.queryAccountByCardNo(fromCardNo); Account to = accountDao.queryAccountByCardNo(toCardNo); from.setMoney(from.getMoney()-money); to.setMoney(to.getMoney()+money); accountDao.updateAccountByCardNo(to); accountDao.updateAccountByCardNo(from); &#125; &#125; public interface AccountDao &#123; /** * 通过卡号查询账户 * @param cardNo * @return * @throws Exception */ Account queryAccountByCardNo(String cardNo) throws Exception; /** * 更新账户信息 * @param account * @return * @throws Exception */ int updateAccountByCardNo(Account account) throws Exception; &#125; public class JdbcAccountDaoImpl implements AccountDao &#123; @Override public Account queryAccountByCardNo(String cardNo) throws Exception &#123; //从连接池获取连接 Connection con = DruidUtils.getInstance().getConnection(); String sql = &quot;select * from account where cardNo=?&quot;; PreparedStatement preparedStatement = con.prepareStatement(sql); preparedStatement.setString(1,cardNo); ResultSet resultSet = preparedStatement.executeQuery(); Account account = new Account(); while(resultSet.next()) &#123; account.setCardNo(resultSet.getString(&quot;cardNo&quot;)); account.setName(resultSet.getString(&quot;name&quot;)); account.setMoney(resultSet.getInt(&quot;money&quot;)); &#125; resultSet.close(); preparedStatement.close(); con.close(); return account; &#125; @Override public int updateAccountByCardNo(Account account) throws Exception &#123; // 从连接池获取连接 Connection con = DruidUtils.getInstance().getConnection(); String sql = &quot;update account set money=? where cardNo=?&quot;; PreparedStatement preparedStatement = con.prepareStatement(sql); preparedStatement.setInt(1,account.getMoney()); preparedStatement.setString(2,account.getCardNo()); int i = preparedStatement.executeUpdate(); preparedStatement.close(); con.close(); return i; &#125; &#125; 案例问题分析 通过上面的流程分析以及简要代码，我们可以发现如下问题： 问题一： new 关键字将 service 层的实现类 TransferServiceImpl 和 Dao 层的具体实现类 JdbcAccountDaoImpl 耦合在了一起，当需要切换Dao层实现类的时候必须要修改 service 的代码、重新编译，这样不符合面向接口开发的最优原则。 问题二： service 层没有事务控制，如果转账过程中出现异常可能会导致数据错乱，后果很严重，尤其是在金融银行领域。 问题解决思路new关键字耦合问题解决方案实例化对象的方式处理 new 之外，还有什么技术？ 答：反射（将类的权限定类名配置在xml文件中） 项目中往往有很多对象需要实例化，考虑使用工程模式通过反射来实例化对象。（工厂模式是解耦合非常好的一种方式） 代码中能否只声明所需实例的接口类型，不出现new关键字，也不出现工厂类的字眼？ 答：可以，声明一个变量并提供一个set方法，在反射的时候将所需要的对象注入进去。 public class TransferServiceImpl implements TransferService &#123; private AccountDao accountDao; public void setAccountDao(AccountDao accountDao) &#123; this.accountDao = accountDao; &#125; &#125; new关键字耦合问题代码改造首先定义 bean.xml 文件 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt; &lt;!--跟标签beans，里面配置一个又一个的bean子标签，每一个bean子标签都代表一个类的配置--&gt; &lt;beans&gt; &lt;!--id标识对象，class是类的全限定类名--&gt; &lt;bean id=&quot;accountDao&quot; class=&quot;com.yanliang.dao.impl.JdbcAccountDaoImpl&quot;&gt; &lt;property name=&quot;ConnectionUtils&quot; ref=&quot;connectionUtils&quot;/&gt; &lt;/bean&gt; &lt;bean id=&quot;transferService&quot; class=&quot;com.yanliang.service.impl.TransferServiceImpl&quot;&gt; &lt;!--set+ name 之后锁定到传值的set方法了，通过反射技术可以调用该方法传入对应的值--&gt; &lt;property name=&quot;AccountDao&quot; ref=&quot;accountDao&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;/beans&gt; 定义BeanFactory /** * 工厂类，生产对象（使用反射技术） * 任务一：读取解析xml，通过反射技术实例化对象并且存储待用（map集合） * 任务二：对外提供获取实例对象的接口（根据id获取） */ public class BeanFactory &#123; private static Map&lt;String,Object&gt; map = new HashMap&lt;&gt;(); // 存储对象 /** * 读取解析xml，通过反射技术实例化对象并且存储待用（map集合） */ static &#123; // 加载xml InputStream resourceAsStream = BeanFactory.class.getClassLoader().getResourceAsStream(&quot;beans.xml&quot;); // 解析xml SAXReader saxReader = new SAXReader(); try &#123; Document document = saxReader.read(resourceAsStream); // 获取根元素 Element rootElement = document.getRootElement(); List&lt;Element&gt; beanList = rootElement.selectNodes(&quot;//bean&quot;); for (int i = 0; i &lt; beanList.size(); i++) &#123; Element element = beanList.get(i); // 处理每个bean元素，获取到该元素的id 和 class 属性 String id = element.attributeValue(&quot;id&quot;); // accountDao String clazz = element.attributeValue(&quot;class&quot;); // com.yanliang.dao.impl.JdbcAccountDaoImpl // 通过反射技术实例化对象 Class&lt;?&gt; aClass = Class.forName(clazz); Object o = aClass.newInstance(); // 实例化之后的对象 // 存储到map中待用 map.put(id,o); &#125; // 实例化完成之后维护对象的依赖关系，检查哪些对象需要传值进入，根据它的配置，我们传入相应的值 // 有property子元素的bean就有传值需求 List&lt;Element&gt; propertyList = rootElement.selectNodes(&quot;//property&quot;); // 解析property，获取父元素 for (int i = 0; i &lt; propertyList.size(); i++) &#123; Element element = propertyList.get(i); //&lt;property name=&quot;AccountDao&quot; ref=&quot;accountDao&quot;&gt;&lt;/property&gt; String name = element.attributeValue(&quot;name&quot;); String ref = element.attributeValue(&quot;ref&quot;); // 找到当前需要被处理依赖关系的bean Element parent = element.getParent(); // 调用父元素对象的反射功能 String parentId = parent.attributeValue(&quot;id&quot;); Object parentObject = map.get(parentId); // 遍历父对象中的所有方法，找到&quot;set&quot; + name Method[] methods = parentObject.getClass().getMethods(); for (int j = 0; j &lt; methods.length; j++) &#123; Method method = methods[j]; if(method.getName().equalsIgnoreCase(&quot;set&quot; + name)) &#123; // 该方法就是 setAccountDao(AccountDao accountDao) method.invoke(parentObject,map.get(ref)); &#125; &#125; // 把处理之后的parentObject重新放到map中 map.put(parentId,parentObject); &#125; &#125; catch (DocumentException e) &#123; e.printStackTrace(); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IllegalAccessException e) &#123; e.printStackTrace(); &#125; catch (InstantiationException e) &#123; e.printStackTrace(); &#125; catch (InvocationTargetException e) &#123; e.printStackTrace(); &#125; &#125; /** * 对外提供获取实例对象的接口（根据id获取） * @param id * @return */ public static Object getBean(String id) &#123; return map.get(id); &#125; &#125; 对象的实例化工作交给BeanFactory来进行之后，我们再具体使用是就可以像如下这样了： @WebServlet(name=&quot;transferServlet&quot;,urlPatterns = &quot;/transferServlet&quot;) public class TransferServlet extends HttpServlet &#123; // // 1. 实例化service层对象 // private TransferService transferService = new TransferServiceImpl(); // 改造为通过Bean工程获取service层对象 private TransferService transferService = (TransferService) BeanFactory.getBean(&quot;transferService&quot;); public class TransferServiceImpl implements TransferService &#123; // private AccountDao accountDao = new JdbcAccountDaoImpl(); // // 改造为通过Bean工厂获取对象 // private AccountDao accountDao = (AccountDao) BeanFactory.getBean(&quot;accountDao&quot;); // 最佳状态 private AccountDao accountDao; public void setAccountDao(AccountDao accountDao) &#123; this.accountDao = accountDao; &#125; 事务控制问题分析在转账的业务代码中手动模拟转账异常，来验证一下。在两个账户的转入和转出之间模拟一个分母为0的异常。 accountDao.updateAccountByCardNo(to); int i = 1/0; accountDao.updateAccountByCardNo(from); 然后启动程序，点击转账（李大雷 向 韩梅梅转 100 ￥）之后，会出现如下错误。 这时我们再查看数据库 发现 韩梅梅 的账户增加了100￥，但是李大雷的账户并没有减少（两个账户原本都有10000￥）。 出现这个问题的原因就是因为Service层没有事务控制的功能，在转账过程中出现错误（转入和转出之间出现异常，转入已经完成，转出没有进行）这事就会造成上面的问题。 数据库的事务问题归根结底是 Connection 的事务 connection.commit() 提交事务 connection.rollback() 回滚事务 在上面银行转账的案例中，两次update操作使用的是两个数据库连接，这样的话，肯定就不属于同一个事务控制了。 解决思路： 通过上面的分析，我们得出问题的原因是两次update使用了两个不同的connection连接。那么要想解决这个问题，我们就需要让两次update使用同一个connection连接 两次update属于同一个线程内的执行调用，我们可以给当前线程绑定一个Connection，和当前线程有关系的数据库操作都去使用这个connection（从当前线程中获取，第一次使用连接，发现当前线程没有，就从连接池获取一个连接绑定到当前线程） 另一方面，目前事务控制是在Dao层进行的（connection），我们需要将事务控制提到service层（service层才是具体执行业务逻辑的地方，这里可能会调用多个dao层的方法，我们需要对service层的方法进行整体的事务控制）。 有了上面两个思路，下面我们进行代码修改。 事务控制代码修改增加 ConnectionUtils 工具类 /** * 获取连接，并将连接与线程绑定 */ public class ConnectionUtils &#123; private ThreadLocal&lt;Connection&gt; threadLocal = new ThreadLocal&lt;&gt;(); // 存储当前线程的连接 /** * 从当前线程获取连接 */ public Connection getCurrentThreadConn() throws SQLException &#123; /** * 判断当前线程中是否已经绑定连接，如果没有绑定，需要从连接池获取一个连接绑定到当前线程 */ Connection connection = threadLocal.get(); if(connection == null) &#123; // 从连接池拿连接并绑定到线程 connection = DruidUtils.getInstance().getConnection(); // 绑定到当前线程 threadLocal.set(connection); &#125; return connection; &#125; &#125; 增加 TransactionManager 事务管理类 /** * 事务管理器类：负责手动事务的开启、提交、回滚 */ public class TransactionManager &#123; private ConnectionUtils connectionUtils; public void setConnectionUtils(ConnectionUtils connectionUtils) &#123; this.connectionUtils = connectionUtils; &#125; // 开启手动事务控制 public void beginTransaction() throws SQLException &#123; connectionUtils.getCurrentThreadConn().setAutoCommit(false); &#125; // 提交事务 public void commit() throws SQLException &#123; connectionUtils.getCurrentThreadConn().commit(); &#125; // 回滚事务 public void rollback() throws SQLException &#123; connectionUtils.getCurrentThreadConn().rollback(); &#125; &#125; 增加代理工厂 ProxyFactory /** * 代理对象工厂：生成代理对象的 */ public class ProxyFactory &#123; private TransactionManager transactionManager; public void setTransactionManager(TransactionManager transactionManager) &#123; this.transactionManager = transactionManager; &#125; /** * Jdk动态代理 * @param obj 委托对象 * @return 代理对象 */ public Object getJdkProxy(Object obj) &#123; // 获取代理对象 return Proxy.newProxyInstance(obj.getClass().getClassLoader(), obj.getClass().getInterfaces(), new InvocationHandler() &#123; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; Object result = null; try&#123; // 开启事务(关闭事务的自动提交) transactionManager.beginTransaction(); result = method.invoke(obj,args); // 提交事务 transactionManager.commit(); &#125;catch (Exception e) &#123; e.printStackTrace(); // 回滚事务 transactionManager.rollback(); // 抛出异常便于上层servlet捕获 throw e; &#125; return result; &#125; &#125;); &#125; /** * 使用cglib动态代理生成代理对象 * @param obj 委托对象 * @return */ public Object getCglibProxy(Object obj) &#123; return Enhancer.create(obj.getClass(), new MethodInterceptor() &#123; @Override public Object intercept(Object o, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable &#123; Object result = null; try&#123; // 开启事务(关闭事务的自动提交) transactionManager.beginTransaction(); result = method.invoke(obj,objects); // 提交事务 transactionManager.commit(); &#125;catch (Exception e) &#123; e.printStackTrace(); // 回滚事务 transactionManager.rollback(); // 抛出异常便于上层servlet捕获 throw e; &#125; return result; &#125; &#125;); &#125; &#125; 修改beans.xml文件 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt; &lt;!--跟标签beans，里面配置一个又一个的bean子标签，每一个bean子标签都代表一个类的配置--&gt; &lt;beans&gt; &lt;!--id标识对象，class是类的全限定类名--&gt; &lt;bean id=&quot;accountDao&quot; class=&quot;com.yanliang.dao.impl.JdbcAccountDaoImpl&quot;&gt; &lt;property name=&quot;ConnectionUtils&quot; ref=&quot;connectionUtils&quot;/&gt; &lt;/bean&gt; &lt;bean id=&quot;transferService&quot; class=&quot;com.yanliang.service.impl.TransferServiceImpl&quot;&gt; &lt;!--set+ name 之后锁定到传值的set方法了，通过反射技术可以调用该方法传入对应的值--&gt; &lt;property name=&quot;AccountDao&quot; ref=&quot;accountDao&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!--配置新增的三个Bean--&gt; &lt;bean id=&quot;connectionUtils&quot; class=&quot;com.yanliang.utils.ConnectionUtils&quot;&gt;&lt;/bean&gt; &lt;!--事务管理器--&gt; &lt;bean id=&quot;transactionManager&quot; class=&quot;com.yanliang.utils.TransactionManager&quot;&gt; &lt;property name=&quot;ConnectionUtils&quot; ref=&quot;connectionUtils&quot;/&gt; &lt;/bean&gt; &lt;!--代理对象工厂--&gt; &lt;bean id=&quot;proxyFactory&quot; class=&quot;com.yanliang.factory.ProxyFactory&quot;&gt; &lt;property name=&quot;TransactionManager&quot; ref=&quot;transactionManager&quot;/&gt; &lt;/bean&gt; &lt;/beans&gt; 修改 JdbcAccountDaoImpl的实现 public class JdbcAccountDaoImpl implements AccountDao &#123; private ConnectionUtils connectionUtils; public void setConnectionUtils(ConnectionUtils connectionUtils) &#123; this.connectionUtils = connectionUtils; &#125; @Override public Account queryAccountByCardNo(String cardNo) throws Exception &#123; //从连接池获取连接 // Connection con = DruidUtils.getInstance().getConnection(); // 改造为：从当前线程当中获取绑定的connection连接 Connection con = connectionUtils.getCurrentThreadConn(); String sql = &quot;select * from account where cardNo=?&quot;; PreparedStatement preparedStatement = con.prepareStatement(sql); preparedStatement.setString(1,cardNo); ResultSet resultSet = preparedStatement.executeQuery(); Account account = new Account(); while(resultSet.next()) &#123; account.setCardNo(resultSet.getString(&quot;cardNo&quot;)); account.setName(resultSet.getString(&quot;name&quot;)); account.setMoney(resultSet.getInt(&quot;money&quot;)); &#125; resultSet.close(); preparedStatement.close(); // con.close(); return account; &#125; @Override public int updateAccountByCardNo(Account account) throws Exception &#123; // 从连接池获取连接 // Connection con = DruidUtils.getInstance().getConnection(); // 改造为：从当前线程当中获取绑定的connection连接 Connection con = connectionUtils.getCurrentThreadConn(); String sql = &quot;update account set money=? where cardNo=?&quot;; PreparedStatement preparedStatement = con.prepareStatement(sql); preparedStatement.setInt(1,account.getMoney()); preparedStatement.setString(2,account.getCardNo()); int i = preparedStatement.executeUpdate(); preparedStatement.close(); // con.close(); return i; &#125; &#125; 修改 TransferServlet @WebServlet(name=&quot;transferServlet&quot;,urlPatterns = &quot;/transferServlet&quot;) public class TransferServlet extends HttpServlet &#123; // // 1. 实例化service层对象 // private TransferService transferService = new TransferServiceImpl(); // 改造为通过Bean工程获取service层对象 // private TransferService transferService = (TransferService) BeanFactory.getBean(&quot;transferService&quot;); // 从工程获取委托对象（委托对象增强了事务控制的功能） private ProxyFactory proxyFactory = (ProxyFactory) BeanFactory.getBean(&quot;proxyFactory&quot;); private TransferService transferService = (TransferService) proxyFactory.getProxy(BeanFactory.getBean(&quot;transferService&quot;)) ; @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; doPost(req,resp); &#125; @Override protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; // 设置请求体的字符编码 req.setCharacterEncoding(&quot;UTF-8&quot;); String fromCardNo = req.getParameter(&quot;fromCardNo&quot;); String toCardNo = req.getParameter(&quot;toCardNo&quot;); String moneyStr = req.getParameter(&quot;money&quot;); int money = Integer.parseInt(moneyStr); Result result = new Result(); try &#123; // 2. 调用service层方法 transferService.transfer(fromCardNo,toCardNo,money); result.setStatus(&quot;200&quot;); &#125; catch (Exception e) &#123; e.printStackTrace(); result.setStatus(&quot;201&quot;); result.setMessage(e.toString()); &#125; // 响应 resp.setContentType(&quot;application/json;charset=utf-8&quot;); resp.getWriter().print(JsonUtils.object2Json(result)); &#125; &#125; 改造完之后，我们再次进行测试，这时会发现当转账过程中出现错误是，事务能够成功的被控制住（转出账户不会少钱，转入账户不会多钱）。 为什么要使用代理的方式来实现事务控制？这里我们可以考虑一个问题，为什么要使用代理的方式来实现事务控制？ 如果没有使用代理的方式，我们要向实现事务控制这需要将，事务控制的相关代码写在service层的TransferServiceImpl 具体实现中。 public class TransferServiceImpl implements TransferService &#123; // 最佳状态 private AccountDao accountDao; // 构造函数传值/set方法传值 public void setAccountDao(AccountDao accountDao) &#123; this.accountDao = accountDao; &#125; @Override public void transfer(String fromCardNo, String toCardNo, int money) throws Exception &#123; try&#123; // 开启事务(关闭事务的自动提交) TransactionManager.getInstance().beginTransaction();*/ Account from = accountDao.queryAccountByCardNo(fromCardNo); Account to = accountDao.queryAccountByCardNo(toCardNo); from.setMoney(from.getMoney()-money); to.setMoney(to.getMoney()+money); accountDao.updateAccountByCardNo(to); // 模拟异常 int c = 1/0; accountDao.updateAccountByCardNo(from); // 提交事务 TransactionManager.getInstance().commit(); &#125;catch (Exception e) &#123; e.printStackTrace(); // 回滚事务 TransactionManager.getInstance().rollback(); // 抛出异常便于上层servlet捕获 throw e; &#125; &#125; &#125; 这样的话，事务控制和具体的业务代码就耦合在了一起，如果有多个方法都需要实现事务控制的功能，我们需要在每个业务方法是都添加上这些代码。这样将会出现大量的重复代码。所以这里使用了 AOP 的思想通过动态代理的方式实现了事务控制。 下载源码https://github.com/gyl-coder/ISpring-IOC-AOP/tree/feat-init","categories":[{"name":"Spring","slug":"Spring","permalink":"https://gyl-coder.top/categories/Spring/"}],"tags":[{"name":"动态代理","slug":"动态代理","permalink":"https://gyl-coder.top/tags/%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86/"},{"name":"Spring","slug":"Spring","permalink":"https://gyl-coder.top/tags/Spring/"},{"name":"IOC","slug":"IOC","permalink":"https://gyl-coder.top/tags/IOC/"},{"name":"AOP","slug":"AOP","permalink":"https://gyl-coder.top/tags/AOP/"}],"author":{"name":"yanliang","avatar":"https://cdn.jsdelivr.net/gh/gyl-coder/blogImgs/images/touxiang.webp","url":"https://gyl-coder.github.io"}},{"title":"动态代理原理剖析","slug":"java/dynamic-proxy","date":"2020-04-02T00:00:00.000Z","updated":"2020-12-27T04:47:15.974Z","comments":true,"path":"java/dynamic-proxy/","link":"","permalink":"https://gyl-coder.top/java/dynamic-proxy/","excerpt":"动态代理的常用实现方式是反射。反射机制是 Java 语言提供的一种基础功能，赋予程序在运行时自省（introspect，官方用语）的能力。通过反射我们可以直接操作类或者对象，比如获取某个对象的类定义，获取类声明的属性和方法，调用方法或者构造对象，甚至可以运行时修改类定义。 动态代理是一种方便运行时动态构建代理、动态处理代理方法调用的机制，很多场景都是利用类似机制做到的，比如用来包装 RPC 调用、面向切面的编程（AOP）。 JDK 自身提供的动态代理，就是主要利用了上面提到的反射机制。但动态代理不止有反射一种实现方式，还有其他的实现方式，比如利用传说中更高性能的字节码操作机制，类似 ASM、cglib（基于 ASM，一个 Java 字节码操作框架）、Javassist 等。简单来说，动态代理是一种行为方式，而反射或 ASM 只是它的一种实现手段而已。","text":"动态代理的常用实现方式是反射。反射机制是 Java 语言提供的一种基础功能，赋予程序在运行时自省（introspect，官方用语）的能力。通过反射我们可以直接操作类或者对象，比如获取某个对象的类定义，获取类声明的属性和方法，调用方法或者构造对象，甚至可以运行时修改类定义。 动态代理是一种方便运行时动态构建代理、动态处理代理方法调用的机制，很多场景都是利用类似机制做到的，比如用来包装 RPC 调用、面向切面的编程（AOP）。 JDK 自身提供的动态代理，就是主要利用了上面提到的反射机制。但动态代理不止有反射一种实现方式，还有其他的实现方式，比如利用传说中更高性能的字节码操作机制，类似 ASM、cglib（基于 ASM，一个 Java 字节码操作框架）、Javassist 等。简单来说，动态代理是一种行为方式，而反射或 ASM 只是它的一种实现手段而已。 JDK Proxy 和 CGLib 的区别主要体现在以下几个方面： JDK Proxy 是 Java 语言自带的功能，无需通过加载第三方类实现； Java 对 JDK Proxy 提供了稳定的支持，并且会持续的升级和更新 JDK Proxy，例如 Java 8 版本中的 JDK Proxy 性能相比于之前版本提升了很多； JDK Proxy 是通过拦截器加反射的方式实现的； JDK Proxy 只能代理继承接口的类； JDK Proxy 实现和调用起来比较简单； CGLib 是第三方提供的工具，基于 ASM 实现的，性能比较高； CGLib 无需通过接口来实现，它是通过实现子类的方式来完成调用的。 什么是静态代理静态代理是代理类在编译期间就创建好了，不是编译器生成的代理类，而是手动创建的类。在编译时就已经将接口，被代理类，代理类等确定下来。，软件设计中所指的代理一般是指静态代理，也就是在代码中显式指定的代理。 下面我们通过一个简单的案例，来了解下静态代理。 /** * 静态代理类接口, 委托类和代理类都需要实现的接口规范。 * 定义了一个猫科动物的两个行为接口，吃东西，奔跑。 * 作为代理类 和委托类之间的约束接口 */ public interface Cat &#123; public String eatFood(String foodName); public boolean running(); &#125; /** * 狮子 实现了猫科动物接口Cat， 并实现了具体的行为。作为委托类实现 */ public class Lion implements Cat &#123; private String name; private int runningSpeed; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getRunningSpeed() &#123; return runningSpeed; &#125; public void setRunningSpeed(int runningSpeed) &#123; this.runningSpeed = runningSpeed; &#125; public Lion() &#123; &#125; @Override public String eatFood(String foodName) &#123; String eat = this.name + &quot; Lion eat food. foodName = &quot; + foodName; System.out.println(eat); return eat; &#125; @Override public boolean running() &#123; System.out.println(this.name + &quot; Lion is running . Speed :&quot; + this.runningSpeed); return false; &#125; &#125; 代理类角色(FeederProxy) /** * 饲养员 实现Cat接口，作为静态代理类实现。代理狮子的行为。 * 代理类中可以新增一些其他行为，在实践中主要做的是参数校验的功能。 */ public class FeederProxy implements Cat &#123; private Cat cat; public FeederProxy()&#123;&#125; public FeederProxy(Cat cat) &#123; if (cat instanceof Cat) &#123; this.cat = cat; &#125; &#125; public void setCat(Cat cat) &#123; if (cat instanceof Cat) &#123; this.cat = cat; &#125; &#125; @Override public String eatFood(String foodName) &#123; System.out.println(&quot;proxy Lion exec eatFood &quot;); return cat.eatFood(foodName); &#125; @Override public boolean running() &#123; System.out.println(&quot;proxy Lion exec running.&quot;); return cat.running(); &#125; &#125; 静态代理类测试 /** * 静态代理类测试 */ public class staticProxyTest &#123; public static void main(String[] args) &#123; Lion lion = new Lion(); lion.setName(&quot;狮子 小王&quot;); lion.setRunningSpeed(100); /** * new 静态代理类，静态代理类在编译前已经创建好了，和动态代理的最大区别点 */ Cat proxy = new FeederProxy(lion); System.out.println(Thread.currentThread().getName()+&quot; -- &quot; + proxy.eatFood(&quot;水牛&quot;)); proxy.running(); &#125; &#125; 静态代理很好的诠释了代理设计模式，代理模式最主要的就是有一个公共接口（Cat），一个委托类（Lion），一个代理类（FeederProxy）,代理类持有委托类的实例，代为执行具体类实例方法。 代理模式就是在访问实际对象时引入一定程度的间接性，因为这种间接性，可以附加多种用途。这里的间接性就是指客户端不直接调用实际对象的方法，客户端依赖公共接口并使用代理类。 那么我们在代理过程中就可以加上一些其他用途。 就这个例子来说在 eatFood 方法调用中，代理类在调用具体实现类之前添加System.out.println(“proxy Lion exec eatFood “);语句 就是添加间接性带来的收益。代理类存在的意义是为了增加一些公共的逻辑代码。 静态代理的缺陷 代理类和委托类实现了相同的接口，代理类通过委托类实现了相同的方法。这样就出现了大量的代码重复。如果接口增加一个方法，除了所有实现类需要实现这个方法外，所有代理类也需要实现此方法。增加了代码维护的复杂度。 代理对象只服务于一种类型的对象，如果要服务多类型的对象。势必要为每一种对象都进行代理，静态代理在程序规模稍大时就无法胜任了。 静态代理一个代理只能代理一种类型，而且是在编译器就已经确定被代理的对象。 JDK Proxy 和 CGLib 的使用样例JDK Proxy 动态代理实现","categories":[{"name":"Java","slug":"Java","permalink":"https://gyl-coder.top/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://gyl-coder.top/tags/Java/"},{"name":"动态代理","slug":"动态代理","permalink":"https://gyl-coder.top/tags/%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86/"}],"author":{"name":"yanliang","avatar":"https://cdn.jsdelivr.net/gh/gyl-coder/blogImgs/images/touxiang.webp","url":"https://gyl-coder.top"}},{"title":"try catch finally执行顺序","slug":"java/try_catch_finally","date":"2020-04-02T00:00:00.000Z","updated":"2020-12-27T04:47:15.974Z","comments":true,"path":"java/try_catch_finally/","link":"","permalink":"https://gyl-coder.top/java/try_catch_finally/","excerpt":"首先在finally和try中对数据的操作时数据分为 基本数据类型和引用数据类型，他们存放的地方也不一样，一个是栈区另一个是在堆区。","text":"首先在finally和try中对数据的操作时数据分为 基本数据类型和引用数据类型，他们存放的地方也不一样，一个是栈区另一个是在堆区。 finally中没有return public class Main &#123; private static int a=2; public static void main(String[] args) &#123; System.out.println(&quot;test输出的结果 &quot;+test()); System.out.println(&quot;main 自加后 a=&quot;+(a++)); &#125; private static int test()&#123; try &#123; a+=3; System.out.println(&quot;try a=&quot;+a); return a; &#125;catch (Exception e)&#123; &#125;finally &#123; ++a; System.out.println(&quot;finally a=&quot;+a); &#125; &#125; 输出结果: try a=5finally a=6test输出的结果 5main a=6 结论: 可见方法test的返回值不受finally中的影响，但是变量a还是受到了影响，值改变了，返回值是存放在栈中的，return是就 已经把返回值压入栈了，相当于一个临时变量。在finally中并不会影响返回值 finally中加入returnpublic class Main &#123; private static int a=2; public static void main(String[] args) &#123; System.out.println(&quot;test输出的结果 &quot;+test()); System.out.println(&quot;main 自加后 a=&quot;+(a++)); &#125; private static int test()&#123; try &#123; a+=3; System.out.println(&quot;try a=&quot;+a); return a; &#125;catch (Exception e)&#123; &#125;finally &#123; ++a; System.out.println(&quot;finally a=&quot;+a); return a; &#125; &#125; 输出结果: try a=5finally a=6test输出的结果 6main a=6 结论: 此时test方法的返回值是6和a的值相等， 在try,catch中的返回值可以发现被finally的返回值给屏蔽了，所以如果finally中 有return则会屏蔽当前方法中的返回值 现在对引用变量进行分析finally中没有return public class Main &#123; private static List&lt;String&gt; list=new ArrayList&lt;&gt;(); public static void main(String[] args) &#123; System.out.println(&quot;test输出的结果 &quot;+test()); System.out.println(&quot;main list=&quot;+list.toString()); &#125; private static List test()&#123; try &#123; list.add(&quot;33&quot;); System.out.println(&quot;try list=&quot;+list.toString()); return list; &#125;catch (Exception e)&#123; &#125;finally &#123; list.add(&quot;66&quot;); System.out.println(&quot;finally list=&quot;+list.toString()); &#125; return null; &#125; &#125; 输出结果: try list=[33]finally list=[33, 66]test输出的结果 [33, 66]main list=[33, 66] 结论: 通过结果可以知道 test的返回值内容也是跟集合list一样的， 因为list是引用类型所以堆区中存放了内存地址，在finally对值 进行改变时还是只想同一个地址，只是地址的内容变化了 2 finally中有return 、 输出值: try list=[33]finally list=[33, 66]test输出的结果 [33, 66]main list=[33, 66] 结论:可见引用类型时finally有没有return都会对当前方法的返回值产生影响","categories":[{"name":"Java","slug":"Java","permalink":"https://gyl-coder.top/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://gyl-coder.top/tags/Java/"},{"name":"try-catch","slug":"try-catch","permalink":"https://gyl-coder.top/tags/try-catch/"}],"author":{"name":"yanliang","avatar":"https://cdn.jsdelivr.net/gh/gyl-coder/blogImgs/images/touxiang.webp","url":"https://gyl-coder.top"}},{"title":"IOC & AOP 详解","slug":"spring/spring-ioc-aop","date":"2020-04-02T00:00:00.000Z","updated":"2020-12-27T04:47:15.978Z","comments":true,"path":"spring/spring-ioc-aop/","link":"","permalink":"https://gyl-coder.top/spring/spring-ioc-aop/","excerpt":"下面从以下几个问题展开对 IOC &amp; AOP 的解释 什么是IOC？ IOC解决了什么问题？ IOC 和 DI 的区别？ 什么是AOP？ AOP解决了什么问题？ AOP为什么叫做切面变成？ 首先声明： IOC &amp; AOP 不是Spring提出来的，它们在Spring之前其实已经存在了，只不过当时更加偏向于理论。 Spring 在技术层次将这两个思想进行了很好的实现。","text":"下面从以下几个问题展开对 IOC &amp; AOP 的解释 什么是IOC？ IOC解决了什么问题？ IOC 和 DI 的区别？ 什么是AOP？ AOP解决了什么问题？ AOP为什么叫做切面变成？ 首先声明： IOC &amp; AOP 不是Spring提出来的，它们在Spring之前其实已经存在了，只不过当时更加偏向于理论。 Spring 在技术层次将这两个思想进行了很好的实现。 什么是 IOC IOC （Inversion of control ） 控制反转/反转控制。它是一种 思想 不是一个技术实现。描述的是：Java 开发领域对象的创建以及管理的问题。 例如：现有 类A依赖于类B 传统的开发方式：往往是在类A中手动通过 new 关键字来 new 一个B的对象出来 使用IOC思想的开发方式： 不通过new 关键字来创建对象，而是通过 IOC容器 (Spring 框架) 来帮助我们实例化对象。我们需要哪个对象，直接从IOC容器里面过去即可。 从以上两种开发方式的对比来看：我们 “丧失了一个权力” (创建、管理对象的权力)，从而也得到了一个好处（不用再考虑对象的创建、管理等一系列的事情） 为什么叫控制反转控制： 指的是对象创建（实例化、管理）的权力 反转： 控制权交给外部环境（Spring框架、IOC容器） IOC 解决了什么问题IOC 主要解决的是对象之间的耦合问题。 例如：现有一个针对User的操作，利用 Service 和 Dao 两层结构进行开发 在没有使用IOC思想的情况下，Service 层想要使用 Dao层的具体实现的话，需要通过 new 关键字在UserServiceImpl 中手动 new出 IUserDao 的具体实现类 UserDaoImpl（不能直接new接口类）。 很完美，这种方式也是可以实现的，但是我们想象一下如下场景：开发过程中突然接到一个新的需求，针对对IUserDao 接口开发出另一个具体实现类。因为Server层依赖了IUserDao的具体实现，所以我们需要修改UserServiceImpl中new的对象。如果只有一个类引用了IUserDao的具体实现，可能觉得还好，修改起来也不是很费力气，但是如果有许许多多的地方都引用了IUserDao的具体实现的话，一旦需要更换IUserDao的实现方式，那修改起来将会非常的头疼。 使用IOC的思想，我们将对象的控制权（创建、管理）交有IOC容器去管理，我们在使用的时候直接向IOC容器 “要” 就可以了 IOC 和 DI 的区别IOC 和 DI 描述的是同一件事情（对象实例化以及依赖关系的维护），只不过角度不同。 IOC （Inversion of control ） 控制反转/反转控制。是站在对象的角度，对象实例化以及管理的权限（反转）交给了容器。 DI （Dependancy Injection）依赖注入。是站在容器的角度，容器会把对象依赖的其他对象注入（送进去）。例如：对象A 实例化过程中因为声明了一个B类型的属性，那么就需要容器把B对象注入到A中。 什么是AOPAOP：Aspect oriented programming 面向切面编程，AOP是 OOP（面向对象编程）的一种延续，下面我们先看一个OOP的例子。 例如：现有三个类，Horse、Pig、Dog，这三个类中都有 eat 和 run 两个方法。 通过OOP思想中的继承，我们可以提取出一个 Animal 的父类，然后将 eat 和 run 方法放入父类中，Horse、Pig、Dog通过继承Animal类即可自动获得 eat 和 run 方法。这样将会少些很多重复的代码。 OOP编程思想可以解决大部分的代码重复问题。但是有一些问题是处理不了的。比如在父类Animal 中的多个方法的相同位置出现了重复的代码，OOP就解决不了。 /** * 动物父类 */ public class Animal &#123; /** 身高 */ private String height; /** 体重 */ private double weight; public void eat() &#123; // 性能监控代码 long start = System.currentTimeMillis(); // 业务逻辑代码 System.out.println(&quot;I can eat...&quot;); // 性能监控代码 System.out.println(&quot;执行时长：&quot; + (System.currentTimeMillis() - start)/1000f + &quot;s&quot;); &#125; public void run() &#123; // 性能监控代码 long start = System.currentTimeMillis(); // 业务逻辑代码 System.out.println(&quot;I can run...&quot;); // 性能监控代码 System.out.println(&quot;执行时长：&quot; + (System.currentTimeMillis() - start)/1000f + &quot;s&quot;); &#125; &#125; 这部分重复的代码，一般统称为 横切逻辑代码。 横切逻辑代码存在的问题： 代码重复问题 横切逻辑代码和业务代码混杂在一起，代码臃肿，不变维护 AOP 就是用来解决这些问题的 AOP 另辟蹊径，提出横向抽取机制，将横切逻辑代码和业务逻辑代码分离 代码拆分比较容易，难的是如何在不改变原有业务逻辑的情况下，悄无声息的将横向逻辑代码应用到原有的业务逻辑中，达到和原来一样的效果。 AOP解决了什么问题通过上面的分析可以发现，AOP主要用来解决：在不改变原有业务逻辑的情况下，增强横切逻辑代码，根本上解耦合，避免横切逻辑代码重复。 AOP为什么叫面向切面编程切： 指的是横切逻辑，原有业务逻辑代码不动，只能操作横切逻辑代码，所以面向横切逻辑 面： 横切逻辑代码往往要影响的是很多个方法，每个方法如同一个点，多个点构成一个面。这里有一个面的概念","categories":[{"name":"Spring","slug":"Spring","permalink":"https://gyl-coder.top/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://gyl-coder.top/tags/Spring/"},{"name":"IOC","slug":"IOC","permalink":"https://gyl-coder.top/tags/IOC/"},{"name":"AOP","slug":"AOP","permalink":"https://gyl-coder.top/tags/AOP/"}],"author":{"name":"yanliang","avatar":"https://cdn.jsdelivr.net/gh/gyl-coder/blogImgs/images/touxiang.webp","url":"https://gyl-coder.github.io"}},{"title":"Settings.xml 详解","slug":"java/settings","date":"2020-04-02T00:00:00.000Z","updated":"2020-12-27T04:47:15.974Z","comments":true,"path":"java/settings/","link":"","permalink":"https://gyl-coder.top/java/settings/","excerpt":"从 settings.xml 的文件名就可以看出，它是用来设置 maven 参数的配置文件。settings.xml 中包含类似本地仓储位置、修改远程仓储服务器、认证信息等配置。 settings.xml 是maven的全局配置文件。 pom.xml 文件是本地项目配置文件。","text":"从 settings.xml 的文件名就可以看出，它是用来设置 maven 参数的配置文件。settings.xml 中包含类似本地仓储位置、修改远程仓储服务器、认证信息等配置。 settings.xml 是maven的全局配置文件。 pom.xml 文件是本地项目配置文件。 settings.xml 文件位置settings.xml 文件一般位于两个位置： 全局配置 ${maven.home}/conf/settings.xml 用户配置 ${user.home}/.m2/settings.xml 注意：用户配置优先于全局配置。$&#123;user.home&#125; 和和所有其他系统属性只能在 3.0+版本上使用。 配置优先级 重要：局部配置优先于全局配置 配置优先级从高到低：pom.xml &gt; user settings &gt; global settings 如果这些文件同时存在，在应用配置时，会合并它们的内容，如果有重复的配置，优先级高的配置会覆盖优先级低的。 settings.xml 元素详解顶级元素概览&lt;settings xmlns=&quot;http://maven.apache.org/SETTINGS/1.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/SETTINGS/1.0.0 https://maven.apache.org/xsd/settings-1.0.0.xsd&quot;&gt; &lt;localRepository/&gt; &lt;interactiveMode/&gt; &lt;usePluginRegistry/&gt; &lt;offline/&gt; &lt;pluginGroups/&gt; &lt;servers/&gt; &lt;mirrors/&gt; &lt;proxies/&gt; &lt;profiles/&gt; &lt;activeProfiles/&gt; &lt;/settings&gt; LocalRepository作用：该值表示构建系统本地仓库的路径。 默认值：~/.m2/repository &lt;localRepository&gt;$&#123;user.home&#125;/.m2/repository&lt;/localRepository&gt; InteractiveMode作用：表示 maven 是否需要和用户交互以获得输入。 如果 maven 需要和用户交互以获得输入，则设置成 true，反之则应为 false。默认为 true。 &lt;interactiveMode&gt;true&lt;/interactiveMode&gt; UsePluginRegistry作用：maven 是否需要使用 plugin-registry.xml 文件来管理插件版本。 如果需要让 maven 使用文件~/.m2/plugin-registry.xml 来管理插件版本，则设为 true。默认为 false。 &lt;usePluginRegistry&gt;false&lt;/usePluginRegistry&gt; Offline作用：表示 maven 是否需要在离线模式下运行。 如果构建系统需要在离线模式下运行，则为 true，默认为 false。 当由于网络设置原因或者安全因素，构建服务器不能连接远程仓库的时候，该配置就十分有用。 &lt;offline&gt;false&lt;/offline&gt; PluginGroups作用：当插件的组织 id（groupId）没有显式提供时，供搜寻插件组织 Id（groupId）的列表。 该元素包含一个 pluginGroup 元素列表，每个子元素包含了一个组织 Id（groupId）。 当我们使用某个插件，并且没有在命令行为其提供组织 Id（groupId）的时候，Maven 就会使用该列表。默认情况下该列表包含了 org.apache.maven.plugins 和 org.codehaus.mojo。 &lt;settings xmlns=&quot;http://maven.apache.org/SETTINGS/1.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/SETTINGS/1.0.0 https://maven.apache.org/xsd/settings-1.0.0.xsd&quot;&gt; ... &lt;pluginGroups&gt; &lt;!--plugin的组织Id（groupId） --&gt; &lt;pluginGroup&gt;org.codehaus.mojo&lt;/pluginGroup&gt; &lt;/pluginGroups&gt; ... &lt;/settings&gt; Servers作用：一般，仓库的下载和部署是在 pom.xml 文件中的 repositories 和 distributionManagement 元素中定义的。然而，一般类似用户名、密码（有些仓库访问是需要安全认证的）等信息不应该在 pom.xml 文件中配置，这些信息可以配置在 settings.xml 中。 &lt;settings xmlns=&quot;http://maven.apache.org/SETTINGS/1.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/SETTINGS/1.0.0 https://maven.apache.org/xsd/settings-1.0.0.xsd&quot;&gt; ... &lt;!--配置服务端的一些设置。一些设置如安全证书不应该和pom.xml一起分发。这种类型的信息应该存在于构建服务器上的settings.xml文件中。 --&gt; &lt;servers&gt; &lt;!--服务器元素包含配置服务器时需要的信息 --&gt; &lt;server&gt; &lt;!--这是server的id（注意不是用户登陆的id），该id与distributionManagement中repository元素的id相匹配。 --&gt; &lt;id&gt;server001&lt;/id&gt; &lt;!--鉴权用户名。鉴权用户名和鉴权密码表示服务器认证所需要的登录名和密码。 --&gt; &lt;username&gt;my_login&lt;/username&gt; &lt;!--鉴权密码 。鉴权用户名和鉴权密码表示服务器认证所需要的登录名和密码。密码加密功能已被添加到2.1.0 +。详情请访问密码加密页面 --&gt; &lt;password&gt;my_password&lt;/password&gt; &lt;!--鉴权时使用的私钥位置。和前两个元素类似，私钥位置和私钥密码指定了一个私钥的路径（默认是$&#123;user.home&#125;/.ssh/id_dsa）以及如果需要的话，一个密语。将来passphrase和password元素可能会被提取到外部，但目前它们必须在settings.xml文件以纯文本的形式声明。 --&gt; &lt;privateKey&gt;$&#123;usr.home&#125;/.ssh/id_dsa&lt;/privateKey&gt; &lt;!--鉴权时使用的私钥密码。 --&gt; &lt;passphrase&gt;some_passphrase&lt;/passphrase&gt; &lt;!--文件被创建时的权限。如果在部署的时候会创建一个仓库文件或者目录，这时候就可以使用权限（permission）。这两个元素合法的值是一个三位数字，其对应了unix文件系统的权限，如664，或者775。 --&gt; &lt;filePermissions&gt;664&lt;/filePermissions&gt; &lt;!--目录被创建时的权限。 --&gt; &lt;directoryPermissions&gt;775&lt;/directoryPermissions&gt; &lt;/server&gt; &lt;/servers&gt; ... &lt;/settings&gt; Mirrors作用：为仓库列表配置的下载镜像列表。 &lt;settings xmlns=&quot;http://maven.apache.org/SETTINGS/1.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/SETTINGS/1.0.0 https://maven.apache.org/xsd/settings-1.0.0.xsd&quot;&gt; ... &lt;mirrors&gt; &lt;!-- 给定仓库的下载镜像。 --&gt; &lt;mirror&gt; &lt;!-- 该镜像的唯一标识符。id用来区分不同的mirror元素。 --&gt; &lt;id&gt;planetmirror.com&lt;/id&gt; &lt;!-- 镜像名称 --&gt; &lt;name&gt;PlanetMirror Australia&lt;/name&gt; &lt;!-- 该镜像的URL。构建系统会优先考虑使用该URL，而非使用默认的服务器URL。 --&gt; &lt;url&gt;http://downloads.planetmirror.com/pub/maven2&lt;/url&gt; &lt;!-- 被镜像的服务器的id。例如，如果我们要设置了一个Maven中央仓库（http://repo.maven.apache.org/maven2/）的镜像，就需要将该元素设置成central。这必须和中央仓库的id central完全一致。 --&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;/mirror&gt; &lt;/mirrors&gt; ... &lt;/settings&gt; Proxies作用：用来配置不同的代理。 &lt;settings xmlns=&quot;http://maven.apache.org/SETTINGS/1.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/SETTINGS/1.0.0 https://maven.apache.org/xsd/settings-1.0.0.xsd&quot;&gt; ... &lt;proxies&gt; &lt;!--代理元素包含配置代理时需要的信息 --&gt; &lt;proxy&gt; &lt;!--代理的唯一定义符，用来区分不同的代理元素。 --&gt; &lt;id&gt;myproxy&lt;/id&gt; &lt;!--该代理是否是激活的那个。true则激活代理。当我们声明了一组代理，而某个时候只需要激活一个代理的时候，该元素就可以派上用处。 --&gt; &lt;active&gt;true&lt;/active&gt; &lt;!--代理的协议。 协议://主机名:端口，分隔成离散的元素以方便配置。 --&gt; &lt;protocol&gt;http&lt;/protocol&gt; &lt;!--代理的主机名。协议://主机名:端口，分隔成离散的元素以方便配置。 --&gt; &lt;host&gt;proxy.somewhere.com&lt;/host&gt; &lt;!--代理的端口。协议://主机名:端口，分隔成离散的元素以方便配置。 --&gt; &lt;port&gt;8080&lt;/port&gt; &lt;!--代理的用户名，用户名和密码表示代理服务器认证的登录名和密码。 --&gt; &lt;username&gt;proxyuser&lt;/username&gt; &lt;!--代理的密码，用户名和密码表示代理服务器认证的登录名和密码。 --&gt; &lt;password&gt;somepassword&lt;/password&gt; &lt;!--不该被代理的主机名列表。该列表的分隔符由代理服务器指定；例子中使用了竖线分隔符，使用逗号分隔也很常见。 --&gt; &lt;nonProxyHosts&gt;*.google.com|ibiblio.org&lt;/nonProxyHosts&gt; &lt;/proxy&gt; &lt;/proxies&gt; ... &lt;/settings&gt; Profiles作用：根据环境参数来调整构建配置的列表。 settings.xml 中的 profile 元素是 pom.xml 中 profile 元素的裁剪版本。 它包含了id、activation、repositories、pluginRepositories 和 properties 元素。这里的 profile 元素只包含这五个子元素是因为这里只关心构建系统这个整体（这正是 settings.xml 文件的角色定位），而非单独的项目对象模型设置。如果一个 settings.xml 中的 profile 被激活，它的值会覆盖任何其它定义在 pom.xml 中带有相同 id 的 profile。 &lt;settings xmlns=&quot;http://maven.apache.org/SETTINGS/1.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/SETTINGS/1.0.0 https://maven.apache.org/xsd/settings-1.0.0.xsd&quot;&gt; ... &lt;profiles&gt; &lt;profile&gt; &lt;!-- profile的唯一标识 --&gt; &lt;id&gt;test&lt;/id&gt; &lt;!-- 自动触发profile的条件逻辑 --&gt; &lt;activation /&gt; &lt;!-- 扩展属性列表 --&gt; &lt;properties /&gt; &lt;!-- 远程仓库列表 --&gt; &lt;repositories /&gt; &lt;!-- 插件仓库列表 --&gt; &lt;pluginRepositories /&gt; &lt;/profile&gt; &lt;/profiles&gt; ... &lt;/settings&gt; Activation作用：自动触发 profile 的条件逻辑。 如 pom.xml 中的 profile 一样，profile 的作用在于它能够在某些特定的环境中自动使用某些特定的值；这些环境通过 activation 元素指定。activation 元素并不是激活 profile 的唯一方式。settings.xml 文件中的 activeProfile 元素可以包含 profile 的 id。profile 也可以通过在命令行，使用 -P 标记和逗号分隔的列表来显式的激活（如，-P test）。 &lt;activation&gt; &lt;!--profile默认是否激活的标识 --&gt; &lt;activeByDefault&gt;false&lt;/activeByDefault&gt; &lt;!--当匹配的jdk被检测到，profile被激活。例如，1.4激活JDK1.4，1.4.0_2，而!1.4激活所有版本不是以1.4开头的JDK。 --&gt; &lt;jdk&gt;1.5&lt;/jdk&gt; &lt;!--当匹配的操作系统属性被检测到，profile被激活。os元素可以定义一些操作系统相关的属性。 --&gt; &lt;os&gt; &lt;!--激活profile的操作系统的名字 --&gt; &lt;name&gt;Windows XP&lt;/name&gt; &lt;!--激活profile的操作系统所属家族(如 &#39;windows&#39;) --&gt; &lt;family&gt;Windows&lt;/family&gt; &lt;!--激活profile的操作系统体系结构 --&gt; &lt;arch&gt;x86&lt;/arch&gt; &lt;!--激活profile的操作系统版本 --&gt; &lt;version&gt;5.1.2600&lt;/version&gt; &lt;/os&gt; &lt;!--如果Maven检测到某一个属性（其值可以在POM中通过$&#123;name&#125;引用），其拥有对应的name = 值，Profile就会被激活。如果值字段是空的，那么存在属性名称字段就会激活profile，否则按区分大小写方式匹配属性值字段 --&gt; &lt;property&gt; &lt;!--激活profile的属性的名称 --&gt; &lt;name&gt;mavenVersion&lt;/name&gt; &lt;!--激活profile的属性的值 --&gt; &lt;value&gt;2.0.3&lt;/value&gt; &lt;/property&gt; &lt;!--提供一个文件名，通过检测该文件的存在或不存在来激活profile。missing检查文件是否存在，如果不存在则激活profile。另一方面，exists则会检查文件是否存在，如果存在则激活profile。 --&gt; &lt;file&gt; &lt;!--如果指定的文件存在，则激活profile。 --&gt; &lt;exists&gt;$&#123;basedir&#125;/file2.properties&lt;/exists&gt; &lt;!--如果指定的文件不存在，则激活profile。 --&gt; &lt;missing&gt;$&#123;basedir&#125;/file1.properties&lt;/missing&gt; &lt;/file&gt; &lt;/activation&gt; 注：在 maven 工程的 pom.xml 所在目录下执行 mvn help:active-profiles 命令可以查看中央仓储的 profile 是否在工程中生效。 properties作用：对应profile的扩展属性列表。 maven 属性和 ant 中的属性一样，可以用来存放一些值。这些值可以在 pom.xml 中的任何地方使用标记${X}来使用，这里 X 是指属性的名称。属性有五种不同的形式，并且都能在 settings.xml 文件中访问。 &lt;!-- 1. env.X: 在一个变量前加上&quot;env.&quot;的前缀，会返回一个shell环境变量。例如,&quot;env.PATH&quot;指代了$path环境变量（在Windows上是%PATH%）。 2. project.x：指代了POM中对应的元素值。例如: &lt;project&gt;&lt;version&gt;1.0&lt;/version&gt;&lt;/project&gt;通过$&#123;project.version&#125;获得version的值。 3. settings.x: 指代了settings.xml中对应元素的值。例如：&lt;settings&gt;&lt;offline&gt;false&lt;/offline&gt;&lt;/settings&gt;通过 $&#123;settings.offline&#125;获得offline的值。 4. Java System Properties: 所有可通过java.lang.System.getProperties()访问的属性都能在POM中使用该形式访问，例如 $&#123;java.home&#125;。 5. x: 在&lt;properties/&gt;元素中，或者外部文件中设置，以$&#123;someVar&#125;的形式使用。 --&gt; &lt;properties&gt; &lt;user.install&gt;$&#123;user.home&#125;/our-project&lt;/user.install&gt; &lt;/properties&gt; 注：如果该 profile 被激活，则可以在pom.xml中使用${user.install}。 Repositories作用：远程仓库列表，它是 maven 用来填充构建系统本地仓库所使用的一组远程仓库。 &lt;repositories&gt; &lt;!--包含需要连接到远程仓库的信息 --&gt; &lt;repository&gt; &lt;!--远程仓库唯一标识 --&gt; &lt;id&gt;codehausSnapshots&lt;/id&gt; &lt;!--远程仓库名称 --&gt; &lt;name&gt;Codehaus Snapshots&lt;/name&gt; &lt;!--如何处理远程仓库里发布版本的下载 --&gt; &lt;releases&gt; &lt;!--true或者false表示该仓库是否为下载某种类型构件（发布版，快照版）开启。 --&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;!--该元素指定更新发生的频率。Maven会比较本地POM和远程POM的时间戳。这里的选项是：always（一直），daily（默认，每日），interval：X（这里X是以分钟为单位的时间间隔），或者never（从不）。 --&gt; &lt;updatePolicy&gt;always&lt;/updatePolicy&gt; &lt;!--当Maven验证构件校验文件失败时该怎么做-ignore（忽略），fail（失败），或者warn（警告）。 --&gt; &lt;checksumPolicy&gt;warn&lt;/checksumPolicy&gt; &lt;/releases&gt; &lt;!--如何处理远程仓库里快照版本的下载。有了releases和snapshots这两组配置，POM就可以在每个单独的仓库中，为每种类型的构件采取不同的策略。例如，可能有人会决定只为开发目的开启对快照版本下载的支持。参见repositories/repository/releases元素 --&gt; &lt;snapshots&gt; &lt;enabled /&gt; &lt;updatePolicy /&gt; &lt;checksumPolicy /&gt; &lt;/snapshots&gt; &lt;!--远程仓库URL，按protocol://hostname/path形式 --&gt; &lt;url&gt;http://snapshots.maven.codehaus.org/maven2&lt;/url&gt; &lt;!--用于定位和排序构件的仓库布局类型-可以是default（默认）或者legacy（遗留）。Maven 2为其仓库提供了一个默认的布局；然而，Maven 1.x有一种不同的布局。我们可以使用该元素指定布局是default（默认）还是legacy（遗留）。 --&gt; &lt;layout&gt;default&lt;/layout&gt; &lt;/repository&gt; &lt;/repositories&gt; pluginRepositories作用：发现插件的远程仓库列表。 和 repository 类似，只是 repository 是管理 jar 包依赖的仓库，pluginRepositories 则是管理插件的仓库。maven 插件是一种特殊类型的构件。由于这个原因，插件仓库独立于其它仓库。pluginRepositories 元素的结构和 repositories 元素的结构类似。每个 pluginRepository 元素指定一个 Maven 可以用来寻找新插件的远程地址。 pluginRepositoriespluginRepositoryreleasesenabledupdatePolicychecksumPolicyreleasessnapshotsenabledupdatePolicychecksumPolicysnapshotsidnameurllayoutpluginRepositorypluginRepositories ActiveProfiles作用：手动激活 profiles 的列表，按照profile被应用的顺序定义activeProfile。 该元素包含了一组 activeProfile 元素，每个 activeProfile 都含有一个 profile id。任何在 activeProfile 中定义的 profile id，不论环境设置如何，其对应的 profile 都会被激活。如果没有匹配的 profile，则什么都不会发生。 例如，env-test 是一个 activeProfile，则在 pom.xml（或者 profile.xml）中对应 id 的 profile 会被激活。如果运行过程中找不到这样一个 profile，Maven 则会像往常一样运行。 settingsactiveProfilesactiveProfileactiveProfileactiveProfilessettings","categories":[{"name":"Java","slug":"Java","permalink":"https://gyl-coder.top/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://gyl-coder.top/tags/Java/"},{"name":"Settings","slug":"Settings","permalink":"https://gyl-coder.top/tags/Settings/"}],"author":{"name":"yanliang","avatar":"https://cdn.jsdelivr.net/gh/gyl-coder/blogImgs/images/touxiang.webp","url":"https://gyl-coder.top"}},{"title":"MyBatis 入门","slug":"mybatis/mybatis-beginner","date":"2020-03-30T00:00:00.000Z","updated":"2020-12-27T04:47:15.978Z","comments":true,"path":"mybatis/mybatis-beginner/","link":"","permalink":"https://gyl-coder.top/mybatis/mybatis-beginner/","excerpt":"MyBatis 是一款优秀的基于 ORM 的半自动轻量级持久层框架。它支持定制化SQL、存储过程以及高级映射。MyBatis 避免了几乎所有的 JDBC 代码和手动设置参数以及获取结果集。MyBatis 可以使用简单的 XML 或注解来配置和映射原生类型、接口和 Java 的POJO（Plain Old Java Objects，普通老式Java 对象）为数据库的记录。 MyBatis 原本是Apache 的一个开源项目IBatis，2010年6月这个项目由 Apache Software Foundation 迁移到了 Google Code，随后更名为 MyBatis 。 iBatis 一词来源于 “internet” 和 “abatis” 的组合，是一个基于Java的持久层框架。 官网地址https://mybatis.org/mybatis-3/","text":"MyBatis 是一款优秀的基于 ORM 的半自动轻量级持久层框架。它支持定制化SQL、存储过程以及高级映射。MyBatis 避免了几乎所有的 JDBC 代码和手动设置参数以及获取结果集。MyBatis 可以使用简单的 XML 或注解来配置和映射原生类型、接口和 Java 的POJO（Plain Old Java Objects，普通老式Java 对象）为数据库的记录。 MyBatis 原本是Apache 的一个开源项目IBatis，2010年6月这个项目由 Apache Software Foundation 迁移到了 Google Code，随后更名为 MyBatis 。 iBatis 一词来源于 “internet” 和 “abatis” 的组合，是一个基于Java的持久层框架。 官网地址https://mybatis.org/mybatis-3/ 上面简单介绍了一下 MyBatis ，下面我们来看下MyBatis 的优势 MyBatis 的优势&amp;劣势优势我认为 MyBatis 有以下几点优势 简单易学 MyBatis 本身体积较小且没有第三方依赖，使用起来简单容易上手。源码也比较容易看懂 边界清晰 MyBatis 是一个半自动化的持久层框架，Sql 和 Java代码分开，功能边界较为清晰。一个专注业务，一个专注数据。 代码量相对较少 因为MyBatis核心是对JDBC代码的封装，避免开发人员写大量重复的代码。 劣势 依赖SQL 要求开发人员有一定的SQL 功底 SQL 语句的编写工作量大（表多，表结构复杂时） 数据库移植性差（针对不同的数据库SQL 有差异） MyBatis 基本应用开发步骤如果要在工程中使用MyBatis 基本流程如下： 在pom 文件中添加Mybatis 的依赖 创建表结构 编写表结构所对应的实体类 xxx.java 编写对应实体类的映射文件 xxxMapper.xml 编写MyBatis 的核心配置文件 SqlMapConfig.xml 编写测试类 下面我们通过使用MyBatis 对一个User 实体进行CRUD 操作。 引入相关依赖在工程所在的 pom 文件中加入以下依赖 &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;maven.compiler.encoding&gt;UTF-8&lt;/maven.compiler.encoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;!--mybatis 依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;3.4.5&lt;/version&gt; &lt;/dependency&gt; &lt;!--mysql 依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.6&lt;/version&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- 单元测试依赖 --&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 创建User 表结构在数据库中建立一个简单的User 表结构 字段名 字段类型 长度 id int 11 username varchar 50 password varchar 50 编写User 实体这里为了减少代码量，引入了 lombok @Getter @Setter @ToString public class User &#123; /** 用户Id */ private int id; /** 用户名 */ private String username; /** 用户密码 */ private String password; &#125; 编写UserMapper 映射文件&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt; &lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt; &lt;mapper namespace=&quot;com.yanliang.mockTest.dao.IUserDao&quot;&gt; &lt;!-- 查询所有 --&gt; &lt;select id=&quot;selectList&quot; resultType=&quot;com.yanliang.mockTest.User&quot;&gt; select * from user &lt;/select&gt; &lt;!-- 查询单个对象 --&gt; &lt;select id=&quot;selectOne&quot; resultType=&quot;com.yanliang.mockTest.User&quot; paramterType=&quot;com.yanliang.mockTest.User&quot;&gt; select * from user where id = #&#123;id&#125; and username = #&#123;username&#125; &lt;/select&gt; &lt;!-- 插入--&gt; &lt;insert id=&quot;insert&quot; parameterType=&quot;com.yanliang.mockTest.User&quot;&gt; insert into user values ( #&#123;id&#125;, #&#123;username&#125; ) &lt;/insert&gt; &lt;!-- 删除--&gt; &lt;delete id=&quot;delete&quot; parameterType=&quot;java.lang.Integer&quot;&gt; delete from user where id = #&#123;id&#125; &lt;/delete&gt; &lt;!-- 更新--&gt; &lt;update id=&quot;update&quot; parameterType=&quot;com.yanliang.mockTest.User&quot;&gt; update user set username = #&#123;username&#125; where id = #&#123;id&#125; &lt;/update&gt; &lt;/mapper&gt;","categories":[{"name":"Mybatis","slug":"Mybatis","permalink":"https://gyl-coder.top/categories/Mybatis/"}],"tags":[{"name":"Mybatis","slug":"Mybatis","permalink":"https://gyl-coder.top/tags/Mybatis/"}],"author":{"name":"yanliang","avatar":"https://cdn.jsdelivr.net/gh/gyl-coder/blogImgs/images/touxiang.webp","url":"https://gyl-coder.github.io"}},{"title":"HashMap 底层实现原理分析","slug":"java/collection/HashMapAnalysis","date":"2020-03-23T00:00:00.000Z","updated":"2020-12-27T04:47:15.974Z","comments":true,"path":"java/collection/HashMapAnalysis/","link":"","permalink":"https://gyl-coder.top/java/collection/HashMapAnalysis/","excerpt":"HashMap 是 Map 的一个实现类，它代表的是一种键值对的数据存储形式。 大多数情况下可以直接定位到它的值，因而具有很快的访问速度，但遍历顺序却是不确定的。 HashMap最多只允许一条记录的键为null，允许多条记录的值为null。不保证有序 (比如插入的顺序)、也不保证序不随时间变化。 jdk 8 之前，其内部是由 数组 + 链表 来实现的，而 jdk 8 对于链表长度超过 8 的链表将转储为 红黑树。 HashMap非线程安全，即任一时刻可以有多个线程同时写HashMap，可能会导致数据的不一致。如果需要满足线程安全，可以用 Collections的synchronizedMap方法使HashMap具有线程安全的能力，或者使用ConcurrentHashMap。 HashMap是 数组 + 链表 + 红黑树（JDK1.8 增加了红黑树部分）实现的。JDK 1.8 之所以添加红黑树是因为一旦链表过长，会严重影响 HashMap 的性能，而红黑树具有快速增删改查的特点，这样就可以有效的解决链表过长时操作比较慢的问题。","text":"HashMap 是 Map 的一个实现类，它代表的是一种键值对的数据存储形式。 大多数情况下可以直接定位到它的值，因而具有很快的访问速度，但遍历顺序却是不确定的。 HashMap最多只允许一条记录的键为null，允许多条记录的值为null。不保证有序 (比如插入的顺序)、也不保证序不随时间变化。 jdk 8 之前，其内部是由 数组 + 链表 来实现的，而 jdk 8 对于链表长度超过 8 的链表将转储为 红黑树。 HashMap非线程安全，即任一时刻可以有多个线程同时写HashMap，可能会导致数据的不一致。如果需要满足线程安全，可以用 Collections的synchronizedMap方法使HashMap具有线程安全的能力，或者使用ConcurrentHashMap。 HashMap是 数组 + 链表 + 红黑树（JDK1.8 增加了红黑树部分）实现的。JDK 1.8 之所以添加红黑树是因为一旦链表过长，会严重影响 HashMap 的性能，而红黑树具有快速增删改查的特点，这样就可以有效的解决链表过长时操作比较慢的问题。 存储结构下面我们先来看一下 HashMap 内部所用到的存储结构 static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; final K key; V value; Node&lt;K,V&gt; next; Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return value; &#125; public final String toString() &#123; return key + &quot;=&quot; + value; &#125; public final int hashCode() &#123; return Objects.hashCode(key) ^ Objects.hashCode(value); &#125; public final V setValue(V newValue) &#123; V oldValue = value; value = newValue; return oldValue; &#125; public final boolean equals(Object o) &#123; if (o == this) return true; if (o instanceof Map.Entry) &#123; Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;)o; if (Objects.equals(key, e.getKey()) &amp;&amp; Objects.equals(value, e.getValue())) return true; &#125; return false; &#125; &#125; 可以看出每个哈希桶中包含了四个字段：hash、key、value、next，其中 next 表示链表的下一个节点。 Node是HashMap的一个内部类，实现了 Map.Entry 接口，本质上就是一个映射 (键值对)。 有时两个key会定位到相同的位置，表示发生了 Hash 碰撞。当然Hash算法计算结果越分散均匀，Hash碰撞的概率就越小，map的存取效率就会越高。 HashMap类中有一个非常重要的字段，就是 Node[] table，即哈希桶数组。如果哈希桶数组很大，即使较差的Hash算法也会比较分散，如果哈希桶数组数组很小，即使好的Hash算法也会出现较多碰撞。 所以就需要在空间成本和时间成本之间权衡，其实就是在根据实际情况确定哈希桶数组的大小，并在此基础上设计好的hash算法减少 Hash 碰撞。那么通过什么方式来控制 map 使得 Hash 碰撞的概率又小，哈希桶数组（Node[] table）占用空间又少呢？答案就是好的 Hash 算法和扩容机制。 上面大体介绍了 HashMap 的组成结构，下面我们来看一些关于 HashMap 的核心问题。 HashMap 中的hash算法如何实现？ JDK1.8HashMap扩容时做了哪些优化？ 加载因子为什么是0.75？ 当有哈希冲突时，HashMap是如何查找并确认元素的？HashMap源码中有哪些重要的方法？ HashMap 源码中有哪些重要的方法？ 成员变量不过在这之前我们先了解下 hashmap 中的变量 // HashMap 初始化长度 static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // HashMap 最大长度 static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; // 默认的加载因子 （扩容因子） static final float DEFAULT_LOAD_FACTOR = 0.75f; // 当链表长度大于此值且容量大于64时 static final int TREEIFY_THRESHOLD = 8; // 转换链表的临界值，当元素小于此值时，将红黑树转换为链表结构 static final int UNTREEIFY_THRESHOLD = 6; // 最小数容量 static final int MIN_TREEIFY_CAPACITY = 64; transient Node&lt;K,V&gt;[] table; transient Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet; transient int size; transient int modCount; int threshold; final float loadFactor; 在 HashMap 中有两个很重要的参数，容量 (Capacity) 和负载因子(Load factor) Capacity就是buckets的数目，Load factor就是buckets填满程度的最大比例。如果对迭代性能要求很高的话不要把capacity设置过大，也不要把load factor设置过小。当bucket填充的数目（即 hashmap 中元素的个数）大于capacity*load factor时就需要调整 buckets 的数目为当前的 2 倍。 HashMap 中的hash算法如何实现？static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16); &#125; static int indexFor(int h, int length) &#123; return h &amp; (length-1); &#125; indexFor 是 jdk1.7 的源码，jdk1.8 没有这个方法但是 jdk1.8 也是通过取模运算来计算的 这里的 Hash 算法本质上就是三步： 取 key 的 hashCode 值 高位运算 取模运算 对于任意给定的对象，只要它的hashCode()返回值相同，那么程序调用方法一所计算得到的Hash码值总是相同的。我们首先想到的就是把hash值对数组长度取模运算，这样一来，元素的分布相对来说是比较均匀的。但是，模运算的消耗还是比较大的，这里我们用 &amp; 位运算来优化效率。 这个方法非常巧妙，它通过h &amp; (table.length -1)来得到该对象的保存位，而HashMap底层数组的长度总是 2 的 n 次方，这是HashMap在速度上的优化。当 length 总是 2 的 n 次方时，h&amp; (length-1)运算等价于对 length 取模，也就是h%length，但是 &amp; 比 % 具有更高的效率。 在JDK1.8的实现中，优化了高位运算的算法，通过hashCode()的高 16 位异或低 16 位实现的：(h = k.hashCode()) ^ (h &gt;&gt;&gt; 16)，主要是从速度、功效、质量来考虑的，这么做可以 Node 数组 table 的 length 比较小的时候，也能保证考虑到高低 Bit 都参与到 Hash 的计算中，同时不会有太大的开销。 JDK1.8HashMap扩容时做了哪些优化？扩容 (resize) 就是重新计算容量，向HashMap对象里不停的添加元素，而HashMap对象内部的数组无法装载更多的元素时，对象就需要扩大数组的长度，以便能装入更多的元素。 当然Java里的数组是无法自动扩容的，方法是使用一个新的数组代替已有的容量小的数组，就像我们用一个小桶装水，如果想装更多的水，就得换大水桶。 当put时，如果发现目前的bucket占用程度已经超过了Load Factor所希望的比例，那么就会发生resize。在resize的过程，简单的说就是把bucket扩充为 2 倍，之后重新计算index，把节点再放到新的bucket中。因为我们使用的是 2 次幂的扩展 (指长度扩为原来 2 倍)，所以，元素的位置要么是在原位置，要么是在原位置再移动 2 次幂的位置。 例如我们从 16 扩展为 32 时，具体的变化如下所示： 因此元素在重新计算hash之后，因为 n 变为 2 倍，那么 n-1 的 mask 范围在高位多 1bit(红色)，因此新的 index 就会发生这样的变化： 因此，我们在扩充HashMap的时候，不需要重新计算hash，只需要看看原来的hash值新增的那个 bit 是 1 还是 0 就好了，是 0 的话索引没变，是 1 的话索引变成 “原索引 + oldCap”。 这个设计确实非常的巧妙，既省去了重新计算 hash 值的时间，而且同时，由于新增的 1bit 是 0 还是 1 可以认为是随机的，因此resize的过程，均匀的把之前的冲突的节点分散到新的bucket了。 final Node&lt;K,V&gt;[] resize() &#123; // 扩容前的数组 Node&lt;K,V&gt;[] oldTab = table; // 扩容前的数组大小和阈值 int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; // 预定义新数组的大小和阈值 int newCap, newThr = 0; if (oldCap &gt; 0) &#123; // 超过最大值就不再扩容了 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; // 扩大容量为当前容量的两倍，但不能超过 MAXIMUM_CAPACITY else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; &#125; // 当前数组没有数据，使用初始化的值 else if (oldThr &gt; 0) newCap = oldThr; else &#123; // 如果初始化的值为 0， 则使用默认的初始化容量 newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr; @SuppressWarnings(&#123;&quot;rawtypes&quot;,&quot;unchecked&quot;&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; // 开始扩容，将新的容量赋值给 table table = newTab; // 原数据不为空，将原始的容量复制到新 table 中 if (oldTab != null) &#123; // 根据容量循环数组，复制非空元素到新 table for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; // 如果链表只有一个，则进行直接赋值 if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) // 红黑树相关的操作 ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // 链表复制,JDK 1.8 扩容优化部分 Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; // 原索引 if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; // 原索引 + oldCap else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); // 将原索引放入到哈希桶中 if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; // 将原索引 + oldCap 放到哈希桶中 if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab; &#125; 加载因子为什么是0.75？加载因子也叫扩容因子或负载因子，用来判断什么时候进行扩容的，假如加载因子是 0.5，HashMap 的初始化容量是 16，那么当 HashMap 中有 16*0.5=8 个元素时，HashMap 就会进行扩容。 那加载因子为什么是 0.75 而不是 0.5 或者 1.0 呢？ 这其实是出于容量和性能之间平衡的结果： 当加载因子设置比较大的时候，扩容的门槛就被提高了，扩容发生的频率比较低，占用的空间会比较小，但此时发生 Hash 冲突的几率就会提升，因此需要更复杂的数据结构来存储元素，这样对元素的操作时间就会增加，运行效率也会因此降低； 而当加载因子值比较小的时候，扩容的门槛会比较低，因此会占用更多的空间，此时元素的存储就比较稀疏，发生哈希冲突的可能性就比较小，因此操作性能会比较高。 所以综合了以上情况就取了一个 0.5 到 1.0 的平均数 0.75 作为加载因子。 当有哈希冲突时，HashMap是如何查找并确认元素的？ HashMap 源码中有哪些重要的方法？ HashMap 中的核心方法HashMap 源码中三个重要方法：查询、新增和数据扩容。 查询bucket 里的第一个节点，直接命中；如果有冲突，则通过 key.equals(k) 去查找对应的 entry若为树，则在树中通过 key.equals(k) 查找，O(logn)；若为链表，则在链表中通过 key.equals(k) 查找，O(n)。 具体代码的实现如下： public V get(Object key) &#123; Node&lt;K,V&gt; e; // 对 key 进行哈希操作 return (e = getNode(hash(key), key)) == null ? null : e.value; &#125; final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; // 非空判断 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; // 判断第一个元素是否是要查询的元素 if (first.hash == hash &amp;&amp; ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; // 一下一个节点非空判断 if ((e = first.next) != null) &#123; // 如果第一节点是树结构，则使用 getTreeNode 直接获取相应的数据 if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); // 非树结构循环节点判断 do &#123; // hash 相等并且 key 相同，则返回此节点 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null; &#125; 从以上源码可以看出，当哈希冲突时我们需要通过判断 key 值是否相等，才能确认此元素是不是我们想要的元素。 新增put 方法也是HashMap中比较重要的方法，因为通过该方法我们可以窥探到 HashMap 在内部是如何进行数据存储的，所谓的数组 + 链表 + 红黑树的存储结构是如何形成的，又是在何种情况下将链表转换成红黑树来优化性能的。 put 方法的大致实现过程如下： 对 key 的 hashCode() 做 hash，然后再计算 index; 如果没碰撞直接放到 bucket 里； 如果碰撞了，以链表的形式存在 buckets 后； 如果碰撞导致链表过长 (大于等于 TREEIFY_THRESHOLD)，就把链表转换成红黑树； 如果节点已经存在就替换 old value(保证 key 的唯一性) 如果 bucket 满了 (超过 load factor*current capacity)，就要 resize。 public V put(K key, V value) &#123; // 对 key 进行哈希操作 return putVal(hash(key), key, value, false, true); &#125; final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // 哈希表为空则创建 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // 根据 key 的哈希值计算出要插入的数组索引 i if ((p = tab[i = (n - 1) &amp; hash]) == null) // 如果 table[i] = null 则直接插入 tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; // 如果 key 已经存在了，直接覆盖 value if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; // 如果 key 不存在，判断是否为红黑树 else if (p instanceof TreeNode) // 红黑树直接插入键值对 e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; // 链表结构，循环准备插入 for (int binCount = 0; ; ++binCount) &#123; // 下一个元素为空时 if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); // 转换为红黑树进行处理 if (binCount &gt;= TREEIFY_THRESHOLD - 1) treeifyBin(tab, hash); break; &#125; // key 已经存在直接覆盖 value if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; if (e != null) &#123; V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; // 超过最大容量，扩容 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null; &#125; 新增方法的执行流程，如下图所示： HashMap 死循环分析以 JDK 1.7 为例，假设 HashMap 默认大小为 2，原本 HashMap 中有一个元素 key(5)，我们再使用两个线程：t1 添加元素 key(3)，t2 添加元素 key(7)，当元素 key(3) 和 key(7) 都添加到 HashMap 中之后，线程 t1 在执行到 Entry&lt;K,V&gt; next = e.next; 时，交出了 CPU 的使用权，源码如下： void transfer(Entry[] newTable, boolean rehash) &#123; int newCapacity = newTable.length; for (Entry&lt;K,V&gt; e: table) &#123; while (null != e) &#123; Entry&lt;K,V&gt; next = e.next; // 线程一执行到此处 if(rehash) &#123; e.hash = null == e.key ? 0 : hash(key); &#125; int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; &#125; &#125; &#125; 那么此时线程 t1 中的 e 指向了 key(3)，而 next 指向了 key(7) ；之后线程 t2 重新 rehash 之后链表的顺序被反转，链表的位置变成了 key(5) → key(7) → key(3)，其中 “→” 用来表示下一个元素。 当 t1 重新获得执行权之后，先执行 newTalbe[i] = e 把 key(3) 的 next 设置为 key(7)，而下次循环时查询到 key(7) 的 next 元素为 key(3)，于是就形成了 key(3) 和 key(7) 的循环引用，因此就导致了死循环的发生，如下图所示： 当然发生死循环的原因是 JDK 1.7 链表插入方式为首部倒序插入，这个问题在 JDK 1.8 得到了改善，变成了尾部正序插入。 参考文章https://coolshell.cn/articles/9606.html 其他方法构造方法public HashMap(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException(&quot;Illegal initial capacity: &quot; + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(&quot;Illegal load factor: &quot; + loadFactor); this.loadFactor = loadFactor; this.threshold = tableSizeFor(initialCapacity); &#125; 这是一个最基本的构造函数，需要调用方传入两个参数，initialCapacity 和 loadFactor。 程序的大部分代码在判断传入参数的合法性，initialCapacity 小于零将抛出异常，大于 MAXIMUM_CAPACITY 将被限定为 MAXIMUM_CAPACITY。loadFactor 如果小于等于零或者非数字类型也会抛出异常。 整个构造函数的核心在对 threshold 的初始化操作： static final int tableSizeFor(int cap) &#123; int n = cap - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1; &#125; 由以上代码可以看出，当在实例化HashMap实例时，如果给定了initialCapacity，由于HashMap的capacity都是 2 的幂次方，因此这个方法用于找到大于等于initialCapacity的最小的 2 的幂（initialCapacity 如果就是 2 的幂，则返回的还是这个数）。 下面分析这个算法： 首先，我们想一下为什么要对 cap 做减 1 操作？ int n = cap - 1 这是为了防止，cap 已经是 2 的幂。如果 cap 已经是 2 的幂，又没有执行这个减 1 操作，则执行完后面的几条无符号右移操作之后，返回的 capacity 将是这个 cap 的 2 倍。如果不懂，要看完后面的几个无符号右移之后再回来看看。 下面看看这几个无符号右移操作： 如果 n 这时为 0 了（经过了 cap-1 之后），则经过后面的几次无符号右移依然是 0，最后返回的 capacity 是 1（最后有个 n+1 的操作）。 这里我们只讨论 n 不等于 0 的情况。 n |= n &gt;&gt;&gt; 1; 由于 n 不等于 0，则 n 的二进制表示中总会有一 bit 为 1，这时考虑最高位的 1。通过无符号右移 1 位，则将最高位的 1 右移了 1 位，再做或操作，使得 n 的二进制表示中与最高位的 1 紧邻的右边一位也为 1，如 000011xxxxxx。 n |= n &gt;&gt;&gt; 2; 注意，这个 n 已经进行过 n |= n &gt;&gt;&gt; 1; 操作。假设此时 n 为 000011xxxxxx ，则 n 无符号右移两位，会将最高位两个连续的 1 右移两位，然后再与原来的 n 做或操作，这样 n 的二进制表示的高位中会有 4 个连续的 1。如 00001111xxxxxx 。 n |= n &gt;&gt;&gt; 4; 这次把已经有的高位中的连续的 4 个 1，右移 4 位，再做或操作，这样 n 的二进制表示的高位中会有 8 个连续的 1。如 00001111 1111xxxxxx 。 以此类推 。。。 注意，容量最大也就是 32bit 的正数，因此最后 n |= n &gt;&gt;&gt; 16; 最多也就 32 个 1，但是这时已经大于了 MAXIMUM_CAPACITY ，所以取值到 MAXIMUM_CAPACITY 。 下面我们通过一个图片来看一下整个过程： HashMap 中还有很多的重载构造函数，但几乎都是基于上述的构造函数的。 public HashMap(int initialCapacity) &#123; this(initialCapacity, DEFAULT_LOAD_FACTOR); &#125; public HashMap() &#123; this.loadFactor = DEFAULT_LOAD_FACTOR; &#125; 以上这些构造函数都没有直接的创建一个切实存在的数组，他们都是在为创建数组需要的一些参数做初始化，所以有些在构造函数中并没有被初始化的属性都会在实际初始化数组的时候用默认值替换。 实际对数组进行初始化是在添加元素的时候进行的（即 put 方法） public HashMap(Map&lt;? extends K, ? extends V&gt; m) &#123; this.loadFactor = DEFAULT_LOAD_FACTOR; putMapEntries(m, false); &#125; remove 方法删除操作就是一个查找 + 删除的过程，相对于添加操作其实容易一些 public V remove(Object key) &#123; Node&lt;K,V&gt; e; return (e = removeNode(hash(key), key, null, false, true)) == null ? null : e.value; &#125; 根据键值删除指定节点，这是一个最常见的操作了。显然，removeNode 方法是核心。 final Node&lt;K,V&gt; removeNode(int hash, Object key, Object value,boolean matchValue, boolean movable) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, index; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (p = tab[index = (n - 1) &amp; hash]) != null) &#123; Node&lt;K,V&gt; node = null, e; K k; V v; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) node = p; else if ((e = p.next) != null) &#123; if (p instanceof TreeNode) node = ((TreeNode&lt;K,V&gt;)p).getTreeNode(hash, key); else &#123; do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) &#123; node = e; break; &#125; p = e; &#125; while ((e = e.next) != null); &#125; &#125; if (node != null &amp;&amp; (!matchValue || (v = node.value) == value ||(value != null &amp;&amp; value.equals(v)))) &#123; if (node instanceof TreeNode) ((TreeNode&lt;K,V&gt;)node).removeTreeNode(this, tab, movable); else if (node == p) tab[index] = node.next; else p.next = node.next; ++modCount; --size; afterNodeRemoval(node); return node; &#125; &#125; return null; &#125; 删除操作需要保证在表不为空的情况下进行，并且 p 节点根据键的 hash 值对应到数组的索引，在该索引处必定有节点，如果为 null ，那么间接说明此键所对应的结点并不存在于整个 HashMap 中，这是不合法的，所以首先要在这两个大前提下才能进行删除结点的操作。 第一步 if (p.hash == hash &amp;&amp;((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) node = p 需要删除的结点就是这个头节点，让 node 引用指向它。否则说明待删除的结点在当前 p 所指向的头节点的链表或红黑树中，于是需要我们遍历查找。 第二步 else if ((e = p.next) != null) &#123; if (p instanceof TreeNode) node = ((TreeNode&lt;K,V&gt;)p).getTreeNode(hash, key); else &#123; do &#123; if (e.hash == hash &amp;&amp;((k = e.key) == key ||(key != null &amp;&amp; key.equals(k)))) &#123; node = e; break; &#125; p = e; &#125; while ((e = e.next) != null); &#125; &#125; 如果头节点是红黑树结点，那么调用红黑树自己的遍历方法去得到这个待删结点。否则就是普通链表，我们使用 do while 循环去遍历找到待删结点。找到节点之后，接下来就是删除操作了。 第三步 if (node != null &amp;&amp; (!matchValue || (v = node.value) == value ||(value != null &amp;&amp; value.equals(v)))) &#123; if (node instanceof TreeNode) ((TreeNode&lt;K,V&gt;)node).removeTreeNode(this, tab, movable); else if (node == p) tab[index] = node.next; else p.next = node.next; ++modCount; --size; afterNodeRemoval(node); return node; &#125; 删除操作也很简单，如果是红黑树结点的删除，直接调用红黑树的删除方法进行删除即可，如果是待删结点就是一个头节点，那么用它的 next 结点顶替它作为头节点存放在 table[index] 中，如果删除的是普通链表中的一个节点，用该结点的前一个节点直接跳过该待删结点指向它的 next 结点即可。 最后，如果 removeNode 方法删除成功将返回被删结点，否则返回 null。 keySettransient volatile Set&lt;K&gt; keySet; public Set&lt;K&gt; keySet() &#123; Set&lt;K&gt; ks; return (ks = keySet) == null ? (keySet = new KeySet()) : ks; &#125; final class KeySet extends AbstractSet&lt;K&gt; &#123; public final int size() &#123; return size; &#125; public final void clear() &#123; HashMap.this.clear(); &#125; public final Iterator&lt;K&gt; iterator() &#123; return new KeyIterator(); &#125; public final boolean contains(Object o) &#123; return containsKey(o); &#125; public final boolean remove(Object key) &#123; return removeNode(hash(key), key, null, false, true) != null; &#125; public final Spliterator&lt;K&gt; spliterator() &#123; return new KeySpliterator&lt;&gt;(HashMap.this, 0, -1, 0, 0); &#125; &#125; HashMap 中定义了一个 keySet 的实例属性，它保存的是整个 HashMap 中所有键的集合。上述所列出的 KeySet 类是 Set 的一个实现类，它负责为我们提供有关 HashMap 中所有对键的操作。 可以看到，KeySet 中的所有的实例方法都依赖当前的 HashMap 实例，也就是说，我们对返回的 keySet 集中的任意一个操作都会直接映射到当前 HashMap 实例中，例如你执行删除一个键的操作，那么 HashMap 中将会少一个节点。","categories":[{"name":"Java","slug":"Java","permalink":"https://gyl-coder.top/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://gyl-coder.top/tags/Java/"},{"name":"集合","slug":"集合","permalink":"https://gyl-coder.top/tags/%E9%9B%86%E5%90%88/"},{"name":"HashMap","slug":"HashMap","permalink":"https://gyl-coder.top/tags/HashMap/"}],"author":{"name":"yanliang","avatar":"https://cdn.jsdelivr.net/gh/gyl-coder/blogImgs/images/touxiang.webp","url":"https://gyl-coder.top"}},{"title":"int 和 Integer 有什么区别？谈谈Integer的值缓存范围","slug":"java/difference_between_int_and_integer","date":"2020-03-10T00:00:00.000Z","updated":"2020-12-27T04:47:15.974Z","comments":true,"path":"java/difference_between_int_and_integer/","link":"","permalink":"https://gyl-coder.top/java/difference_between_int_and_integer/","excerpt":"Java 虽然是面向对象的编程语言，但是依旧保留了一些原始数据类型，同时为了兼容java面向对象的特性，每种原始数据类型，都有对应的包装类型。 原始数据类型：boolean、char、byte、short、int、long、float、double包装类型：Boolean、Character、Byte、Short、Integer、Long、Float、Double 包装类型是对原始数据类型的一种封装，包含对原始类型数据的存储、基本操作以及两种类型直接的转换。Java5中，引入了 自动装箱和自动拆箱 功能（boxing/unboxing），Java可以根据上下文，自动进行转换，极大的简化了相关的编程。 Integer就是int对应的包装类，它有一个int类型的字段存储数据，并且提供了基本操作，比如数学运算、类型转换等。 关于Integer的值缓存，是Java5中的一个改进。构造一个Integer对象的正常操作应该是直接使用Integer的构造器，new一个对象出来。但是根据具体实践，大部分的操作都较为集中在有限且较小的数值范围。因此，Java5中新增了静态工厂方法 valueOf，在调用它的时候会利用一个缓存机制。缓存的数值范围为[-128, 127]。","text":"Java 虽然是面向对象的编程语言，但是依旧保留了一些原始数据类型，同时为了兼容java面向对象的特性，每种原始数据类型，都有对应的包装类型。 原始数据类型：boolean、char、byte、short、int、long、float、double包装类型：Boolean、Character、Byte、Short、Integer、Long、Float、Double 包装类型是对原始数据类型的一种封装，包含对原始类型数据的存储、基本操作以及两种类型直接的转换。Java5中，引入了 自动装箱和自动拆箱 功能（boxing/unboxing），Java可以根据上下文，自动进行转换，极大的简化了相关的编程。 Integer就是int对应的包装类，它有一个int类型的字段存储数据，并且提供了基本操作，比如数学运算、类型转换等。 关于Integer的值缓存，是Java5中的一个改进。构造一个Integer对象的正常操作应该是直接使用Integer的构造器，new一个对象出来。但是根据具体实践，大部分的操作都较为集中在有限且较小的数值范围。因此，Java5中新增了静态工厂方法 valueOf，在调用它的时候会利用一个缓存机制。缓存的数值范围为[-128, 127]。 知识点解读如何理解自动装箱、拆箱自动装箱实际上算是一种语法糖。Java会自动进行转换，保证不同的写法在运行时是等价的，他们发生在编译阶段，也就是生成的字节码是一致的。 语法糖虽好，用时小心 建议避免无意的装箱、拆线行为（创建100万个java对象和100万个整数的消耗不是一个数量级的，不管是内存使用还是处理速度，光是对象头的空间占用就已经是数量级的差距了。）","categories":[{"name":"Java","slug":"Java","permalink":"https://gyl-coder.top/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://gyl-coder.top/tags/Java/"},{"name":"Int","slug":"Int","permalink":"https://gyl-coder.top/tags/Int/"},{"name":"Integer","slug":"Integer","permalink":"https://gyl-coder.top/tags/Integer/"}],"author":{"name":"yanliang","avatar":"https://cdn.jsdelivr.net/gh/gyl-coder/blogImgs/images/touxiang.webp","url":"https://gyl-coder.top"}},{"title":"如何设计一个可扩展的限流算法 [转]","slug":"good/how-to-design-extensible-limiting-algorithm","date":"2020-03-05T00:00:00.000Z","updated":"2020-12-27T04:47:15.974Z","comments":true,"path":"good/how-to-design-extensible-limiting-algorithm/","link":"","permalink":"https://gyl-coder.top/good/how-to-design-extensible-limiting-algorithm/","excerpt":"限流（Rate Limiting，即速率限制）通过限制每个用户调用API的频率来防止API被过度使用，这可以防止他们因疏忽或恶意导致的API滥用。在没有速率限制的情况下，每个用户可以随心所欲地请求，这可能会导致“峰值”请求，从而导致其他用户得不到响应。在启用速率限制之后，它们的请求将被限制为每秒固定的数量。","text":"限流（Rate Limiting，即速率限制）通过限制每个用户调用API的频率来防止API被过度使用，这可以防止他们因疏忽或恶意导致的API滥用。在没有速率限制的情况下，每个用户可以随心所欲地请求，这可能会导致“峰值”请求，从而导致其他用户得不到响应。在启用速率限制之后，它们的请求将被限制为每秒固定的数量。 在示例图表中，你可以看到速率限制如何在一段时间内阻塞请求。API最初每分钟接收4个请求，用绿色表示。当12:02启用速率限制时，以红色显示的其他请求将被拒绝。 速率限制对于公共API是非常重要的，因为你想要为每个消费者（API调用者）维护良好的服务质量，即使有些用户获取了超出其公平配额的服务。计算密集型的端点特别需要速率限制——特别是通过自动伸缩或AWS Lambda和OpenWhisk等按计算付费服务来提供服务时。你还可能希望对提供敏感数据的API进行评级，因为如果攻击者在某些不可预见的事件中获得访问权限，这可能会限制暴露的数据。 实际上有许多不同的方法来实现速率限制，我们将探讨不同速率限制算法的优缺点。我们还将探讨跨集群扩展时出现的问题。最后，我们将向你展示一个如何使用Kong快速设置速率限制的示例，Kong是最流行的开源API网关。 速度限制算法有各种各样的速率限制算法，每一种都有自己的优点和缺点。让我们回顾一下，这样你就可以根据自己的需要选择最好的限流算法。 漏桶算法漏桶算法（Leaky Bucket，与令牌桶密切相关）是这样一种算法，它提供了一种简单、直观的方法来通过队列限制速率，你可以将队列看作一个存储请求的桶。当一个请求被注册时，它被附加到队列的末尾。每隔一段时间处理队列上的第一项。这也称为先进先出（FIFO）队列。如果队列已满，则丢弃（或泄漏）其他请求。 这种算法的优点是它可以平滑请求的爆发，并以近似平均的速度处理它们。它也很容易在单个服务器或负载均衡器上实现，并且在有限的队列大小下对于每个用户都是内存有效的。 然而，突发的访问量会用旧的请求填满队列，并使最近的请求无法被处理。它也不能保证在固定的时间内处理请求。此外，如果为了容错或增加吞吐量而负载平衡服务器，则必须使用策略来协调和强制它们之间的限制。稍后我们将讨论分布式环境的挑战。 固定窗口算法在固定窗口（Fixed Window）算法中，使用n秒的窗口大小（通常使用对人类友好的值，如60秒或3600秒）来跟踪速率。每个传入的请求都会增加窗口的计数器。如果计数器超过阈值，则丢弃请求。窗口通常由当前时间戳的层定义，因此12:00:03的窗口长度为60秒，应该在12:00:00的窗口中。 这种算法的优点是，它可以确保处理更多最近的请求，而不会被旧的请求饿死。然而，发生在窗口边界附近的单个流量突发会导致处理请求的速度增加一倍，因为它允许在短时间内同时处理当前窗口和下一个窗口的请求。另外，如果许多消费者等待一个重置窗口，例如在一个小时的顶部，那么他们可能同时扰乱你的API。 滑动日志算法滑动日志（Sliding Log）速率限制涉及到跟踪每个使用者请求的时间戳日志。这些日志通常存储在按时间排序的散列集或表中。时间戳超过阈值的日志将被丢弃。当新请求出现时，我们计算日志的总和来确定请求率。如果请求将超过阈值速率，则保留该请求。 该算法的优点是不受固定窗口边界条件的限制，速率限制将严格执行。此外，因为滑动日志是针对每个消费者进行跟踪的，所以不会出现对固定窗口造成挑战的踩踏效应。但是，为每个请求存储无限数量的日志可能非常昂贵。它的计算成本也很高，因为每个请求都需要计算使用者先前请求的总和，这些请求可能跨越一个服务器集群。因此，它不能很好地处理大规模的流量突发或拒绝服务攻击。 滑动窗口这是一种将固定窗口算法的低处理成本与改进的滑动日志边界条件相结合的混合方法。与固定窗口算法一样，我们跟踪每个固定窗口的计数器。接下来，我们根据当前的时间戳计算前一个窗口请求率的加权值，以平滑突发的流量。例如，如果当前窗口通过了25%，那么我们将前一个窗口的计数加权为75%。跟踪每个键所需的相对较少的数据点允许我们在大型集群中扩展和分布。 我们推荐使用滑动窗口方法，因为它可以灵活地调整速率限制，并且具有良好的性能。速率窗口是它向API消费者提供速率限制数据的一种直观方式。它还避免了漏桶的饥饿问题和固定窗口实现的突发问题。 分布式系统中的速率限制同步策略如果希望在使用多个节点的集群时实施全局速率限制，则必须设置策略来实施该限制。如果每个节点都要跟踪自己的速率限制，那么当请求被发送到不同节点时，使用者可能会超过全局速率限制。实际上，节点的数量越大，用户越有可能超过全局限制。 执行此限制的最简单方法是在负载均衡器中设置粘性会话，以便每个使用者都被精确地发送到一个节点。缺点包括缺乏容错性和节点过载时的缩放问题。 允许更灵活的负载平衡规则的更好解决方案是使用集中的数据存储，如Redis或Cassandra。这将存储每个窗口和消费者的计数。这种方法的两个主要问题是增加了向数据存储发出请求的延迟，以及竞争条件（我们将在下面讨论）。 竞态条件集中式数据存储的最大问题之一是高并发请求模式中的竞争条件。当你使用一种简单的“get-then-set”方法时，就会发生这种情况，在这种方法中，你检索当前速率限制计数器，增加它的值，然后将其推回到数据存储中。这个模型的问题是，在执行一个完整的读递增存储周期时，可能会出现额外的请求，每个请求都试图用无效的（较低的）计数器值存储递增计数器。这允许使用者发送非常高的请求率来绕过速率限制控制。 避免这个问题的一种方法是在有问题的密钥周围放置一个“锁”，防止任何其他进程访问或写入计数器。这将很快成为一个主要的性能瓶颈，而且伸缩性不好，特别是在使用诸如Redis之类的远程服务器作为备份数据存储时。 更好的方法是使用“先设置后获取”的心态，依赖于原子操作符，它们以一种非常高性能的方式实现锁，允许你快速增加和检查计数器值，而不让原子操作成为障碍。 性能优化使用集中式数据存储的另一个缺点是，在检查速率限制计数器时增加了延迟。不幸的是，即使是检查像Redis这样的快速数据存储，也会导致每个请求增加毫秒的延迟。 为了以最小的延迟确定这些速率限制，有必要在内存中进行本地检查。这可以通过放松速率检查条件和使用最终一致的模型来实现。例如，每个节点可以创建一个数据同步周期，该周期将与中央数据存储同步。每个节点定期将每个使用者的计数器增量和它看到的窗口推送到数据存储，数据存储将自动更新这些值。然后，节点可以检索更新后的值来更新其内存版本。集群内节点之间的这种收敛→发散→再收敛的循环最终是一致的。 节点聚合的周期速率应该是可配置的。当流量分布在群集中的多个节点上时（例如，当坐在一个轮询调度平衡器后面时），较短的同步间隔将导致较少的数据点分散，而较长的同步间隔将对数据存储施加较小的读/写压力，更少的开销在每个节点上获取新的同步值。 使用Kong快速设置速率限制Kong是一个开源的API网关，它使构建具有速率限制的可伸缩服务变得非常容易。它被全球超过300,000个活动实例使用。它可以完美地从单个的Kong节点扩展到大规模的、跨越全球的Kong集群。 Kong位于API前面，是上游API的主要入口。在处理请求和响应时，Kong将执行你决定添加到API中的任何插件。 Kong的速率限制插件是高度可配置的。它提供了为每个API和消费者定义多个速率限制窗口和速率的灵活性。它支持本地内存、Redis、Postgres和Cassandra备份数据存储。它还提供了各种数据同步选项，包括同步和最终一致的模型。 你可以在其中一台开发机器上快速安装Kong来测试它。我最喜欢的入门方式是使用AWS云形成模板，因为只需几次单击就可以获得预先配置好的开发机器。只需选择一个HVM选项，并将实例大小设置为使用t2.micro，这些对于测试都是负担得起的。然后ssh到新实例上的命令行进行下一步。 在Kong上添加API下一步是使用Kong的admin API在Kong上添加一个API。我们将使用httpbin作为示例，它是一个针对API的免费测试服务。get URL将把我的请求数据镜像成JSON。我们还假设Kong在本地系统上的默认端口上运行。 curl -i -X POST \\ --url http://localhost:8001/apis/ \\ --data &#39;name=test&#39; \\ --data &#39;uris=/test&#39; \\ --data &#39;upstream_url=http://httpbin.org/get&#39; 现在 Kong 意识到每个发送到 “/test” 的请求都应该代理到 httpbin。我们可以向它的代理端口上的 Kong 发出以下请求来测试它： curl http://localhost:8000/test &#123; &quot;args&quot;: &#123;&#125;, &quot;headers&quot;: &#123; &quot;Accept&quot;: &quot;*/*&quot;, &quot;Connection&quot;: &quot;close&quot;, &quot;Host&quot;: &quot;httpbin.org&quot;, &quot;User-Agent&quot;: &quot;curl/7.51.0&quot;, &quot;X-Forwarded-Host&quot;: &quot;localhost&quot; &#125;, &quot;origin&quot;: &quot;localhost, 52.89.171.202&quot;, &quot;url&quot;: &quot;http://localhost/get&quot; &#125; 它还是好的！Kong 已经接收了请求并将其代理到 httpbin，httpbin 已将我的请求头和我的原始 IP 地址进行了镜像。 添加基本的速率限制让我们继续，通过使用社区版的限速插件 [1] 添加限速功能来保护它不受过多请求的影响，每个消费者每分钟只能发出 5 个请求： curl -i -X POST http://localhost:8001/apis/test/plugins/ \\ -d &quot;name=rate-limiting&quot; \\ -d &quot;config.minute=5&quot; 如果我们现在发出超过 5 个请求，Kong 会回复以下错误信息： curl http://localhost:8000/test &#123; &quot;message&quot;:&quot;API rate limit exceeded&quot; &#125; 看上去不错！我们在 Kong 上添加了一个 API，并且仅在两个 HTTP 请求中向 Kong 的 admin API 添加了速率限制。 它默认使用固定的窗口来限制 IP 地址的速率，并使用默认的数据存储在集群中的所有节点之间进行同步。有关其他选项，包括每个用户的速率限制或使用其他数据存储（如 Redis），请参阅文档 [1]。 企业版 Kong，更好的性能企业版 [2] 的速率限制增加了对滑动窗口算法的支持，以更好地控制和性能。滑动窗口可以防止你的 API 在窗口边界附近重载，如上面的部分所述。对于低延迟，它使用计数器的内存表，可以使用异步或同步更新跨集群进行同步。这提供了本地阈值的延迟，并且可以跨整个集群扩展。 第一步是安装企业版的 Kong。然后，可以配置速率限制、以秒为单位的窗口大小和同步计数器值的频率。它真的很容易使用，你可以得到这个强大的控制与一个简单的 API 调用： curl -i -X POST http://localhost:8001/apis/test/plugins \\ -d &quot;name=rate-limiting&quot; \\ -d &quot;config.limit=5&quot; \\ -d &quot;config.window_size=60&quot; \\ -d &quot;config.sync_rate=10&quot; 企业还增加了对 Redis Sentinel 的支持，这使得 Redis 高可用性和更强的容错能力。你可以阅读更多的企业速率限制插件文档。 阅读原文https://mp.weixin.qq.com/s?__biz=MzA5OTAyNzQ2OA==&mid=2649704593&idx=1&sn=bccc83c5fc87bf4f032f91405be15cd4&scene=21#wechat_redirect'>","categories":[{"name":"设计","slug":"设计","permalink":"https://gyl-coder.top/categories/%E8%AE%BE%E8%AE%A1/"}],"tags":[{"name":"限流算法","slug":"限流算法","permalink":"https://gyl-coder.top/tags/%E9%99%90%E6%B5%81%E7%AE%97%E6%B3%95/"},{"name":"设计","slug":"设计","permalink":"https://gyl-coder.top/tags/%E8%AE%BE%E8%AE%A1/"}],"author":{"name":"yanliang","avatar":"https://cdn.jsdelivr.net/gh/gyl-coder/blogImgs/images/touxiang.webp","url":"https://gyl-coder.top"}},{"title":"kafka 主题管理","slug":"kafka/kafka-topic-manage","date":"2019-12-08T00:00:00.000Z","updated":"2020-12-27T04:47:15.978Z","comments":true,"path":"kafka/kafka-topic-manage/","link":"","permalink":"https://gyl-coder.top/kafka/kafka-topic-manage/","excerpt":"对于 kafka 主题（topic）的管理（增删改查），使用最多的便是kafka自带的脚本。","text":"对于 kafka 主题（topic）的管理（增删改查），使用最多的便是kafka自带的脚本。 创建主题kafka提供了自带的 kafka-topics 脚本，用来帮助用户创建主题（topic）。 bin/kafka-topics.sh --bootstrap-server broker_host:port --create --topic my_topic_name --partitions 1 --replication-factor 1 create 表明我们要创建主题，而 partitions 和 replication factor 分别设置了主题的分区数以及每个分区下的副本数。 这里为什么用的 --bootstrap-server 参数，而不是 --zookeeper ?--zookeeper 参数是之前版本的用法，从kafka 2.2 版本开始，社区推荐使用 --bootstrap-server 参数替换 --zoookeeper ，并且显式地将后者标记为 “已过期”，因此，如果你已经在使用 2.2 版本了，那么创建主题请指定 --bootstrap-server 参数。 推荐使用 --bootstrap-server 而非 --zookeeper 的原因主要有两个。 使用 –zookeeper 会绕过 Kafka 的安全体系。这就是说，即使你为 Kafka 集群设置了安全认证，限制了主题的创建，如果你使用 –zookeeper 的命令，依然能成功创建任意主题，不受认证体系的约束。这显然是 Kafka 集群的运维人员不希望看到的。 使用 –bootstrap-server 与集群进行交互，越来越成为使用 Kafka 的标准姿势。换句话说，以后会有越来越少的命令和 API 需要与 ZooKeeper 进行连接。这样，我们只需要一套连接信息，就能与 Kafka 进行全方位的交互，不用像以前一样，必须同时维护 ZooKeeper 和 Broker 的连接信息。 查询主题创建好主题之后，Kafka 允许我们使用相同的脚本查询主题。你可以使用下面的命令，查询所有主题的列表。 bin/kafka-topics.sh --bootstrap-server broker_host:port --list 如果要查询单个主题的详细数据，你可以使用下面的命令。 bin/kafka-topics.sh --bootstrap-server broker_host:port --describe --topic &lt;topic_name&gt; 如果 describe 命令不指定具体的主题名称，那么 Kafka 默认会返回所有 “可见” 主题的详细数据给你。 这里的 “可见”，是指发起这个命令的用户能够看到的 Kafka 主题。这和前面说到主题创建时，使用 –zookeeper 和 –bootstrap-server 的区别是一样的。如果指定了 –bootstrap-server，那么这条命令就会受到安全认证体系的约束，即对命令发起者进行权限验证，然后返回它能看到的主题。否则，如果指定 –zookeeper 参数，那么默认会返回集群中所有的主题详细数据。基于这些原因，我建议你最好统一使用 –bootstrap-server 连接参数。 修改主题修改主题分区其实就是增加分区，目前 Kafka 不允许减少某个主题的分区数。你可以使用 kafka-topics 脚本，结合 –alter 参数来增加某个主题的分区数，命令如下： bin/kafka-topics.sh --bootstrap-server broker_host:port --alter --topic &lt;topic_name&gt; --partitions &lt; 新分区数 &gt; 这里要注意的是，你指定的分区数一定要比原有分区数大，否则 Kafka 会抛出 InvalidPartitionsException 异常。 修改主题级别参数在主题创建之后，我们可以使用 kafka-configs 脚本修改对应的参数。 假设我们要设置主题级别参数 max.message.bytes，那么命令如下： bin/kafka-configs.sh --zookeeper zookeeper_host:port --entity-type topics --entity-name &lt;topic_name&gt; --alter --add-config max.message.bytes=10485760 也许你会觉得奇怪，为什么这个脚本就要指定 –zookeeper，而不是 –bootstrap-server 呢？其实，这个脚本也能指定 –bootstrap-server 参数，只是它是用来设置动态参数的。在专栏后面，我会详细介绍什么是动态参数，以及动态参数都有哪些。现在，你只需要了解设置常规的主题级别参数，还是使用 –zookeeper。 变更副本数使用自带的 kafka-reassign-partitions 脚本，帮助我们增加主题的副本数。 假设kafka的内部主题 __consumer_offsets 只有 1 个副本，现在我们想要增加至 3 个副本。下面是操作： 创建一个 json 文件，显式提供 50 个分区对应的副本数。注意，replicas 中的 3 台 Broker 排列顺序不同，目的是将 Leader 副本均匀地分散在 Broker 上。该文件具体格式如下 &#123;&quot;version&quot;:1, &quot;partitions&quot;:[ &#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[0,1,2]&#125;, &#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[0,2,1]&#125;, &#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[1,0,2]&#125;, &#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:3,&quot;replicas&quot;:[1,2,0]&#125;, ... &#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:49,&quot;replicas&quot;:[0,1,2]&#125; ]&#125; 执行 kafka-reassign-patitions 脚本，命令如下： bin/kafka-reassign-partitions.sh --zookeeper zookeeper_host:port --reassignment-json-file reassign.json --execute 除了修改内部主题，我们可能还想查看这些内部主题的消息内容。特别是对于 __consumer_offsets 而言，由于它保存了消费者组的位移数据，有时候直接查看该主题消息是很方便的事情。下面的命令可以帮助我们直接查看消费者组提交的位移数据。 bin/kafka-console-consumer.sh --bootstrap-server kafka_host:port --topic __consumer_offsets --formatter &quot;kafka.coordinator.group.GroupMetadataManager\\$OffsetsMessageFormatter&quot; --from-beginning 除了查看位移提交数据，我们还可以直接读取该主题消息，查看消费者组的状态信息。 bin/kafka-console-consumer.sh --bootstrap-server kafka_host:port --topic __consumer_offsets --formatter &quot;kafka.coordinator.group.GroupMetadataManager\\$GroupMetadataMessageFormatter&quot; --from-beginning 对于内部主题 __transaction_state 而言，方法是相同的。你只需要指定 kafka.coordinator.transaction.TransactionLog$TransactionLogMessageFormatter 即可。 修改主题限速这里主要是指设置 Leader 副本和 Follower 副本使用的带宽。有时候，我们想要让某个主题的副本在执行副本同步机制时，不要消耗过多的带宽。Kafka 提供了这样的功能。我来举个例子。假设我有个主题，名为 test，我想让该主题各个分区的 Leader 副本和 Follower 副本在处理副本同步时，不得占用超过 100MBps 的带宽。注意是大写 B，即每秒不超过 100MB。那么，我们应该怎么设置呢？ 要达到这个目的，我们必须先设置 Broker 端参数 leader.replication.throttled.rate 和 follower.replication.throttled.rate，命令如下： bin/kafka-configs.sh --zookeeper zookeeper_host:port --alter --add-config &#39;leader.replication.throttled.rate=104857600,follower.replication.throttled.rate=104857600&#39; --entity-type brokers --entity-name 0 这条命令结尾处的 –entity-name 就是 Broker ID。倘若该主题的副本分别在 0、1、2、3 多个 Broker 上，那么你还要依次为 Broker 1、2、3 执行这条命令。 设置好这个参数之后，我们还需要为该主题设置要限速的副本。在这个例子中，我们想要为所有副本都设置限速，因此统一使用通配符 * 来表示，命令如下： bin/kafka-configs.sh --zookeeper zookeeper_host:port --alter --add-config &#39;leader.replication.throttled.replicas=*,follower.replication.throttled.replicas=*&#39; --entity-type topics --entity-name test 主题分区迁移同样是使用 kafka-reassign-partitions 脚本，对主题各个分区的副本进行 “手术” 般的调整，比如把某些分区批量迁移到其他 Broker 上。 删除主题bin/kafka-topics.sh --bootstrap-server broker_host:port --delete --topic &lt;topic_name&gt; 删除主题的命令并不复杂，关键是删除操作是异步的，执行完这条命令不代表主题立即就被删除了。它仅仅是被标记成 “已删除” 状态而已。Kafka 会在后台默默地开启主题删除操作。因此，通常情况下，你都需要耐心地等待一段时间。 主题删除失败当运行完上面的删除命令后，很多人发现已删除主题的分区数据依然 “躺在” 硬盘上，没有被清除。这时该怎么办呢？ 实际上，造成主题删除失败的原因有很多，最常见的原因有两个： 副本所在的 Broker 宕机了 待删除主题的部分分区依然在执行迁移过程。 如果是因为前者，通常你重启对应的 Broker 之后，删除操作就能自动恢复；如果是因为后者，那就麻烦了，很可能两个操作会相互干扰。 不管什么原因，一旦你碰到主题无法删除的问题，可以采用这样的方法： 手动删除 ZooKeeper 节点 /admin/delete_topics 下以待删除主题为名的 znode。 手动删除该主题在磁盘上的分区目录。 在 ZooKeeper 中执行 rmr /controller，触发 Controller 重选举，刷新 Controller 缓存。 在执行最后一步时，你一定要谨慎，因为它可能造成大面积的分区 Leader 重选举。事实上，仅仅执行前两步也是可以的，只是 Controller 缓存中没有清空待删除主题罢了，也不影响使用。 常见问题__consumer_offsets 占用太多的磁盘一旦你发现这个主题消耗了过多的磁盘空间，那么，你一定要显式地用 jstack 命令查看一下 kafka-log-cleaner-thread 前缀的线程状态。通常情况下，这都是因为该线程挂掉了，无法及时清理此内部主题。倘若真是这个原因导致的，那我们就只能重启相应的 Broker 了。另外，请你注意保留出错日志，因为这通常都是 Bug 导致的，最好提交到社区看一下。","categories":[{"name":"kafka","slug":"kafka","permalink":"https://gyl-coder.top/categories/kafka/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"https://gyl-coder.top/tags/kafka/"},{"name":"topic","slug":"topic","permalink":"https://gyl-coder.top/tags/topic/"}],"author":{"name":"yanliang","avatar":"https://cdn.jsdelivr.net/gh/gyl-coder/blogImgs/images/touxiang.webp","url":"https://gyl-coder.github.io"}},{"title":"kafka 问题总结","slug":"kafka/kafka-todo-list","date":"2019-12-07T00:00:00.000Z","updated":"2020-12-27T04:47:15.978Z","comments":true,"path":"kafka/kafka-todo-list/","link":"","permalink":"https://gyl-coder.top/kafka/kafka-todo-list/","excerpt":"Kafka中的基本概念 Kafka中的ISR、AR又代表什么？ISR的伸缩又指什么？ Kafka中的HW、LEO、LSO等分别代表什么？ kafka如何保证数据可靠性和数据一致性 Kafka Rebalance机制分析 kafka 主题管理 Kafka有哪几处地方有分区分配的概念？简述大致的过程及原理 Kafka中是怎么体现消息顺序性的？","text":"Kafka中的基本概念 Kafka中的ISR、AR又代表什么？ISR的伸缩又指什么？ Kafka中的HW、LEO、LSO等分别代表什么？ kafka如何保证数据可靠性和数据一致性 Kafka Rebalance机制分析 kafka 主题管理 Kafka有哪几处地方有分区分配的概念？简述大致的过程及原理 Kafka中是怎么体现消息顺序性的？ Kafka的用途有哪些？使用场景如何？ Kafka生产者客户端的整体结构是什么样子的？ Kafka生产者客户端中使用了几个线程来处理？分别是什么？ Kafka中的分区器、序列化器、拦截器是否了解？它们之间的处理顺序是什么？ 消费再均衡的原理是什么？（提示：消费者协调器和消费组协调器） “消费组中的消费者个数如果超过topic的分区，那么就会有消费者消费不到数据” 这句话是否正确？如果正确，那有没有什么hack的手段？ Kafka的旧版Scala的消费者客户端的设计有什么缺陷？ 消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset+1? 有哪些情形会造成重复消费？ 那些情景下会造成消息漏消费？ KafkaConsumer是非线程安全的，那么怎么样实现多线程消费？ 简述消费者与消费组之间的关系 当你使用kafka-topics.sh创建（删除）了一个topic之后，Kafka背后会执行什么逻辑？ topic的分区数可不可以增加？如果可以怎么增加？如果不可以，那又是为什么？ topic的分区数可不可以减少？如果可以怎么减少？如果不可以，那又是为什么？ 创建topic时如何选择合适的分区数？ Kafka目前有那些内部topic，它们都有什么特征？各自的作用又是什么？ 优先副本是什么？它有什么特殊的作用？ 为什么Kafka不支持读写分离？ Kafka中有那些地方需要选举？这些地方的选举策略又有哪些？ 失效副本是指什么？有那些应对措施？ 多副本下，各个副本中的HW和LEO的演变过程 聊一聊Kafka控制器的作用 简述Kafka的日志目录结构 Kafka中有那些索引文件？ 如果我指定了一个offset，Kafka怎么查找到对应的消息？ 如果我指定了一个timestamp，Kafka怎么查找到对应的消息？ 聊一聊你对Kafka的Log Retention的理解 聊一聊你对Kafka的Log Compaction的理解 聊一聊你对Kafka底层存储的理解（页缓存、内核层、块层、设备层） 聊一聊Kafka的延时操作的原理 Kafka中的幂等是怎么实现的 Kafka中的事务是怎么实现的（这题我去面试6加被问4次，照着答案念也要念十几分钟，面试官简直凑不要脸) Kafka中怎么实现死信队列和重试队列？ Kafka中的延迟队列怎么实现（这题被问的比事务那题还要多！！！听说你会Kafka，那你说说延迟队列怎么实现？） Kafka中怎么做消息审计？ Kafka中怎么做消息轨迹？ Kafka中有那些配置参数比较有意思？聊一聊你的看法 Kafka中有那些命名比较有意思？聊一聊你的看法 Kafka有哪些指标需要着重关注？ 怎么计算Lag？(注意read_uncommitted和read_committed状态下的不同) Kafka的那些设计让它有如此高的性能？ Kafka有什么优缺点？ 还用过什么同质类的其它产品，与Kafka相比有什么优缺点？ 为什么选择Kafka? 在使用Kafka的过程中遇到过什么困难？怎么解决的？ 怎么样才能确保Kafka极大程度上的可靠性？ 聊一聊你对Kafka生态的理解","categories":[{"name":"kafka","slug":"kafka","permalink":"https://gyl-coder.top/categories/kafka/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"https://gyl-coder.top/tags/kafka/"}],"author":{"name":"yanliang","avatar":"https://cdn.jsdelivr.net/gh/gyl-coder/blogImgs/images/touxiang.webp","url":"https://gyl-coder.github.io"}},{"title":"kafka 如何保障数据可靠性？","slug":"kafka/kafka-reliability","date":"2019-12-06T00:00:00.000Z","updated":"2020-12-27T04:47:15.978Z","comments":true,"path":"kafka/kafka-reliability/","link":"","permalink":"https://gyl-coder.top/kafka/kafka-reliability/","excerpt":"Kafka 中采用了多副本的机制，这是大多数分布式系统中惯用的手法，以此来实现水平扩展、提供容灾能力、提升可用性和可靠性等。 我们对此可以引申出一系列的疑问： Kafka 多副本之间如何进行数据同步，尤其是在发生异常时候的处理机制又是什么？ 多副本间的数据一致性如何解决，基于的一致性协议又是什么？ 如何确保Kafka 的可靠性？ Kafka 中的可靠性和可用性之间的关系又如何？ 下面从副本的角度切入来看看Kafka如何保障数据一致性、数据可靠性等问题，主要包括副本剖析、日志同步机制和可靠性分析等内容。","text":"Kafka 中采用了多副本的机制，这是大多数分布式系统中惯用的手法，以此来实现水平扩展、提供容灾能力、提升可用性和可靠性等。 我们对此可以引申出一系列的疑问： Kafka 多副本之间如何进行数据同步，尤其是在发生异常时候的处理机制又是什么？ 多副本间的数据一致性如何解决，基于的一致性协议又是什么？ 如何确保Kafka 的可靠性？ Kafka 中的可靠性和可用性之间的关系又如何？ 下面从副本的角度切入来看看Kafka如何保障数据一致性、数据可靠性等问题，主要包括副本剖析、日志同步机制和可靠性分析等内容。 副本剖析副本（Replica）是分布式系统中常见的概念之一，指的是分布式系统对数据和服务提供的一种冗余方式。在常见的分布式系统中，为了对外提供可用的服务，我们往往会对数据和服务进行副本处理。数据副本是指在不同的节点上持久化同一份数据，当某一个节点上存储的数据丢失时，可以从副本上读取该数据，这是解决分布式系统数据丢失问题最有效的手段。另一类副本是服务副本，指多个节点提供同样的服务，每个节点都有能力接收来自外部的请求并进行相应的处理。 Kafka从0.8版本开始为分区引入了多副本机制，通过增加副本数量来提升数据容灾能力。同时，Kafka通过多副本机制实现故障自动转移，在Kafka集群中某个broker节点失效的情况下仍然保证服务可用。 一下内容会涉及到AR、ISR、HW等基础概念，下面我们先简要回顾一下，详情请参考 &lt;Kafka中的基本概念&gt; 副本是相对于分区而言的，即副本是特定分区的副本。 一个分区中包含一个或多个副本，其中一个为leader副本，其余为follower副本，各个副本位于不同的broker节点中。只有leader副本对外提供服务，follower副本只负责数据同步。 分区中的所有副本统称为 AR，而ISR 是指与leader 副本保持同步状态的副本集合，当然leader副本本身也是这个集合中的一员。 LEO标识每个分区中最后一条消息的下一个位置，分区的每个副本都有自己的LEO，ISR中最小的LEO即为HW，俗称高水位，消费者只能拉取到HW之前的消息。从生产者发出的一条消息首先会被写入分区的leader副本，不过还需要等待ISR集合中的所有 follower 副本都同步完之后才能被认为已经提交，之后才会更新分区的 HW，进而消费者可以消费到这条消息。 从生产者发出的一条消息首先会被写入分区的leader副本，不过还需要等待ISR集合中的所有 follower 副本都同步完之后才能被认为已经提交，之后才会更新分区的 HW，进而消费者可以消费到这条消息。 失效副本正常情况下，分区的所有副本都处于ISR集合中，但是难免会有异常情况发生，从而某些副本被剥离出ISR集合中。在ISR集合之外，也就是处于同步失效或功能失效（比如副本处于非存活状态）的副本统称为失效副本，失效副本对应的分区也就称为同步失效分区（under-replicated分区）。 可以通过 kafka-topic.sh 脚本的 under-replicated-partitions 参数来显示主题中包含失效副本的分区。 bin/kafka-topic.sh --zookeeper localhost:2181 --describe --topic topic-partitions -under-replicatied-partitions 失效副本不仅是指处于功能失效状态的副本，处于同步失效状态的副本也可以看作失效副本。怎么判定一个分区是否有副本处于同步失效的状态呢？Kafka从0.9.x版本开始就通过唯一的broker端参数 replica.lag.time.max.ms 来抉择，当ISR集合中的一个follower副本滞后leader副本的时间超过此参数指定的值时则判定为同步失败，需要将此follower副本剔除出ISR集合，replica.lag.time.max.ms 参数的默认值为10000。 具体的实现原理也很容易理解，当follower副本将leader副本LEO（LogEndOffset）之前的日志全部同步时，则认为该 follower 副本已经追赶上leader 副本，此时更新该副本的lastCaughtUpTimeMs 标识。Kafka 的副本管理器会启动一个副本过期检测的定时任务，而这个定时任务会定时检查当前时间与副本的 lastCaughtUpTimeMs 差值是否大于参数replica.lag.time.max.ms 指定的值。 什么情况会导致副本失效？ follower副本进程卡住，在一段时间内根本没有向leader副本发起同步请求，比如频繁的Full GC。 follower副本进程同步过慢，在一段时间内都无法追赶上leader副本，比如I/O开销过大。 当通过脚本工具增加了副本因子，新增加的副本因子在赶上leader之前都处于失效状态 ISR的伸缩Kafka 在启动的时候会开启两个与 ISR 相关的定时任务，名称分别为“isr-expiration”和“isr-change-propagation”。 isr-expiration任务会周期性地检测每个分区是否需要缩减其ISR集合。这个周期和 replica.lag.time.max.ms 参数有关，大小是这个参数值的一半，默认值为5000ms。当检测到ISR集合中有失效副本时，就会收缩ISR集合。如果某个分区的ISR集合发生变更，则会将变更后的数据记录到 ZooKeeper 对应的 /brokers/topics/＜topic＞/partition/＜parititon＞/state 节点中。节点中的数据示例如下： &#123;&quot;controller epoch&quot;: 26, &quot;leader&quot;: 0, &quot;version&quot;: 1, &quot;leader epoch&quot;: 2, &quot;isr&quot;: [0, 1]&#125; controller_epoch 表示当前Kafka控制器的epoch leader表示当前分区的leader副本所在的broker的id编号 version表示版本号（当前版本固定为1） leader_epoch表示当前分区的leader纪元 isr表示变更后的ISR列表。 当 ISR 集合发生变更时还会将变更后的记录缓存到 isrChangeSet 中，isr-change-propagation任务会周期性（固定值为 2500ms）地检查isrChangeSet，如果发现isrChangeSet中有ISR集合的变更记录，那么它会在ZooKeeper的/isr_change_notification路径下创建一个以 isr_change_开头的持久顺序节点（比如/isr_change_notification/isr_change_0000000000），并将isrChangeSet中的信息保存到这个节点中。 Kafka控制器为/isr_change_notification添加了一个Watcher，当这个节点中有子节点发生变化时会触发Watcher的动作，以此通知控制器更新相关元数据信息并向它管理的broker节点发送更新元数据的请求，最后删除/isr_change_notification路径下已经处理过的节点。频繁地触发Watcher会影响Kafka控制器、ZooKeeper甚至其他broker节点的性能。为了避免这种情况，Kafka添加了限定条件，当检测到分区的ISR集合发生变化时，还需要检查以下两个条件： 上一次ISR集合发生变化距离现在已经超过5s。 上一次写入ZooKeeper的时间距离现在已经超过60s。 满足以上两个条件之一才可以将ISR集合的变化写入目标节点。 有缩减对应就会有扩充，那么Kafka又是何时扩充ISR的呢？ 随着follower副本不断与leader副本进行消息同步，follower副本的LEO也会逐渐后移，并最终追赶上leader副本，此时该follower副本就有资格进入ISR集合。追赶上leader副本的判定准则是此副本的LEO是否不小于leader副本的HW，注意这里并不是和leader副本的LEO相比。ISR扩充之后同样会更新ZooKeeper中的/brokers/topics/＜topic＞/partition/＜parititon＞/state节点和isrChangeSet，之后的步骤就和ISR收缩时的相同。 当ISR集合发生增减时，或者ISR集合中任一副本的LEO发生变化时，都可能会影响整个分区的HW。 例如如下，leader副本的LEO为9，follower1副本的LEO为7，而follower2副本的LEO为6，如果判定这3个副本都处于ISR集合中，那么这个分区的HW为6；如果follower3已经被判定为失效副本被剥离出ISR集合，那么此时分区的HW为leader副本和follower1副本中LEO的最小值，即为7。 LEO 和 HW这两个概念可以参考： 对于副本而言，还有两个概念：本地副本（Local Replica）和远程副本（RemoteReplica），本地副本是指对应的Log分配在当前的broker节点上，远程副本是指对应的Log分配在其他的broker节点上。在Kafka中，同一个分区的信息会存在多个broker节点上，并被其上的副本管理器所管理，这样在逻辑层面每个broker节点上的分区就有了多个副本，但是只有本地副本才有对应的日志。 整个消息追加的过程可以概括如下： 生产者客户端发送消息至leader副本（副本1）中。 消息被追加到leader副本的本地日志，并且会更新日志的偏移量。 follower副本（副本2和副本3）向leader副本请求同步数据。 leader副本所在的服务器读取本地日志，并更新对应拉取的follower副本的信息。 leader副本所在的服务器将拉取结果返回给follower副本。 follower副本收到leader副本返回的拉取结果，将消息追加到本地日志中，并更新日志的偏移量信息。 了解了这些内容后，我们再来分析在这个过程中各个副本LEO和HW的变化情况。下面的示例，生产者一直在往leader副本中写入消息。某一时刻，leader副本的LEO增加至5，并且所有副本的HW还都为0。之后follower副本向leader副本拉取消息，在拉取的请求中会带有自身的LEO信息，这个LEO信息对应的是FetchRequest请求中的fetch_offset。leader副本返回给follower副本相应的消息，并且还带有自身的HW信息，如图8-5所示，这个HW信息对应的是FetchResponse中的high_watermark。 此时两个follower副本各自拉取到了消息，并更新各自的LEO为3和4。与此同时，follower副本还会更新自己的HW，更新HW的算法是比较当前LEO和leader副本中传送过来的HW的值，取较小值作为自己的HW值。当前两个follower副本的HW都等于0（min（0，0）=0）。 接下来follower副本再次请求拉取leader副本中的消息。 此时leader副本收到来自follower副本的FetchRequest请求，其中带有LEO的相关信息，选取其中的最小值作为新的HW，即min（15，3，4）=3。然后连同消息和HW一起返回FetchResponse给follower副本。注意leader副本的HW是一个很重要的东西，因为它直接影响了分区数据对消费者的可见性。 两个follower副本在收到新的消息之后更新LEO并且更新自己的HW为3（min（LEO，3）=3）。 在一个分区中，leader副本所在的节点会记录所有副本的LEO，而follower副本所在的节点只会记录自身的LEO，而不会记录其他副本的LEO。对HW而言，各个副本所在的节点都只记录它自身的HW。leader 副本收到 follower副本的FetchRequest请求之后，它首先会从自己的日志文件中读取数据，然后在返回给follower副本数据前先更新follower副本的LEO。 Kafka 的根目录下有 cleaner-offset-checkpoint、log-start-offset-checkpoint、recovery-point-offset-checkpoint和replication-offset-checkpoint四个检查点文件 recovery-point-offset-checkpoint 和replication-offset-checkpoint 这两个文件分别对应了 LEO和 HW。Kafka 中会有一个定时任务负责将所有分区的 LEO 刷写到恢复点文件 recovery-point-offset-checkpoint 中，定时周期由 broker 端参数 log.flush.offset.checkpoint.interval.ms来配置，默认值为60000。还有一个定时任务负责将所有分区的HW刷写到复制点文件replication-offset-checkpoint中，定时周期由broker端参数replica.high.watermark.checkpoint.interval.ms来配置，默认值为5000。 log-start-offset-checkpoint文件对应logStartOffset（注意不能缩写为LSO，因为在Kafka中LSO是LastStableOffset的缩写），在FetchRequest和FetchResponse中也有它的身影，它用来标识日志的起始偏移量。各个副本在变动 LEO 和 HW 的过程中，logStartOffset 也有可能随之而动。Kafka 也有一个定时任务来负责将所有分区的 logStartOffset书写到起始点文件log-start-offset-checkpoint中，定时周期由broker端参数log.flush.start.offset.checkpoint.interval.ms来配置，默认值为60000。 Leader Epoch日志同步机制在分布式系统中，日志同步机制既要保证数据的一致性，也要保证数据的顺序性。虽然有许多方式可以实现这些功能，但最简单高效的方式还是从集群中选出一个leader来负责处理数据写入的顺序性。只要leader还处于存活状态，那么follower只需按照leader中的写入顺序来进行同步即可。 通常情况下，只要leader不宕机我们就不需要关心follower的同步问题。不过当leader宕机时，我们就要从follower中选举出一个新的leader。follower的同步状态可能落后leader很多，甚至还可能处于宕机状态，所以必须确保选择具有最新日志消息的follower作为新的leader。日志同步机制的一个基本原则就是：如果告知客户端已经成功提交了某条消息，那么即使 leader宕机，也要保证新选举出来的leader中能够包含这条消息。这里就有一个需要权衡（tradeoff）的地方，如果leader在消息被提交前需要等待更多的follower确认，那么在它宕机之后就可以有更多的follower替代它，不过这也会造成性能的下降。 对于这种tradeoff，一种常见的做法是“少数服从多数”，“少数服从多数”的方式有一个很大的优势，系统的延迟取决于最快的几个节点，比如副本数为3，那么延迟就取决于最快的那个follower而不是最慢的那个（除了leader，只需要另一个follower确认即可）。不过它也有一些劣势，为了保证leader选举的正常进行，它所能容忍的失败follower数比较少，如果要容忍1个follower失败，那么至少要有3个副本，如果要容忍2个follower失败，必须要有5个副本。也就是说，在生产环境下为了保证较高的容错率，必须要有大量的副本，而大量的副本又会在大数据量下导致性能的急剧下降。这也就是“少数服从多数”的这种Quorum模型常被用作共享集群配置（比如ZooKeeper），而很少用于主流的数据存储中的原因。 与“少数服从多数”相关的一致性协议有很多，比如Zab、Raft和ViewstampedReplication等。而Kafka使用的更像是微软的PacificA算法。在Kafka中动态维护着一个ISR集合，处于ISR集合内的节点保持与leader相同的高水位（HW），只有位列其中的副本（unclean.leader.election.enable配置为false）才有资格被选为新的 leader。写入消息时只有等到所有 ISR 集合中的副本都确认收到之后才能被认为已经提交。位于 ISR 中的任何副本节点都有资格成为leader，选举过程简单、开销低，这也是Kafka选用此模型的重要因素。Kafka中包含大量的分区，leader副本的均衡保障了整体负载的均衡，所以这一因素也极大地影响Kafka的性能指标。 在采用ISR模型和（f+1）个副本数的配置下，一个Kafka分区能够容忍最大f个节点失败，相比于“少数服从多数”的方式所需的节点数大幅减少。实际上，为了能够容忍f个节点失败，“少数服从多数”的方式和ISR的方式都需要相同数量副本的确认信息才能提交消息。比如，为了容忍1个节点失败，“少数服从多数”需要3个副本和1个follower的确认信息，采用ISR的方式需要2个副本和1个follower的确认信息。在需要相同确认信息数的情况下，采用ISR的方式所需要的副本总数变少，复制带来的集群开销也就更低，“少数服从多数”的优势在于它可以绕开最慢副本的确认信息，降低提交的延迟，而对Kafka而言，这种能力可以交由客户端自己去选择。 总结 关于：怎样可以确保Kafka 完全可靠？如果这样做就可以确保消息不丢失了吗？ 这类问题，就可靠性本身而言，它并不是一个可以用简单的“是”或“否”来衡量的一个指标，而一般是采用几个9来衡量的。任何东西不可能做到完全的可靠，即使能应付单机故障，也难以应付集群、数据中心等集体故障，即使躲得过天灾也未必躲得过人祸。就可靠性而言，我们可以基于一定的假设前提来做分析。本问要讲述的是：在只考虑Kafka本身使用方式的前提下如何最大程度地提高可靠性。 kafka对可靠性的保障体现在多个方面，消息发送阶段、消息存储阶段以及消费消息阶段均有涉及。 消息发送阶段消息发送的3种模式，即发后即忘、同步和异步。对于发后即忘的模式，不管消息有没有被成功写入，生产者都不会收到通知，那么即使消息写入失败也无从得知，因此发后即忘的模式不适合高可靠性要求的场景。如果要提升可靠性，那么生产者可以采用同步或异步的模式，在出现异常情况时可以及时获得通知，以便可以做相应的补救措施，比如选择重试发送（可能会引起消息重复）。 有些发送异常属于可重试异常，比如 NetworkException，这个可能是由瞬时的网络故障而导致的，一般通过重试就可以解决。对于这类异常，如果直接抛给客户端的使用方也未免过于兴师动众，客户端内部本身提供了重试机制来应对这种类型的异常，通过 retries 参数即可配置。默认情况下，retries参数设置为0，即不进行重试，对于高可靠性要求的场景，需要将这个值设置为大于 0 的值，与 retries 参数相关的还有一个retry.backoff.ms参数，它用来设定两次重试之间的时间间隔，以此避免无效的频繁重试。在配置retries和retry.backoff.ms之前，最好先估算一下可能的异常恢复时间，这样可以设定总的重试时间大于这个异常恢复时间，以此来避免生产者过早地放弃重试。如果不知道 retries 参数应该配置为多少，则可以参考 KafkaAdminClient，在 KafkaAdminClient 中retries参数的默认值为5。 如果配置的retries参数值大于0，则可能引起一些负面的影响。由于默认的max.in.flight.requests.per.connection参数值为5，这样可能会影响消息的顺序性，对此要么放弃客户端内部的重试功能，要么将max.in.flight.requests.per.connection参数设置为1，这样也就放弃了吞吐。其次，有些应用对于时延的要求很高，很多时候都是需要快速失败的，设置retries＞0会增加客户端对于异常的反馈时延，如此可能会对应用造成不良的影响。 生产者客户端参数 acks 也是用来支撑可靠性的。该参数有三个可选项：0、1、-1（客户端还可以配置为all，它的含义与-1一样，以下只以-1来进行陈述） 对于acks=1的配置，生产者将消息发送到leader副本，leader副本在成功写入本地日志之后会告知生产者已经成功提交。（如果此时ISR集合的follower副本还没来得及拉取到leader中新写入的消息，leader就宕机了，那么此次发送的消息就会丢失。） 对于ack=-1的配置，生产者将消息发送到leader副本，leader副本在成功写入本地日志之后还要等待 ISR 中的 follower 副本全部同步完成才能够告知生产者已经成功提交，即使此时leader副本宕机，消息也不会丢失 在acks=-1的情形中，它要求ISR中所有的副本都收到相关的消息之后才能够告知生产者已经成功提交。试想一下这样的情形，leader 副本的消息流入速度很快，而follower副本的同步速度很慢，在某个临界点时所有的follower副本都被剔除出了ISR集合，那么ISR中只有一个leader副本，最终acks=-1演变为acks=1的情形，如此也就加大了消息丢失的风险。 Kafka也考虑到了这种情况，并为此提供了min.insync.replicas参数（默认值为1）来作为辅助（配合acks=-1来使用），这个参数指定了ISR集合中最小的副本数，如果不满足条件就会抛出NotEnoughReplicasException或NotEnoughReplicasAfterAppendException。 在正常的配置下，需要满足副本数 ＞ min.insync.replicas参数的值。一个典型的配置方案为：副本数配置为 3，min.insync.replicas 参数值配置为 2。注意min.insync.replicas参数在提升可靠性的时候会从侧面影响可用性。（试想如果ISR中只有一个leader副本，那么最起码还可以使用，而此时如果配置min.insync.replicas＞1，则会使消息无法写入。） 与可靠性和ISR集合有关的还有一个参数—unclean.leader.election.enable。这个参数的默认值为false，如果设置为true就意味着当leader下线时候可以从非ISR集合中选举出新的 leader，这样有可能造成数据的丢失。如果这个参数设置为false，那么也会影响可用性，非ISR集合中的副本虽然没能及时同步所有的消息，但最起码还是存活的可用副本。从0.11.0.0 版本开始，unclean.leader.election.enable 的默认值由原来的 true 改为了false，可以看出Kafka的设计者愈发地偏向于可靠性的提升。 消息存储阶段存储消息阶段需要在消息刷盘之后再给生产者响应，假设消息写入缓存中就返回响应，那么机器突然断电这消息就没了，而生产者以为已经发送成功了。 如果Broker是集群部署，有多副本机制，即消息不仅仅要写入当前Broker,还需要写入副本机中。那配置成至少写入两台机子后再给生产者响应。这样基本上就能保证存储的可靠了。一台挂了还有一台还在呢。 那假如来个地震机房机子都挂了呢？emmmmmm…大公司基本上都有异地多活。 就Kafka而言，越多的副本数越能够保证数据的可靠性，副本数可以在创建主题时配置，也可以在后期修改，不过副本数越多也会引起磁盘、网络带宽的浪费，同时会引起性能的下降。一般而言，设置副本数为3即可满足绝大多数场景对可靠性的要求，而对可靠性要求更高的场景下，可以适当增大这个数值，比如国内部分银行在使用 Kafka 时就会设置副本数为 5。 在broker端还有两个参数log.flush.interval.messages 和 log.flush.interval.ms，用来调整同步刷盘的策略，默认是不做控制而交由操作系统本身来进行处理。同步刷盘是增强一个组件可靠性的有效方式，不过这种方式极其损耗性能，最好还是采用多副本的机制来保障。 消息消费阶段消费者需要真正执行完业务逻辑之后，再发送给Broker消费成功，这才是真正的消费了。 所以只要我们在消息业务逻辑处理完成之后再给Broker响应，那么消费阶段消息就不会丢失。 在kafka中，消息在被追加到 Partition(分区)的时候都会分配一个特定的偏移量（offset）。偏移量（offset)表示 Consumer 当前消费到的 Partition(分区)的所在的位置。Kafka 通过偏移量（offset）可以保证消息在分区内的顺序性。 当消费者拉取到了分区的某个消息之后，消费者会自动提交了 offset（enable.auto.commit 参数的默认值为 true）。虽然这种方式非常简便，但它会带来重复消费和消息丢失的问题，对于高可靠性要求的应用来说显然不可取，所以需要将 enable.auto.commit 参数设置为 false 来执行手动位移提交。在执行手动位移提交的时候也要遵循一个原则：如果消息没有被成功消费，那么就不能提交所对应的消费位移。对于高可靠要求的应用来说，宁愿重复消费也不应该因为消费异常而导致消息丢失。 对于消费端，Kafka 还提供了一个可以兜底的功能，即回溯消费，通过这个功能可以让我们能够有机会对漏掉的消息相应地进行回补，进而可以进一步提高可靠性。","categories":[{"name":"kafka","slug":"kafka","permalink":"https://gyl-coder.top/categories/kafka/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"https://gyl-coder.top/tags/kafka/"},{"name":"可靠性","slug":"可靠性","permalink":"https://gyl-coder.top/tags/%E5%8F%AF%E9%9D%A0%E6%80%A7/"},{"name":"一致性","slug":"一致性","permalink":"https://gyl-coder.top/tags/%E4%B8%80%E8%87%B4%E6%80%A7/"}],"author":{"name":"yanliang","avatar":"https://cdn.jsdelivr.net/gh/gyl-coder/blogImgs/images/touxiang.webp","url":"https://gyl-coder.top"}},{"title":"Kafka Rebalance机制分析","slug":"kafka/kafka-rebalance","date":"2019-12-05T00:00:00.000Z","updated":"2020-12-27T04:47:15.974Z","comments":true,"path":"kafka/kafka-rebalance/","link":"","permalink":"https://gyl-coder.top/kafka/kafka-rebalance/","excerpt":"Rebalance 本质上是一种协议，规定了一个 Consumer Group 下的所有 consumer 如何达成一致，来分配订阅 Topic 的每个分区。 例如：某 Group 下有 20 个 consumer 实例，它订阅了一个具有 100 个 partition 的 Topic 。正常情况下，kafka 会为每个 Consumer 平均的分配 5 个分区。这个分配的过程就是 Rebalance。","text":"Rebalance 本质上是一种协议，规定了一个 Consumer Group 下的所有 consumer 如何达成一致，来分配订阅 Topic 的每个分区。 例如：某 Group 下有 20 个 consumer 实例，它订阅了一个具有 100 个 partition 的 Topic 。正常情况下，kafka 会为每个 Consumer 平均的分配 5 个分区。这个分配的过程就是 Rebalance。 触发 Rebalance 的时机Rebalance 的触发条件有3个。 组成员个数发生变化。例如有新的 consumer 实例加入该消费组或者离开组。 订阅的 Topic 个数发生变化。 订阅 Topic 的分区数发生变化。 Rebalance 发生时，Group 下所有 consumer 实例都会协调在一起共同参与，kafka 能够保证尽量达到最公平的分配。但是 Rebalance 过程对 consumer group 会造成比较严重的影响。在 Rebalance 的过程中 consumer group 下的所有消费者实例都会停止工作，等待 Rebalance 过程完成。 Rebalance 过程分析Rebalance 过程分为两步：Join 和 Sync。 Join 顾名思义就是加入组。这一步中，所有成员都向coordinator发送JoinGroup请求，请求加入消费组。一旦所有成员都发送了JoinGroup请求，coordinator会从中选择一个consumer担任leader的角色，并把组成员信息以及订阅信息发给leader——注意leader和coordinator不是一个概念。leader负责消费分配方案的制定。 Sync，这一步leader开始分配消费方案，即哪个consumer负责消费哪些topic的哪些partition。一旦完成分配，leader会将这个方案封装进SyncGroup请求中发给coordinator，非leader也会发SyncGroup请求，只是内容为空。coordinator接收到分配方案之后会把方案塞进SyncGroup的response中发给各个consumer。这样组内的所有成员就都知道自己应该消费哪些分区了。 Rebalance 场景分析新成员加入组 组成员“崩溃”组成员崩溃和组成员主动离开是两个不同的场景。因为在崩溃时成员并不会主动地告知coordinator此事，coordinator有可能需要一个完整的session.timeout周期(心跳周期)才能检测到这种崩溃，这必然会造成consumer的滞后。可以说离开组是主动地发起rebalance；而崩溃则是被动地发起rebalance。 组成员主动离开组 提交位移 如何避免不必要的rebalance要避免 Rebalance，还是要从 Rebalance 发生的时机入手。我们在前面说过，Rebalance 发生的时机有三个： 组成员数量发生变化 订阅主题数量发生变化 订阅主题的分区数发生变化 后两个我们大可以人为的避免，发生rebalance最常见的原因是消费组成员的变化。 消费者成员正常的添加和停掉导致rebalance，这种情况无法避免，但是时在某些情况下，Consumer 实例会被 Coordinator 错误地认为 “已停止” 从而被“踢出”Group。从而导致rebalance。 当 Consumer Group 完成 Rebalance 之后，每个 Consumer 实例都会定期地向 Coordinator 发送心跳请求，表明它还存活着。如果某个 Consumer 实例不能及时地发送这些心跳请求，Coordinator 就会认为该 Consumer 已经 “死” 了，从而将其从 Group 中移除，然后开启新一轮 Rebalance。这个时间可以通过Consumer 端的参数 session.timeout.ms进行配置。默认值是 10 秒。 除了这个参数，Consumer 还提供了一个控制发送心跳请求频率的参数，就是 heartbeat.interval.ms。这个值设置得越小，Consumer 实例发送心跳请求的频率就越高。频繁地发送心跳请求会额外消耗带宽资源，但好处是能够更加快速地知晓当前是否开启 Rebalance，因为，目前 Coordinator 通知各个 Consumer 实例开启 Rebalance 的方法，就是将 REBALANCE_NEEDED 标志封装进心跳请求的响应体中。 除了以上两个参数，Consumer 端还有一个参数，用于控制 Consumer 实际消费能力对 Rebalance 的影响，即 max.poll.interval.ms 参数。它限定了 Consumer 端应用程序两次调用 poll 方法的最大时间间隔。它的默认值是 5 分钟，表示你的 Consumer 程序如果在 5 分钟之内无法消费完 poll 方法返回的消息，那么 Consumer 会主动发起 “离开组” 的请求，Coordinator 也会开启新一轮 Rebalance。 通过上面的分析，我们可以看一下那些rebalance是可以避免的： 第一类非必要 Rebalance 是因为未能及时发送心跳，导致 Consumer 被 “踢出”Group 而引发的。这种情况下我们可以设置 session.timeout.ms 和 heartbeat.interval.ms 的值，来尽量避免rebalance的出现。（以下的配置是在网上找到的最佳实践，暂时还没测试过） 设置 session.timeout.ms = 6s。 设置 heartbeat.interval.ms = 2s。 要保证 Consumer 实例在被判定为 “dead” 之前，能够发送至少 3 轮的心跳请求，即 session.timeout.ms &gt;= 3 * heartbeat.interval.ms。 将 session.timeout.ms 设置成 6s 主要是为了让 Coordinator 能够更快地定位已经挂掉的 Consumer，早日把它们踢出 Group。 第二类非必要 Rebalance 是 Consumer 消费时间过长导致的。此时，max.poll.interval.ms 参数值的设置显得尤为关键。如果要避免非预期的 Rebalance，你最好将该参数值设置得大一点，比你的下游最大处理时间稍长一点。 总之，要为业务处理逻辑留下充足的时间。这样，Consumer 就不会因为处理这些消息的时间太长而引发 Rebalance 。 相关概念coordinatorGroup Coordinator是一个服务，每个Broker在启动的时候都会启动一个该服务。Group Coordinator的作用是用来存储Group的相关Meta信息，并将对应Partition的Offset信息记录到Kafka内置Topic(__consumer_offsets)中。Kafka在0.9之前是基于Zookeeper来存储Partition的Offset信息(consumers/{group}/offsets/{topic}/{partition})，因为ZK并不适用于频繁的写操作，所以在0.9之后通过内置Topic的方式来记录对应Partition的Offset。 每个Group都会选择一个Coordinator来完成自己组内各Partition的Offset信息，选择的规则如下： 1，计算Group对应在__consumer_offsets上的Partition 2，根据对应的Partition寻找该Partition的leader所对应的Broker，该Broker上的Group Coordinator即就是该Group的Coordinator Partition计算规则： partition-Id(__consumer_offsets) = Math.abs(groupId.hashCode() % groupMetadataTopicPartitionCount) 其中groupMetadataTopicPartitionCount对应offsets.topic.num.partitions参数值，默认值是50个分区。","categories":[{"name":"kafka","slug":"kafka","permalink":"https://gyl-coder.top/categories/kafka/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"https://gyl-coder.top/tags/kafka/"}],"author":{"name":"yanliang","avatar":"https://cdn.jsdelivr.net/gh/gyl-coder/blogImgs/images/touxiang.webp","url":"https://gyl-coder.github.io"}},{"title":"Kafka有哪几处地方有分区分配的概念？简述大致的过程及原理","slug":"kafka/kafka-partition-alocation","date":"2019-12-04T00:00:00.000Z","updated":"2020-12-27T04:47:15.974Z","comments":true,"path":"kafka/kafka-partition-alocation/","link":"","permalink":"https://gyl-coder.top/kafka/kafka-partition-alocation/","excerpt":"在 kafka 中，分区分配是一个很重要的概念，它会影响Kafka整体的性能均衡。kafka 中一共有三处地方涉及此概念，分别是：生产者发送消息、消费者消费消息和创建主题。虽然这三处的对应操作都可以被称之为“分区分配”，但是其实质上所包含的内容却并不相同。","text":"在 kafka 中，分区分配是一个很重要的概念，它会影响Kafka整体的性能均衡。kafka 中一共有三处地方涉及此概念，分别是：生产者发送消息、消费者消费消息和创建主题。虽然这三处的对应操作都可以被称之为“分区分配”，但是其实质上所包含的内容却并不相同。 生产者的分区分配用户在使用 kafka 客户端发送消息时，调用 send 方法发送消息之后，消息就自然而然的发送到了 broker 中。 其实这一过程需要经过拦截器、序列化器、分区器等一系列作用之后才能被真正发往 broker。消息在发往 broker 之前需要确认它需要发送到的分区，如果 ProducerRecord 中指定了 partition 字段，那就不需要分区器的作用，因为 partition 就代表的是所要发往的分区号。如果消息ProducerRecord中没有指定partition字段，那么就需要依赖分区器，根据key这个字段来计算partition的值。分区器的作用就是为消息分配分区。 Kafka中提供的默认分区器是DefaultPartitioner，它实现了Partitioner接口（用户可以实现这个接口来自定义分区器），其中的partition方法就是用来实现具体的分区分配逻辑： public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster); 默认情况下，如果消息的key不为null，那么默认的分区器会对key进行哈希（采用MurmurHash2算法，具备高运算性能及低碰撞率），最终根据得到的哈希值来计算分区号，拥有相同key的消息会被写入同一个分区。如果key为null，那么消息将会以轮询的方式发往 topic 的各个可用分区。 注意：如果key不为null，那么计算得到的分区号会是所有分区中的任意一个；如果key为null并且有可用分区，那么计算得到的分区号仅为可用分区中的任意一个，注意两者之间的差别。 消费者的分区分配在Kafka的默认规则中，每一个分区只能被同一个消费组中的一个消费者消费。消费者的分区分配是指为消费组中的消费者分配所订阅主题中的分区。 如图所示，某个主题中共有4个分区（Partition）：P0、P1、P2、P3。有两个消费组A和B都订阅了这个主题，消费组A中有4个消费者（C0、C1、C2和C3），消费组B中有2个消费者（C4和C5）。按照Kafka默认的规则，最后的分配结果是消费组A中的每一个消费者分配到1个分区，消费组B中的每一个消费者分配到2个分区，两个消费组之间互不影响。每个消费者只能消费所分配到的分区中的消息。 对于消费者的分区分配而言，Kafka自身提供了三种策略，分别为 RangeAssignor、 RoundRobinAssignor 以及 StickyAssignor ，其中 RangeAssignor 为默认的分区分配策略。 RangeAssignorRangeAssignor策略的原理是按照消费者总数和分区总数进行整除运算来获得一个跨度，然后将分区按照跨度进行平均分配，以保证分区尽可能均匀地分配给所有的消费者。对于每一个topic，RangeAssignor策略会将消费组内所有订阅这个topic的消费者按照名称的字典序排序，然后为每个消费者划分固定的分区范围，如果不够平均分配，那么字典序靠前的消费者会被多分配一个分区。 假设n=分区数/消费者数量，m=分区数%消费者数量，那么前m个消费者每个分配n+1个分区，后面的（消费者数量-m）个消费者每个分配n个分区。 为了更加通俗的讲解RangeAssignor策略，我们不妨再举一些示例。假设消费组内有2个消费者C0和C1，都订阅了主题t0和t1，并且每个主题都有4个分区，那么所订阅的所有分区可以标识为：t0p0、t0p1、t0p2、t0p3、t1p0、t1p1、t1p2、t1p3。最终的分配结果为： 消费者C0：t0p0、t0p1、t1p0、t1p1 消费者C1：t0p2、t0p3、t1p2、t1p3 这样分配的很均匀，那么此种分配策略能够一直保持这种良好的特性呢？我们再来看下另外一种情况。假设上面例子中2个主题都只有3个分区，那么所订阅的所有分区可以标识为：t0p0、t0p1、t0p2、t1p0、t1p1、t1p2。最终的分配结果为： 消费者C0：t0p0、t0p1、t1p0、t1p1 消费者C1：t0p2、t1p2 可以明显的看到这样的分配并不均匀，如果将类似的情形扩大，有可能会出现部分消费者过载的情况。对此我们再来看下另一种RoundRobinAssignor策略的分配效果如何。 RoundRobinAssignorRoundRobinAssignor策略的原理是将消费组内所有消费者以及消费者所订阅的所有topic的partition按照字典序排序，然后通过轮询方式逐个将分区以此分配给每个消费者。RoundRobinAssignor策略对应的partition.assignment.strategy参数值为：org.apache.kafka.clients.consumer.RoundRobinAssignor。 如果同一个消费组内所有的消费者的订阅信息都是相同的，那么RoundRobinAssignor策略的分区分配会是均匀的。举例，假设消费组中有2个消费者C0和C1，都订阅了主题t0和t1，并且每个主题都有3个分区，那么所订阅的所有分区可以标识为：t0p0、t0p1、t0p2、t1p0、t1p1、t1p2。最终的分配结果为： 消费者C0：t0p0、t0p2、t1p1 消费者C1：t0p1、t1p0、t1p2 如果同一个消费组内的消费者所订阅的信息是不相同的，那么在执行分区分配的时候就不是完全的轮询分配，有可能会导致分区分配的不均匀。如果某个消费者没有订阅消费组内的某个topic，那么在分配分区的时候此消费者将分配不到这个topic的任何分区。 举例，假设消费组内有3个消费者C0、C1和C2，它们共订阅了3个主题：t0、t1、t2，这3个主题分别有1、2、3个分区，即整个消费组订阅了t0p0、t1p0、t1p1、t2p0、t2p1、t2p2这6个分区。具体而言，消费者C0订阅的是主题t0，消费者C1订阅的是主题t0和t1，消费者C2订阅的是主题t0、t1和t2，那么最终的分配结果为： 消费者C0：t0p0 消费者C1：t1p0 消费者C2：t1p1、t2p0、t2p1、t2p2 可以看到RoundRobinAssignor策略也不是十分完美，这样分配其实并不是最优解，因为完全可以将分区t1p1分配给消费者C1。 StickyAssignor我们再来看一下StickyAssignor策略，“sticky”这个单词可以翻译为“粘性的”，Kafka从0.11.x版本开始引入这种分配策略，它主要有两个目的： 分区的分配要尽可能的均匀； 分区的分配尽可能的与上次分配的保持相同。当两者发生冲突时，第一个目标优先于第二个目标。鉴于这两个目标，StickyAssignor策略的具体实现要比RangeAssignor和RoundRobinAssignor这两种分配策略要复杂很多。我们举例来看一下StickyAssignor策略的实际效果。 假设消费组内有3个消费者：C0、C1和C2，它们都订阅了4个主题：t0、t1、t2、t3，并且每个主题有2个分区，也就是说整个消费组订阅了t0p0、t0p1、t1p0、t1p1、t2p0、t2p1、t3p0、t3p1这8个分区。最终的分配结果如下： 消费者C0：t0p0、t1p1、t3p0 消费者C1：t0p1、t2p0、t3p1 消费者C2：t1p0、t2p1 这样初看上去似乎与采用RoundRobinAssignor策略所分配的结果相同，但事实是否真的如此呢？再假设此时消费者C1脱离了消费组，那么消费组就会执行再平衡操作，进而消费分区会重新分配。如果采用RoundRobinAssignor策略，那么此时的分配结果如下： 消费者C0：t0p0、t1p0、t2p0、t3p0 消费者C2：t0p1、t1p1、t2p1、t3p1 如分配结果所示，RoundRobinAssignor策略会按照消费者C0和C2进行重新轮询分配。而如果此时使用的是StickyAssignor策略，那么分配结果为： 消费者C0：t0p0、t1p1、t3p0、t2p0 消费者C2：t1p0、t2p1、t0p1、t3p1 可以看到分配结果中保留了上一次分配中对于消费者C0和C2的所有分配结果，并将原来消费者C1的“负担”分配给了剩余的两个消费者C0和C2，最终C0和C2的分配还保持了均衡。 如果发生分区重分配，那么对于同一个分区而言有可能之前的消费者和新指派的消费者不是同一个，对于之前消费者进行到一半的处理还要在新指派的消费者中再次复现一遍，这显然很浪费系统资源。StickyAssignor策略如同其名称中的“sticky”一样，让分配策略具备一定的“粘性”，尽可能地让前后两次分配相同，进而减少系统资源的损耗以及其它异常情况的发生。 到目前为止所分析的都是消费者的订阅信息都是相同的情况，我们来看一下订阅信息不同的情况下的处理。 举例，同样消费组内有3个消费者：C0、C1和C2，集群中有3个主题：t0、t1和t2，这3个主题分别有1、2、3个分区，也就是说集群中有t0p0、t1p0、t1p1、t2p0、t2p1、t2p2这6个分区。消费者C0订阅了主题t0，消费者C1订阅了主题t0和t1，消费者C2订阅了主题t0、t1和t2。 如果此时采用RoundRobinAssignor策略，那么最终的分配结果如下所示（和讲述RoundRobinAssignor策略时的一样，这样不妨赘述一下）： 【分配结果集1】 消费者C0：t0p0 消费者C1：t1p0 消费者C2：t1p1、t2p0、t2p1、t2p2 如果此时采用的是StickyAssignor策略，那么最终的分配结果为： 【分配结果集2】 消费者C0：t0p0 消费者C1：t1p0、t1p1 消费者C2：t2p0、t2p1、t2p2 可以看到这是一个最优解（消费者C0没有订阅主题t1和t2，所以不能分配主题t1和t2中的任何分区给它，对于消费者C1也可同理推断）。假如此时消费者C0脱离了消费组，那么RoundRobinAssignor策略的分配结果为： 消费者C1：t0p0、t1p1 消费者C2：t1p0、t2p0、t2p1、t2p2 可以看到RoundRobinAssignor策略保留了消费者C1和C2中原有的3个分区的分配：t2p0、t2p1和t2p2（针对结果集1）。而如果采用的是StickyAssignor策略，那么分配结果为： 消费者C1：t1p0、t1p1、t0p0 消费者C2：t2p0、t2p1、t2p2 可以看到StickyAssignor策略保留了消费者C1和C2中原有的5个分区的分配：t1p0、t1p1、t2p0、t2p1、t2p2。 从结果上看StickyAssignor策略比另外两者分配策略而言显得更加的优异，这个策略的代码实现也是异常复杂。 自定义分区分配策略kafka 处理支持默认提供的三种分区分配算法，还支持用户自定义分区分配算法，自定义的分配策略必须要实现org.apache.kafka.clients.consumer.internals.PartitionAssignor接口。PartitionAssignor接口的定义如下： Subscription subscription(Set&lt;String&gt; topics); String name(); Map&lt;String, Assignment&gt; assign(Cluster metadata, Map&lt;String, Subscription&gt; subscriptions); void onAssignment(Assignment assignment); class Subscription &#123; private final List&lt;String&gt; topics; private final ByteBuffer userData; （省略若干方法……） &#125; class Assignment &#123; private final List&lt;TopicPartition&gt; partitions; private final ByteBuffer userData; （省略若干方法……） &#125; PartitionAssignor接口中定义了两个内部类：Subscription和Assignment。 Subscription类用来表示消费者的订阅信息，类中有两个属性：topics和userData，分别表示消费者所订阅topic列表和用户自定义信息。PartitionAssignor接口通过subscription()方法来设置消费者自身相关的Subscription信息，注意到此方法中只有一个参数topics，与Subscription类中的topics的相互呼应，但是并没有有关userData的参数体现。为了增强用户对分配结果的控制，可以在subscription()方法内部添加一些影响分配的用户自定义信息赋予userData，比如：权重、ip地址、host或者机架（rack）等等。 举例，在subscription()这个方法中提供机架信息，标识此消费者所部署的机架位置，在分区分配时可以根据分区的leader副本所在的机架位置来实施具体的分配，这样可以让消费者与所需拉取消息的broker节点处于同一机架。参考下图，消费者consumer1和broker1都部署在机架rack1上，消费者consumer2和broker2都部署在机架rack2上。如果分区的分配不是机架感知的，那么有可能与图（上部分）中的分配结果一样，consumer1消费broker2中的分区，而consumer2消费broker1中的分区；如果分区的分配是机架感知的，那么就会出现图（下部分）的分配结果，consumer1消费broker1中的分区，而consumer2消费broker2中的分区，这样相比于前一种情形而言，既可以减少消费延迟又可以减少跨机架带宽的占用。 再来说一下Assignment类，它是用来表示分配结果信息的，类中也有两个属性：partitions和userData，分别表示所分配到的分区集合和用户自定义的数据。可以通过PartitionAssignor接口中的onAssignment()方法是在每个消费者收到消费组leader分配结果时的回调函数，例如在StickyAssignor策略中就是通过这个方法保存当前的分配方案，以备在下次消费组再平衡（rebalance）时可以提供分配参考依据。 接口中的name()方法用来提供分配策略的名称，对于Kafka提供的3种分配策略而言，RangeAssignor对应的protocol_name为“range”，RoundRobinAssignor对应的protocol_name为“roundrobin”，StickyAssignor对应的protocol_name为“sticky”，所以自定义的分配策略中要注意命名的时候不要与已存在的分配策略发生冲突。这个命名用来标识分配策略的名称，在后面所描述的加入消费组以及选举消费组leader的时候会有涉及。 真正的分区分配方案的实现是在assign()方法中，方法中的参数metadata表示集群的元数据信息，而subscriptions表示消费组内各个消费者成员的订阅信息，最终方法返回各个消费者的分配信息。 Kafka中还提供了一个抽象类org.apache.kafka.clients.consumer.internals.AbstractPartitionAssignor，它可以简化PartitionAssignor接口的实现，对assign()方法进行了实现，其中会将Subscription中的userData信息去掉后，在进行分配。Kafka提供的3种分配策略都是继承自这个抽象类。如果开发人员在自定义分区分配策略时需要使用userData信息来控制分区分配的结果，那么就不能直接继承AbstractPartitionAssignor这个抽象类，而需要直接实现PartitionAssignor接口。 下面代码参考Kafka中的RangeAssignor策略来自定义一个随机的分配策略，这里笔者称之为RandomAssignor，具体代码实现如下： package org.apache.kafka.clients.consumer; import org.apache.kafka.clients.consumer.internals.AbstractPartitionAssignor; import org.apache.kafka.common.TopicPartition; import java.util.*; public class RandomAssignor extends AbstractPartitionAssignor &#123; @Override public String name() &#123; return &quot;random&quot;; &#125; @Override public Map&lt;String, List&lt;TopicPartition&gt;&gt; assign( Map&lt;String, Integer&gt; partitionsPerTopic, Map&lt;String, Subscription&gt; subscriptions) &#123; Map&lt;String, List&lt;String&gt;&gt; consumersPerTopic = consumersPerTopic(subscriptions); Map&lt;String, List&lt;TopicPartition&gt;&gt; assignment = new HashMap&lt;&gt;(); for (String memberId : subscriptions.keySet()) &#123; assignment.put(memberId, new ArrayList&lt;&gt;()); &#125; // 针对每一个topic进行分区分配 for (Map.Entry&lt;String, List&lt;String&gt;&gt; topicEntry : consumersPerTopic.entrySet()) &#123; String topic = topicEntry.getKey(); List&lt;String&gt; consumersForTopic = topicEntry.getValue(); int consumerSize = consumersForTopic.size(); Integer numPartitionsForTopic = partitionsPerTopic.get(topic); if (numPartitionsForTopic == null) &#123; continue; &#125; // 当前topic下的所有分区 List&lt;TopicPartition&gt; partitions = AbstractPartitionAssignor.partitions(topic, numPartitionsForTopic); // 将每个分区随机分配给一个消费者 for (TopicPartition partition : partitions) &#123; int rand = new Random().nextInt(consumerSize); String randomConsumer = consumersForTopic.get(rand); assignment.get(randomConsumer).add(partition); &#125; &#125; return assignment; &#125; // 获取每个topic所对应的消费者列表，即：[topic, List[consumer]] private Map&lt;String, List&lt;String&gt;&gt; consumersPerTopic( Map&lt;String, Subscription&gt; consumerMetadata) &#123; Map&lt;String, List&lt;String&gt;&gt; res = new HashMap&lt;&gt;(); for (Map.Entry&lt;String, Subscription&gt; subscriptionEntry : consumerMetadata.entrySet()) &#123; String consumerId = subscriptionEntry.getKey(); for (String topic : subscriptionEntry.getValue().topics()) put(res, topic, consumerId); &#125; return res; &#125; &#125; 在使用时，消费者客户端需要添加相应的Properties参数，示例如下： properties.put(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG, RandomAssignor.class.getName()); 分配的实施我们了解了Kafka中消费者的分区分配策略之后是否会有这样的疑问：如果消费者客户端中配置了两个分配策略，那么以哪个为准？如果有多个消费者，彼此所配置的分配策略并不完全相同，那么以哪个为准？多个消费者之间的分区分配是需要协同的，那么这个协同的过程又是怎样？ 在kafka中有一个组协调器（GroupCoordinator）负责来协调消费组内各个消费者的分区分配，对于每一个消费组而言，在kafka服务端都会有其对应的一个组协调器。具体的协调分区分配的过程如下：1.首先各个消费者向GroupCoordinator提案各自的分配策略。如下图所示，各个消费者提案的分配策略和订阅信息都包含在JoinGroupRequest请求中。 2.GroupCoordinator收集各个消费者的提案，然后执行以下两个步骤：一、选举消费组的leader；二、选举消费组的分区分配策略。 选举消费组的分区分配策略比较好理解，为什么这里还要选举消费组的leader，因为最终的分区分配策略的实施需要有一个成员来执行，而这个leader消费者正好扮演了这一个角色。在Kafka中把具体的分区分配策略的具体执行权交给了消费者客户端，这样可以提供更高的灵活性。比如需要变更分配策略，那么只需修改消费者客户端就醒来，而不必要修改并重启Kafka服务端。 怎么选举消费组的leader? 这个分两种情况分析：如果消费组内还没有leader，那么第一个加入消费组的消费者即为消费组的leader；如果某一时刻leader消费者由于某些原因退出了消费组，那么就会重新选举一个新的leader，这个重新选举leader的过程又更为“随意”了，相关代码如下： //scala code. private val members = new mutable.HashMap[String, MemberMetadata] var leaderId = members.keys.head 解释一下这2行代码：在GroupCoordinator中消费者的信息是以HashMap的形式存储的，其中key为消费者的名称，而value是消费者相关的元数据信息。leaderId表示leader消费者的名称，它的取值为HashMap中的第一个键值对的key，这种选举的方式基本上和随机挑选无异。总体上来说，消费组的leader选举过程是很随意的。 怎么选举消费组的分配策略？投票决定。每个消费者都可以设置自己的分区分配策略，对于消费组而言需要从各个消费者所呈报上来的各个分配策略中选举一个彼此都“信服”的策略来进行整体上的分区分配。这个分区分配的选举并非由leader消费者来决定，而是根据消费组内的各个消费者投票来决定。这里所说的“根据组内的各个消费者投票来决定”不是指GroupCoordinator还要与各个消费者进行进一步交互来实施，而是根据各个消费者所呈报的分配策略来实施。最终所选举的分配策略基本上可以看做是被各个消费者所支持的最多的策略，具体的选举过程如下： 收集各个消费者所支持的所有分配策略，组成候选集candidates。每个消费者从候选集candidates中找出第一个自身所支持的策略，为这个策略投上一票。计算候选集中各个策略的选票数，选票数最多的策略即为当前消费组的分配策略。如果某个消费者并不支持所选举出的分配策略，那么就会报错。3.GroupCoordinator发送回执给各个消费者，并交由leader消费者执行具体的分区分配。 如上图所示，JoinGroupResponse回执中包含有GroupCoordinator中投票选举出的分配策略的信息。并且，只有leader消费者的回执中包含各个消费者的订阅信息，因为只需要leader消费者根据订阅信息来执行具体的分配，其余的消费并不需要。 4.leader消费者在整理出具体的分区分配方案后通过SyncGroupRequest请求提交给GroupCoordinator，然后GroupCoordinator为每个消费者挑选出各自的分配结果并通过SyncGroupResponse回执以告知它们。 broker端的分区分配生产者的分区分配是指为每条消息指定其所要发往的分区，消费者中的分区分配是指为消费者指定其可以消费消息的分区，而这里的分区分配是指为集群制定创建主题时的分区副本分配方案，即在哪个broker中创建哪些分区的副本。分区分配是否均衡会影响到Kafka整体的负载均衡，具体还会牵涉到优先副本等概念。 在创建主题时，如果使用了replica-assignment参数，那么就按照指定的方案来进行分区副本的创建；如果没有使用replica-assignment参数，那么就需要按照内部的逻辑来计算分配方案了。使用kafka-topics.sh脚本创建主题时的内部分配逻辑按照机架信息划分成两种策略：未指定机架信息和指定机架信息。如果集群中所有的broker节点都没有配置broker.rack参数，或者使用disable-rack-aware参数来创建主题，那么采用的就是未指定机架信息的分配策略，否则采用的就是指定机架信息的分配策略。","categories":[{"name":"kafka","slug":"kafka","permalink":"https://gyl-coder.top/categories/kafka/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"https://gyl-coder.top/tags/kafka/"}],"author":{"name":"yanliang","avatar":"https://cdn.jsdelivr.net/gh/gyl-coder/blogImgs/images/touxiang.webp","url":"https://gyl-coder.github.io"}},{"title":"kafka的listeners配置错误导致主线程阻塞","slug":"kafka/kafka-listeners-config","date":"2019-12-03T00:00:00.000Z","updated":"2020-12-27T04:47:15.974Z","comments":true,"path":"kafka/kafka-listeners-config/","link":"","permalink":"https://gyl-coder.top/kafka/kafka-listeners-config/","excerpt":"问题背景我们在用kafka的时候，偶尔会遇到这样这样一个问题。 我们写的kafka的客户端程序，在启动的时候，会无缘无故的 卡住（阻塞） 如下图所示：","text":"问题背景我们在用kafka的时候，偶尔会遇到这样这样一个问题。 我们写的kafka的客户端程序，在启动的时候，会无缘无故的 卡住（阻塞） 如下图所示： 这时程序会长时间阻塞在这里，无法继续进行后续操作。 问题排查因为日志没有任何报错信息，但是又可以肯定当前项目并没有完全启动成功。感觉像是程序当中有个地方卡到了。通过 VisualVM 工具dump 线程相关的信息，很快发现了问题所在。原来卡在了consumer初始化的地方。 一下是我这边的处理方式，大家可以参考下。如果有更好的方式欢迎大家相互交流。 以下方法是在初始化Consumer的时候进行处理的： public KafkaConsumerImpl init() &#123; if (group == null || group.isEmpty()) &#123; throw new RuntimeException(&quot;phoenix.mq.group is empty&quot;); &#125; Properties props = new Properties(); props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, namesrvAddr); props.put(ConsumerConfig.GROUP_ID_CONFIG, group); // 是否允许自动提交offset，这里设为false，下面手动提交 props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, &quot;false&quot;); // ... consumer = new KafkaConsumer&lt;&gt;(props); // consumer 订阅的topic及partition topicPartition = new TopicPartition(this.topic, this.partitionId); this.partitions = Collections.singletonList(topicPartition); // 元数据初始化和连接测试，3次失败后抛出异常 Callable&lt;Boolean&gt; call = new Callable&lt;Boolean&gt;() &#123; boolean res = false; int tryTimes = 3; @Override public Boolean call() throws Exception &#123; while (tryTimes-- &gt; 0) &#123; try &#123; consumer.assign(partitions); // 默认初始化offset当前最大值 nextBeginOffset = consumer.position(topicPartition); res = true; break; &#125; catch (Exception e) &#123; if (e instanceof InterruptedException) &#123; break; // 如果position在阻塞状态时，调用了 task.cancel 会抛出此异常。直接退出即可 &#125; LOG.error(e.getMessage(), e); LOG.error(&quot; ==&gt; error when trying to fetch metadata for kafka. topic&lt;&#123;&#125;&gt;, partition&lt;&#123;&#125;&gt;&quot;, topic, partitionId); &#125; // sleep try &#123; Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; return res; &#125; &#125;; FutureTask&lt;Boolean&gt; task = new FutureTask&lt;&gt;(call); new Thread(task).start(); boolean isOk = false; try &#123; isOk = task.get(10000, TimeUnit.MILLISECONDS); &#125; catch (Exception e) &#123; LOG.error(&quot;Get task result timeout&quot;, e); &#125; task.cancel(true); if (isOk) &#123; LOG.info(&quot; ==&gt; init kafka consumer succeed: servers&lt;&#123;&#125;&gt;, topic&lt;&#123;&#125;&gt;, partition&lt;&#123;&#125;&gt;, nextBeginOffset&lt;&#123;&#125;&gt;&quot;, namesrvAddr, topic, partitionId, nextBeginOffset); &#125; else &#123; throw new RuntimeException(String.format( &quot; ==&gt; init kafka consumer failed. please check the conf (listeners or advertised.listeners or ...) and try to ping the host name in the conf value&quot;)); &#125; return this; &#125; 利用 FutureTask 的特性，定义一个定时任务， 在初始化Consumer的时候，尝试去连接kafka，如果配置的kafka的地址有误，或者配置出错在这里可以通过抛出错误体现出来。 最后通过task.get() 方法返回的结果来判断 Consumer 是否成功初始化。","categories":[{"name":"kafka","slug":"kafka","permalink":"https://gyl-coder.top/categories/kafka/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"https://gyl-coder.top/tags/kafka/"},{"name":"线程阻塞","slug":"线程阻塞","permalink":"https://gyl-coder.top/tags/%E7%BA%BF%E7%A8%8B%E9%98%BB%E5%A1%9E/"}],"author":{"name":"yanliang","avatar":"https://cdn.jsdelivr.net/gh/gyl-coder/blogImgs/images/touxiang.webp","url":"https://gyl-coder.github.io"}},{"title":"Kafka中的基本概念","slug":"kafka/kafka-concept","date":"2019-12-01T00:00:00.000Z","updated":"2020-12-27T04:47:15.974Z","comments":true,"path":"kafka/kafka-concept/","link":"","permalink":"https://gyl-coder.top/kafka/kafka-concept/","excerpt":"本文主要介绍一下kafka中的基本概念，主要包括：Producer、Consumer、topic、partition、offset、broker、ISR、AR、HW、LEO等。","text":"本文主要介绍一下kafka中的基本概念，主要包括：Producer、Consumer、topic、partition、offset、broker、ISR、AR、HW、LEO等。 一个典型的 Kafka 体系架构包括若干 Producer、若干 Broker、若干Consumer，以及一个ZooKeeper集群。其中ZooKeeper是Kafka用来负责集群元数据的管理、控制器的选举等操作的。Producer将消息发送到Broker，Broker负责将收到的消息存储到磁盘中，而Consumer负责从Broker订阅并消费消息。 Producer：生产者，也就是发送消息的一方。生产者负责创建消息，然后将其投递到Kafka中。 Consumer：消费者，也就是接收消息的一方。消费者连接到Kafka上并接收消息，进而进行相应的业务逻辑处理。 Broker：服务代理节点。对于Kafka而言，Broker可以简单地看作一个独立的Kafka服务节点或Kafka服务实例。大多数情况下也可以将Broker看作一台Kafka服务器，前提是这台服务器上只部署了一个Kafka实例。一个或多个Broker组成了一个Kafka集群。 消费者（Consumer）负责订阅Kafka中的主题（Topic），并且从订阅的主题上拉取消息。与其他一些消息中间件不同的是：在Kafka的消费理念中还有一层消费组（Consumer Group）的概念，每个消费者都有一个对应的消费组。当消息发布到主题后，只会被投递给订阅它的每个消费组中的一个消费者。 例如，某个主题中共有4个分区（Partition）：P0、P1、P2、P3。有两个消费组A和B都订阅了这个主题，消费组A中有4个消费者（C0、C1、C2和C3），消费组B中有2个消费者（C4和C5）。按照Kafka默认的规则，最后的分配结果是消费组A中的每一个消费者分配到1个分区，消费组B中的每一个消费者分配到2个分区，两个消费组之间互不影响。每个消费者只能消费所分配到的分区中的消息。换言之，每一个分区只能被一个消费组中的一个消费者所消费。 消费者与消费组这种模型可以让整体的消费能力具备横向伸缩性，我们可以增加（或减少）消费者的个数来提高（或降低）整体的消费能力。对于分区数固定的情况，一味地增加消费者并不会让消费能力一直得到提升，如果消费者过多，出现了消费者的个数大于分区个数的情况，就会有消费者分配不到任何分区。 以上分配逻辑都是基于默认的分区分配策略进行分析的，可以通过消费者客户端参数partition.assignment.strategy 来设置消费者与订阅主题之间的分区分配策略。 对于消息中间件而言，一般有两种消息投递模式：点对点（P2P，Point-to-Point）模式和发布/订阅（Pub/Sub）模式。点对点模式是基于队列的，消息生产者发送消息到队列，消息消费者从队列中接收消息。发布订阅模式定义了如何向一个内容节点发布和订阅消息，这个内容节点称为主题（Topic），主题可以认为是消息传递的中介，消息发布者将消息发布到某个主题，而消息订阅者从主题中订阅消息。主题使得消息的订阅者和发布者互相保持独立，不需要进行接触即可保证消息的传递，发布/订阅模式在消息的一对多广播时采用。Kafka 同时支持两种消息投递模式，而这正是得益于消费者与消费组模型的契合： 如果所有的消费者都隶属于同一个消费组，那么所有的消息都会被均衡地投递给每一个消费者，即每条消息只会被一个消费者处理，这就相当于点对点模式的应用。 如果所有的消费者都隶属于不同的消费组，那么所有的消息都会被广播给所有的消费者，即每条消息会被所有的消费者处理，这就相当于发布/订阅模式的应用。 在Kafka中还有两个特别重要的概念 — 主题（Topic） 与 分区（Partition）。 Kafka中的消息以主题为单位进行归类，生产者负责将消息发送到特定的主题（发送到Kafka集群中的每一条消息都要指定一个主题），而消费者负责订阅主题并进行消费。主题是一个逻辑上的概念，它还可以细分为多个分区，一个分区只属于单个主题，很多时候也会把分区称为主题分区（Topic-Partition）。同一主题下的不同分区包含的消息是不同的，分区在存储层面可以看作一个可追加的日志（Log）文件，消息在被追加到分区日志文件的时候都会分配一个特定的偏移量（offset）。offset是消息在分区中的唯一标识，Kafka通过它来保证消息在分区内的顺序性，不过offset并不跨越分区，也就是说，Kafka保证的是分区有序而不是主题有序。Kafka中的分区可以分布在不同的服务器（broker）上，也就是说，一个主题可以横跨多个broker，以此来提供比单个broker更强大的性能。 每一条消息被发送到broker之前，会根据分区规则选择存储到哪个具体的分区。如果分区规则设定得合理，所有的消息都可以均匀地分配到不同的分区中。如果一个主题只对应一个文件，那么这个文件所在的机器 I/O 将会成为这个主题的性能瓶颈，而分区解决了这个问题。在创建主题的时候可以通过指定的参数来设置分区的个数，当然也可以在主题创建完成之后去修改分区的数量，通过增加分区的数量可以实现水平扩展。 Kafka 为分区引入了多副本（Replica）机制，通过增加副本数量可以提升容灾能力。同一分区的不同副本中保存的是相同的消息（在同一时刻，副本之间并非完全一样），副本之间是“一主多从”的关系，其中leader副本负责处理读写请求，follower副本只负责与leader副本的消息同步。副本处于不同的broker中，当leader副本出现故障时，从follower副本中重新选举新的leader副本对外提供服务。Kafka通过多副本机制实现了故障的自动转移，当Kafka集群中某个broker失效时仍然能保证服务可用。 Kafka 消费端也具备一定的容灾能力。Consumer 使用拉（Pull）模式从服务端拉取消息，并且保存消费的具体位置，当消费者宕机后恢复上线时可以根据之前保存的消费位置重新拉取需要的消息进行消费，这样就不会造成消息丢失。 分区中的所有副本统称为AR（Assigned Replicas）。所有与leader副本保持一定程度同步的副本（包括leader副本在内）组成ISR（In-Sync Replicas），ISR集合是AR集合中的一个子集。消息会先发送到leader副本，然后follower副本才能从leader副本中拉取消息进行同步，同步期间内follower副本相对于leader副本而言会有一定程度的滞后。前面所说的“一定程度的同步”是指可忍受的滞后范围，这个范围可以通过参数进行配置。与leader副本同步滞后过多的副本（不包括leader副本）组成OSR（Out-of-Sync Replicas），由此可见，AR=ISR+OSR。在正常情况下，所有的 follower 副本都应该与 leader 副本保持一定程度的同步，即AR=ISR，OSR集合为空。 leader副本负责维护和跟踪ISR集合中所有follower副本的滞后状态，当follower副本落后太多或失效时，leader副本会把它从ISR集合中剔除。如果OSR集合中有follower副本“追上”了leader副本，那么leader副本会把它从OSR集合转移至ISR集合。默认情况下，当leader副本发生故障时，只有在ISR集合中的副本才有资格被选举为新的leader，而在OSR集合中的副本则没有任何机会（不过这个原则也可以通过修改相应的参数配置来改变）。 HW 、 LEO 和上面提到的 ISR有着紧密的关系。 HW （High Watermark）俗称高水位，它标识了一个特定的消息偏移量（offset），消费者只能拉取到这个offset之前的消息。 下图表示一个日志文件，这个日志文件中只有9条消息，第一条消息的offset（LogStartOffset）为0，最有一条消息的offset为8，offset为9的消息使用虚线表示的，代表下一条待写入的消息。日志文件的 HW 为6，表示消费者只能拉取offset在 0 到 5 之间的消息，offset为6的消息对消费者而言是不可见的。 LEO （Log End Offset），标识当前日志文件中下一条待写入的消息的offset。上图中offset为9的位置即为当前日志文件的 LEO，LEO 的大小相当于当前日志分区中最后一条消息的offset值加1.分区 ISR 集合中的每个副本都会维护自身的 LEO ，而 ISR 集合中最小的 LEO 即为分区的 HW，对消费者而言只能消费 HW 之前的消息。 LW是Low Watermark的缩写，俗称“低水位”，代表AR集合中最小的logStartOffset值。副本的拉取请求（FetchRequest，它有可能触发新建日志分段而旧的被清理，进而导致logStartOffset的增加）和删除消息请求（DeleteRecordRequest）都有可能促使LW的增长。 下面具体分析一下 ISR 集合和 HW、LEO的关系。 假设某分区的 ISR 集合中有 3 个副本，即一个 leader 副本和 2 个 follower 副本，此时分区的 LEO 和 HW 都分别为 3 。消息3和消息4从生产者出发之后先被存入leader副本。 在消息被写入leader副本之后，follower副本会发送拉取请求来拉取消息3和消息4进行消息同步。 在同步过程中不同的副本同步的效率不尽相同，在某一时刻follower1完全跟上了leader副本而follower2只同步了消息3，如此leader副本的LEO为5，follower1的LEO为5，follower2的LEO 为4，那么当前分区的HW取最小值4，此时消费者可以消费到offset0至3之间的消息。 当所有副本都成功写入消息3和消息4之后，整个分区的HW和LEO都变为5，因此消费者可以消费到offset为4的消息了。 由此可见kafka的复制机制既不是完全的同步复制，也不是单纯的异步复制。事实上，同步复制要求所有能工作的follower副本都复制完，这条消息才会被确认已成功提交，这种复制方式极大的影响了性能。而在异步复制的方式下，follower副本异步的从leader副本中复制数据，数据只要被leader副本写入就会被认为已经成功提交。在这种情况下，如果follower副本都还没有复制完而落后于leader副本，然后leader副本宕机，则会造成数据丢失。kafka使用这种ISR的方式有效的权衡了数据可靠性和性能之间的关系。","categories":[{"name":"kafka","slug":"kafka","permalink":"https://gyl-coder.top/categories/kafka/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"https://gyl-coder.top/tags/kafka/"}],"author":{"name":"yanliang","avatar":"https://cdn.jsdelivr.net/gh/gyl-coder/blogImgs/images/touxiang.webp","url":"https://gyl-coder.top"}},{"title":"智力题","slug":"iq","date":"2019-11-14T00:00:00.000Z","updated":"2020-12-27T04:47:15.974Z","comments":true,"path":"iq/","link":"","permalink":"https://gyl-coder.top/iq/","excerpt":"","text":"No.1 给一个瞎子52张扑克牌，并告诉他里面恰好有10张牌是正面朝上的。要求这个瞎子把牌分成两堆，使得每堆牌里正面朝上的牌的张数一样多。瞎子应该怎么做？ 答案：把扑克牌分成两堆，一堆10张，一堆42张。然后，把小的那一堆里的所有牌全部翻过来。 No.2 如何用一枚硬币等概率地产生一个1到3之间的随机整数？如果这枚硬币是不公正的呢？ 答案：如果是公正的硬币，则投掷两次，“正反”为1，“反正”为2，“正正”为3，“反反”重来。 如果是不公正的硬币，注意到出现“正反”和“反正”的概率一样，因此令“正反反正”、“反正正反”、“正反正反”分别为1、2、3，其余情况重来。另一种更妙的办法是，投掷三次硬币，“正反反”为1，“反正反”为2，“反反正”为3，其余情况重来。 No.330枚面值不全相同的硬币摆成一排，甲、乙两个人轮流选择这排硬币的其中一端，并取走最外边的那枚硬币。如果你先取硬币，能保证得到的钱不会比对手少吗？ 答案：先取者可以让自己总是取奇数位置上的硬币或者总是取偶数位置上的硬币。数一数是奇数位置上的面值总和多还是偶数位置上的面值总和多，然后总是取这些位置上的硬币就可以了。 No.4 一个环形轨道上有n个加油站，所有加油站的油量总和正好够车跑一圈。证明，总能找到其中一个加油站，使得初始时油箱为空的汽车从这里出发，能够顺利环行一圈回到起点。 答案：总存在一个加油站，仅用它的油就足够跑到下一个加油站（否则所有加油站的油量加起来将不够全程）。把下一个加油站的所有油都提前搬到这个加 油站来，并把油已被搬走的加油站无视掉。在剩下的加油站中继续寻找油量足以到达下个加油站的地方，不断合并加油站，直到只剩一个加油站为止。显然从这里出发就能顺利跑完全程。 另一种证明方法：先让汽车油箱里装好足够多的油，随便从哪个加油站出发试跑一圈。车每到一个加油站时，记录此时油箱里剩下的油量，然后把那个加油站的油全部装上。试跑完一圈后，检查刚才路上到哪个加油站时剩的油量最少，那么空着油箱从那里出发显然一定能跑完全程。 No.5初始时，两个口袋里各有一个球。把后面的n-2个球依次放入口袋，放进哪个口袋其概率与各口袋已有的球数成正比。这样下来，球数较少的那个口袋平均期望有多少个球？ 答案：先考虑一个看似无关的问题——怎样产生一个1到n的随机排列。首先，在纸上写下数字1；然后，把2写在1的左边或者右边；然后，把3写在最 左边，最右边，或者插进1和2之间……总之，把数字i等概率地放进由前面i-1个数产生的（包括最左端和最右端在内的）共i个空位中的一个。这样生成的显 然是一个完全随机的排列。 我们换一个角度来看题目描述的过程：假想用一根绳子把两个球拴在一起，把这根绳子标号为1。接下来，把其中一个小球分裂成两个小球，这两个小球用 标号为2的绳子相连。总之，把“放进第i个球”的操作想象成把其中一个球分裂成两个用标有i-1的绳子相连的小球。联想我们前面的讨论，这些绳子的标号事 实上是一个随机的全排列，也就是说最开始绳子1的位置最后等可能地出现在每个地方。也就是说，它两边的小球个数(1,n-1)、(2,n-2)、 (3,n-3)、……、(n-1,1)这n-1种情况等可能地发生。因此，小袋子里的球数大约为n/4个。准确地说，当n为奇数时，小袋子里的球数为 (n+1)/4；当n为偶数时，小袋子里的球数为n^2/(4n-4)。 No.6考虑一个n*n的棋盘，把有公共边的两个格子叫做相邻的格子。初始时，有些格子里有病毒。每一秒钟后，只要一个格子至少有两个相邻格子染上了病毒，那么他自己也会被感染。为了让所有的格子都被感染，初始时最少需要有几个带病毒的格子？给出一种方案并证明最优性。 答案：至少要n个，比如一条对角线上的n个格子。n个格子也是必需的。当一个新的格子被感染后，全体被感染的格子所组成的图形的周长将减少0个、 2个或4个单位（具体减少了多少要看它周围被感染的格子有多少个）。又因为当所有格子都被感染后，图形的周长为4n，因此初始时至少要有n个被感染的格 子。 No.7在一个m*n的棋盘上，有k个格子里放有棋子。是否总能对所有棋子进行红蓝二染色，使得每行每列的红色棋子和蓝色棋子最多差一个？ 答案：可以。建一个二分图G(X,Y)，其中X有m个顶点代表了棋盘的m个行，Y有n个顶点代表了棋盘的n个列。第i行第j列有棋子就在X(i) 和Y(j)之间连一条边。先找出图G里的所有环（由于是二分图，环的长度一定是偶数），把环里的边红蓝交替染色。剩下的没染色的图一定是一些树。对每棵树 递归地进行操作：去掉一个叶子节点和对应边，把剩下的树进行合法的红蓝二染色，再把刚才去掉的顶点和边加回去，给这个边适当的颜色以满足要求。 No.8任意给一个88的01矩阵，你每次只能选一个33或者4*4的子矩阵并把里面的元素全部取反。是否总有办法把矩阵里的所有数全部变为1？ 答案：不能。大矩阵中有36个33的小矩阵和25个44的小矩阵，因此总共有61种可能的操作。显然，给定一个操作序列，这些操作的先后顺序 是无关紧要的；另外，在一个操作序列中使用两种或两种以上相同的操作也是无用的。因此，实质不同的操作序列只有2^61种。但8*8的01矩阵一共有 2^64种，因此不是每种情况都有办法达到目的。 No.9五个洞排成一排，其中一个洞里藏有一只狐狸。每个夜晚，狐狸都会跳到一个相邻的洞里；每个白天，你都只允许检查其中一个洞。怎样才能保证狐狸最终会被抓住？ 答案：按照2, 3, 4, 2, 3, 4的顺序检查狐狸洞可以保证抓住狐狸。为了说明这个方案是可行的，用集合F表示狐狸可能出现的位置，初始时F = {1, 2, 3, 4, 5}。如果它不在2号洞，则第二天狐狸已经跑到了F = {2, 3, 4, 5}。如果此时它不在3号洞，则第三天狐狸一定跑到了F = {1, 3, 4, 5}。如果此时它不在4号洞，则再过一晚后F = {2, 4}。如果此时它不在2号洞，则再过一天F = {3, 5}。如果此时它不在3号洞，再过一天它就一定跑到4号洞了。 方案不是唯一的，下面这些方案都是可行的： 2, 3, 4, 4, 3, 24, 3, 2, 2, 3, 44, 3, 2, 4, 3, 2 No.10一个经典老题是说，把一个333的立方体切成27个单位立方体，若每一刀切完后都允许重新摆放各个小块的位置，最少可以用几刀？答案仍然是6刀，因为 正中间那个单位立方体的6个面都是后来才切出来的，因此怎么也需要6刀。考虑这个问题：若把一个nnn的立方体切成一个个单位立方体，最少需要几刀？ 答案：事实上，从一个更强的命题出发反而能使问题变得更简单。对于一个abc的长方体，我们需要f(a)+f(b)+f(c)刀，其中 f(x)=⌈log(x)/log(2)⌉。只需要注意到，在整个过程中的任何一步，切完当前最大的块所需要的刀数也就等于整个过程还需要的刀数，因为其 它小块需要的刀数都不会超过最大块所需刀数，它们都可以与最大块一道并行处理。这表明，我们的最优决策即是让当前的最大块尽可能的小，也就是说要把当前的 最大块尽可能相等地切成两半。利用数学归纳法，我们可以很快得到本段开头的结论。","categories":[],"tags":[],"author":{"name":"yanliang","avatar":"https://cdn.jsdelivr.net/gh/gyl-coder/blogImgs/images/touxiang.webp","url":"https://gyl-coder.github.io"}},{"title":"等待通知机制","slug":"concurrency/waiting_for_notification","date":"2019-11-06T00:00:00.000Z","updated":"2020-12-27T04:47:15.974Z","comments":true,"path":"concurrency/waiting_for_notification/","link":"","permalink":"https://gyl-coder.top/concurrency/waiting_for_notification/","excerpt":"多线程环境中，经常出现这样一种场景。一个线程修改了一个对象的值，而另一个线程感知到了变化，然后进行相应的操作，整个过程开始于一个线程，最终结束于另一个线程。这就是传说中的生产者/消费者模型（前者是生产者，后者是消费者）。 而实现这种功能比较简单的方法就是让消费者线程不断的循环变量判断是否符合预期。如下所示 while (!isOK(...))&#123; Thread.sleep(1000); &#125; doIt(); while 循环中设置不满足的条件，如果条件满足则退出循环（睡眠可以防止过快的“无效尝试”） 如果isOK方法的判断逻辑比较简单，操作耗时较短，而且并发冲突量不大的情况下，使用这种方案也未尝不可。但是如果判断逻辑比较复杂，或者并发冲突较大时。这种方案就不行了，while循环会执行很多很多次，CPU消耗较大。 针对这种问题，Java内置的等待通知机制能够较好的解决。当条件不满足时，线程阻塞自己，进入等待状态。当线程条件满足时，通知等待的线程重新执行。线程阻塞避免了循环等待带来的CPU消耗问题。","text":"多线程环境中，经常出现这样一种场景。一个线程修改了一个对象的值，而另一个线程感知到了变化，然后进行相应的操作，整个过程开始于一个线程，最终结束于另一个线程。这就是传说中的生产者/消费者模型（前者是生产者，后者是消费者）。 而实现这种功能比较简单的方法就是让消费者线程不断的循环变量判断是否符合预期。如下所示 while (!isOK(...))&#123; Thread.sleep(1000); &#125; doIt(); while 循环中设置不满足的条件，如果条件满足则退出循环（睡眠可以防止过快的“无效尝试”） 如果isOK方法的判断逻辑比较简单，操作耗时较短，而且并发冲突量不大的情况下，使用这种方案也未尝不可。但是如果判断逻辑比较复杂，或者并发冲突较大时。这种方案就不行了，while循环会执行很多很多次，CPU消耗较大。 针对这种问题，Java内置的等待通知机制能够较好的解决。当条件不满足时，线程阻塞自己，进入等待状态。当线程条件满足时，通知等待的线程重新执行。线程阻塞避免了循环等待带来的CPU消耗问题。 就诊流程这里借用极客时间中大佬文章（文末有链接，名额十个，先到先得）中的一个例子来更好的理解一下等待通知机制。 我们去医院看病，一般都会经历以下流程： 患者先去挂号，然后到就诊门口分诊，等待叫号。 当叫到自己的号时，患者就可以去找大夫了。 就诊过程中，大夫可能会让患者去做一些辅助检查，同时叫下一位患者。 当患者做完检查之后，拿到检测报告重新分诊，等待叫号。 当大夫在此叫到自己的号时，患者再去找大夫就诊。 第一/二步中，患者去挂号分诊类似于线程去过去互斥锁的过程，当患者会叫到号时类似于拿到了锁 大夫让患者去做检查的操作类似于，线程没有满足条件。 患者去做检查类似于线程进入等待状态，然后大夫叫下一个患者，这个步骤我们在前面的等待 - 通知机制中忽略掉了，这个步骤对应到程序里，本质是线程释放持有的互斥锁。 患者做完检查，类似于线程要求的条件已经满足；患者拿检测报告重新分诊，类似于线程需要重新获取互斥锁，这个步骤我们在前面的等待 - 通知机制中也忽视了。 由此可以总结出：一个完整的等待 - 通知机制：线程首先获取互斥锁，当线程要求的条件不满足时，释放互斥锁，进入等待状态；当要求的条件满足时，通知等待的线程，重新获取互斥锁。 用Synchronized实现等待通知机制通过synchronized配合wait(), notifyAll(), notify()方法来实现等待通知机制。 // 调用该方法的线程进入WAITING状态，只有等待另外线程的通知或被中断才返回，调用wait方法会释放锁 wait(); // 通知一个在对象上等待的线程从wait()方法返回，返回的前提是该线程获取到对象锁 notify(); // 通知所有等待在该对象上的线程 notifyAll(); 下面通过一个具体实例来具体实现以下 public class WaitNotify &#123; static boolean flag = true; static Object lock = new Object(); public static void main(String[] args) throws Exception &#123; new Thread(new Wait(), &quot;WaitThread&quot;).start(); Thread.sleep(1000); new Thread(new Notify(), &quot;NotifyThread&quot;).start(); &#125; // 等待 static class Wait implements Runnable &#123; public void run() &#123; // 加锁，拥有lock的Monitor synchronized (lock) &#123; // 当条件不满足时，继续wait，同时释放了lock的锁 while (flag) &#123; try &#123; System.out.println(Thread.currentThread().getName() + &quot; flag is true. wait &quot;); lock.wait(); &#125; catch (InterruptedException e) &#123; &#125; &#125; // 条件满足时，完成工作 System.out.println(Thread.currentThread().getName() + &quot; flag is false. running &quot;); &#125; &#125; &#125; // 通知 static class Notify implements Runnable &#123; public void run() &#123; // 加锁，拥有lock的Monitor synchronized (lock) &#123; // 获取lock的锁，然后进行通知，通知时不会释放lock的锁， // 直到当前线程释放了lock后，WaitThread才能从wait方法中返回 System.out.println(Thread.currentThread().getName() + &quot; hold lock. notify &quot;); lock.notifyAll(); flag = false; try &#123; Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; // 再次加锁 synchronized (lock) &#123; System.out.println(Thread.currentThread().getName() + &quot; hold lock again. sleep &quot;); try &#123; Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; &#125; 在上面的实例中，等待线程(WaitThread)首先获得了对象锁，然后调用对象的wait()方法，从而放弃了锁，并进入对象的等待队列中，进入等待状态。由于等待线程释放了对象锁，通知线程(NotifyThread)随后获取了对象的锁，改变状态使满足条件，并调用对象的notify()方法，通知等待队列（互斥锁的等待队列）中的线程，告诉它条件曾经满足过。（这里notify()只能保证在通知时间点，条件是满足的。而被通知线程的执行时间点和通知时间点基本上不会重合，所以当线程执行的时候，很可能线程条件已经不满足了）等待线程从等待队列中移出，状态变为阻塞状态。通知线程释放锁之后，等待线程再次获取锁从wait()方法返回继续执行。 这个过程需要注意以下几点： 使用wait(), notify(), notifyAll()时需要先对调用对象加锁。 调用wait()方法后，线程状态有RUNNING变为WAITING，并将当前线程放置到对象的等待队列 notify()或notifyAll()方法调用后，等待线程依旧不会从wait()返回，需要调用notify()或 notifAll()的线程释放锁之后，等待线程才有机会从wait()返回。 从wait()方法返回的前提是获得了调用对象的锁。 等待超时模式在开发的过程中经常会遇到这样的情况，调用一个方法时等待一段时间（一般来说是给定一个时间段），如果该方法能够在给定的时间段之内得到结果，那么将结果立刻返回，反之，超时返回默认结果或自定义处理。 而要在等待/通知模型中要加入超市等待逻辑，只需要做一些小小的改动。 假设超时时间段是T，那么可以推断出在当前时间now+T之后就会超时。 定义如下变量： 等待持续时间：REMAINING=T。 超时时间：FUTURE=now+T。 这时仅需要wait(REMAINING)即可，在wait(REMAINING)返回之后会将执行： REMAINING=FUTURE–now。如果REMAINING小于等于0，表示已经超时，直接退出，否则将 继续执行wait(REMAINING)。 相关实现如下 // 对当前对象加锁 public synchronized Object get(long mills) throws InterruptedException &#123; long future = System.currentTimeMillis() + mills; long remaining = mills; // 当超时大于0并且result返回值不满足要求 while ((result == null) &amp;&amp; remaining &gt; 0) &#123; wait(remaining); remaining = future - System.currentTimeMillis(); &#125; return result; &#125; 一个简单的数据库连接池示例我们使用等待超时模式来构造一个简单的数据库连接池，在示例中模拟从连接池中获取、使用和释放连接的过程，而客户端获取连接的过程被设定为等待超时的模式，也就是在1000毫秒内如果无法获取到可用连接，将会返回给客户端一个null。设定连接池的大小为10个，然后通过调节客户端的线程数来模拟无法获取连接的场景。 首先看一下连接池的定义。它通过构造函数初始化连接的最大上限，通过一个双向队列来维护连接，调用方需要先调用fetchConnection(long)方法来指定在多少毫秒内超时获取连接，当连接使用完成后，需要调用releaseConnection(Connection)方法将连接放回线程池。 import java.sql.Connection; import java.util.LinkedList; public class ConnectionPool &#123; private LinkedList&lt;Connection&gt; pool = new LinkedList&lt;Connection&gt;(); public ConnectionPool(int initialSize) &#123; if (initialSize &gt; 0) &#123; for (int i = 0; i &lt; initialSize; i++) &#123; pool.addLast(ConnectionDriver.createConnection()); &#125; &#125; &#125; public void releaseConnection(Connection connection) &#123; if (connection != null) &#123; synchronized (pool) &#123; // 连接释放后需要进行通知，这样其他消费者能够感知到连接池中已经归还了一个连接 pool.addLast(connection); pool.notifyAll(); &#125; &#125; &#125; // 在mills内无法获取到连接，将会返回null public Connection fetchConnection(long mills) throws InterruptedException &#123; synchronized (pool) &#123; // 完全超时 if (mills &lt;= 0) &#123; while (pool.isEmpty()) &#123; pool.wait(); &#125; return pool.removeFirst(); &#125; else &#123; // 超过设置的时间mills，将推出while循环 long future = System.currentTimeMillis() + mills; long remaining = mills; while (pool.isEmpty() &amp;&amp; remaining &gt; 0) &#123; pool.wait(remaining); remaining = future - System.currentTimeMillis(); &#125; Connection result = null; if (!pool.isEmpty()) &#123; result = pool.removeFirst(); &#125; return result; &#125; &#125; &#125; &#125; 由于java.sql.Connection是一个接口，最终的实现是由数据库驱动提供方来实现的，考虑到只是个示例，我们通过动态代理构造了一个Connection，该Connection的代理实现仅仅是在commit()方法调用时休眠100毫秒. import java.lang.reflect.InvocationHandler; import java.lang.reflect.Method; import java.lang.reflect.Proxy; import java.sql.Connection; import java.util.concurrent.TimeUnit; public class ConnectionDriver &#123; static class ConnectionHandler implements InvocationHandler &#123; // commit时会回调 public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; if (method.getName().equals(&quot;commit&quot;)) &#123; TimeUnit.MILLISECONDS.sleep(100); &#125; return null; &#125; &#125; // 创建一个Connection的代理，在commit时休眠100毫秒 public static final Connection createConnection() &#123; return (Connection) Proxy.newProxyInstance(ConnectionDriver.class.getClassLoader(), new Class[] &#123; Connection.class &#125;, new ConnectionHandler()); &#125; &#125; 下面通过一个示例来测试简易数据库连接池的工作情况，模拟客户端ConnectionRunner获取、使用、最后释放连接的过程，当它使用时连接将会增加获取到连接的数量，反之，将会增加未获取到连接的数量。 import java.sql.Connection; import java.util.concurrent.CountDownLatch; import java.util.concurrent.atomic.AtomicInteger; public class Main &#123; static ConnectionPool pool = new ConnectionPool(10); // 保证所有CountDownLatch能够同时开始 static CountDownLatch start = new CountDownLatch(1); // 保证main线程等待所有ConnectionRunner结束之后才继续执行 static CountDownLatch end; public static void main(String[] args) throws InterruptedException &#123; // 线程数量，可以修改线程数量进行观察 int threadCount = 10; end = new CountDownLatch(threadCount); int count = 20; // 成功的个数 AtomicInteger got = new AtomicInteger(); // 失败的个数 AtomicInteger notGot = new AtomicInteger(); for (int i = 0; i &lt; threadCount; i++) &#123; Thread thread = new Thread(new ConnetionRunner(count, got, notGot), &quot;ConnectionRunnerThread&quot;); thread.start(); &#125; start.countDown(); end.await(); System.out.println(&quot;total invoke: &quot; + (threadCount * count)); System.out.println(&quot;got connection: &quot; + got); System.out.println(&quot;not got connection &quot; + notGot); &#125; static class ConnetionRunner implements Runnable &#123; int count; AtomicInteger got; AtomicInteger notGot; public ConnetionRunner(int count, AtomicInteger got, AtomicInteger notGot) &#123; this.count = count; this.got = got; this.notGot = notGot; &#125; @Override public void run() &#123; try &#123; start.await(); &#125; catch (Exception ex) &#123; &#125; while (count &gt; 0) &#123; try &#123; // 从线程池中获取连接，如果1000ms内无法获取到，将会返回null // 分别统计连接获取的数量got和未获取到的数量notGot Connection connection = pool.fetchConnection(1000); if (connection != null) &#123; try &#123; connection.createStatement(); connection.commit(); &#125; finally &#123; pool.releaseConnection(connection); // 成功个数+1 got.incrementAndGet(); &#125; &#125; else &#123; // 失败个数+1 notGot.incrementAndGet(); &#125; &#125; catch (Exception ex) &#123; &#125; finally &#123; count--; &#125; &#125; end.countDown(); &#125; &#125; &#125; 上述示例中使用了CountDownLatch来确保ConnectionRunnerThread能够同时开始执行，并且在全部结束之后，才使main线程从等待状态中返回。 当前设定的场景是10个线程同时运行获取连接池（10个连接）中的连接，可以通过调节线程数量来观察未获取到连接的情况。线程数、总获取次数、获取到的数量、未获取到的数量以及未获取到的比率。","categories":[{"name":"并发","slug":"并发","permalink":"https://gyl-coder.top/categories/%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://gyl-coder.top/tags/Java/"},{"name":"并发","slug":"并发","permalink":"https://gyl-coder.top/tags/%E5%B9%B6%E5%8F%91/"},{"name":"等待通知机制","slug":"等待通知机制","permalink":"https://gyl-coder.top/tags/%E7%AD%89%E5%BE%85%E9%80%9A%E7%9F%A5%E6%9C%BA%E5%88%B6/"}],"author":{"name":"yanliang","avatar":"https://cdn.jsdelivr.net/gh/gyl-coder/blogImgs/images/touxiang.webp","url":"https://gyl-coder.top"}},{"title":"多个线程交替打印内容","slug":"concurrency/alternate_printing","date":"2019-11-05T00:00:00.000Z","updated":"2020-12-27T04:47:15.974Z","comments":true,"path":"concurrency/alternate_printing/","link":"","permalink":"https://gyl-coder.top/concurrency/alternate_printing/","excerpt":"需求： 启动两个线程，交替打印奇偶数。 效果如下所示 偶数线程：0 奇数线程：1 偶数线程：2 ...","text":"需求： 启动两个线程，交替打印奇偶数。 效果如下所示 偶数线程：0 奇数线程：1 偶数线程：2 ... 两个线程交替打印无锁实现不需要进行任何加锁，利用并发包中的AtomicInteger和volidate修饰符组合进行实现。 public class Thread_demo1 &#123; private static volatile Boolean flag = true; private static AtomicInteger num = new AtomicInteger(); public static CountDownLatch latch = new CountDownLatch(2); public static void main(String[] args) throws InterruptedException &#123; long start = System.currentTimeMillis(); Thread thread1 = new Thread(new Runnable() &#123; @Override public void run() &#123; while(num.get() &lt;= 10000)&#123; if(!flag)&#123; System.out.println(Thread.currentThread().getName()+&quot;: &quot; + num.getAndIncrement()); flag = true; &#125; &#125; latch.countDown(); &#125; &#125;, &quot;奇数线程&quot;); Thread thread2 = new Thread(new Runnable() &#123; @Override public void run() &#123; while(num.get() &lt;= 10000)&#123; if(flag)&#123; System.out.println(Thread.currentThread().getName()+ &quot;：&quot; + num.getAndIncrement()); flag = false; &#125; &#125; latch.countDown(); &#125; &#125;, &quot;偶数线程&quot;); thread1.start(); thread2.start(); latch.await(); System.out.println(&quot;共耗时：&quot;+(System.currentTimeMillis() - start) + &quot;ms&quot;); &#125; &#125; 加锁实现A通过一个boolean类型的变量来限制两个线程，分别只输出奇数和偶数。 public class Thread_demo2 &#123; private int count = 0; private final Object lock = new Object(); public static CountDownLatch latch = new CountDownLatch(2); public void go() throws InterruptedException &#123; long start = System.currentTimeMillis(); Thread thread1 = new Thread(() -&gt; &#123; while (count &lt; 10000) &#123; synchronized (lock) &#123; if ((count &amp; 1) == 0) &#123; System.out.println(Thread.currentThread().getName() + &quot;: &quot; + count ++); &#125; &#125; &#125; latch.countDown(); &#125;, &quot;偶数线程&quot;); Thread thread2 = new Thread(() -&gt; &#123; while (count &lt; 10000) &#123; synchronized (lock) &#123; if ((count &amp; 1) == 1) &#123; System.out.println(Thread.currentThread().getName() + &quot;: &quot; + count ++); &#125; &#125; &#125; latch.countDown(); &#125;, &quot;奇数线程&quot;); thread1.start(); thread2.start(); latch.await(); System.out.println(&quot;共耗时：&quot;+(System.currentTimeMillis() - start) + &quot;ms&quot;); &#125; public static void main(String[] args) throws InterruptedException &#123; Thread_demo2 threaddemo2 = new Thread_demo2(); threaddemo2.go(); &#125; &#125; 加锁实现B通过同一个对象锁来实现。 public class Thread_demo3 &#123; private int count = 0; private final Object lock = new Object(); public static CountDownLatch latch = new CountDownLatch(2); public void go() throws InterruptedException &#123; long begin = System.currentTimeMillis(); new Thread(new RunnerTest(), &quot;偶数线程&quot;).start(); // 确保偶数线程线先获取到锁 Thread.sleep(1); new Thread(new RunnerTest(), &quot;奇数线程&quot;).start(); latch.await(); System.out.println(System.currentTimeMillis() - begin); &#125; class RunnerTest implements Runnable &#123; @Override public void run() &#123; while (count &lt; 10000) &#123; synchronized (lock) &#123; // 拿到锁就打印 System.out.println(Thread.currentThread().getName() + &quot;: &quot; + count ++); // 唤醒其他线程 lock.notifyAll(); try &#123; if (count &lt; 10000) &#123; // 如果任务还没有结束，则让出当前的锁并休眠 lock.wait(); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; latch.countDown(); &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; Thread_demo3 threaddemo3 = new Thread_demo3(); threaddemo3.go(); &#125; &#125; 多个线程交替打印需求： n个线程，交替打印数字。 效果如下所示 线程1：0 线程2：1 线程3：2 线程1：3 线程2：4 线程3：5 线程1：6 ... public class Demo implements Runnable &#123; private static final Object LOCK = new Object(); /** * 当前即将打印的数字 */ private static int current = 0; /** * 当前线程编号，从0开始 */ private int threadNo; /** * 线程数量 */ private int threadCount; /** * 打印的最大数值 */ private int maxInt; public Demo(int threadNo, int threadCount, int maxInt) &#123; this.threadNo = threadNo; this.threadCount = threadCount; this.maxInt = maxInt; &#125; @Override public void run() &#123; while (true) &#123; synchronized (LOCK) &#123; // 判断是否轮到当前线程执行 while (current % threadCount != threadNo) &#123; if (current &gt; maxInt) &#123; break; &#125; try &#123; // 如果不是，则当前线程进入wait LOCK.wait(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; // 最大值跳出循环 if (current &gt; maxInt) &#123; break; &#125; System.out.println(&quot;thread&quot; + threadNo + &quot; : &quot; + current); current++; // 唤醒其他wait线程 LOCK.notifyAll(); &#125; &#125; &#125; public static void main(String[] args) &#123; int threadCount = 3; int max = 100; for (int i = 0; i &lt; threadCount; i++) &#123; new Thread(new Demo(i, threadCount, max)).start(); &#125; &#125; &#125; 可以看到，核心思想都是差不多的。都用的是等待通知机制。 这里我们需要注意一个问题。当threadCount（线程数量）比较大的时候，在执行notifyAll唤醒其他线程的时候，可能会出现线程抢到了锁但并不该自己执行，然后又进入wait的情况。比如现在有100个线程，现在是第一个线程在执行，他执行完之后需要第二个线程执行，但是第100个线程抢到了，发现不是自己然后又进入wait，然后第99个线程抢到了，发现不是自己然后又进入wait，然后第98,97…直到第3个线程都抢到了，最后才到第二个线程抢到同步锁，这里就会白白的多执行很多过程，虽然最后能完成目标。 解决办法多种多样，这里介绍一种基于信号量的实现方式。 public class Main &#123; static int result = 0; public static void main(String[] args) throws InterruptedException &#123; int N = 3; Thread[] threads = new Thread[N]; final Semaphore[] syncObjects = new Semaphore[N]; for (int i = 0; i &lt; N; i++) &#123; syncObjects[i] = new Semaphore(1); if (i != N-1)&#123; syncObjects[i].acquire(); &#125; &#125; for (int i = 0; i &lt; N; i++) &#123; final Semaphore lastSemphore = i == 0 ? syncObjects[N - 1] : syncObjects[i - 1]; final Semaphore curSemphore = syncObjects[i]; final int index = i; threads[i] = new Thread(new Runnable() &#123; public void run() &#123; try &#123; while (true) &#123; // 第一次执行时，由于最后一个信号量并没有执行acquire，所以这里不会阻塞 lastSemphore.acquire(); System.out.println(&quot;thread&quot; + index + &quot;: &quot; + result++); if (result &gt; 100)&#123; System.exit(0); &#125; // 释放下一个要执行的线程的 curSemphore.release(); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;); threads[i].start(); &#125; &#125; &#125;","categories":[{"name":"并发","slug":"并发","permalink":"https://gyl-coder.top/categories/%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://gyl-coder.top/tags/Java/"},{"name":"并发","slug":"并发","permalink":"https://gyl-coder.top/tags/%E5%B9%B6%E5%8F%91/"}],"author":{"name":"yanliang","avatar":"https://cdn.jsdelivr.net/gh/gyl-coder/blogImgs/images/touxiang.webp","url":"https://gyl-coder.top"}},{"title":"Lock & Synchronized","slug":"concurrency/lock_and_synchronized","date":"2019-11-05T00:00:00.000Z","updated":"2020-12-27T04:47:15.974Z","comments":true,"path":"concurrency/lock_and_synchronized/","link":"","permalink":"https://gyl-coder.top/concurrency/lock_and_synchronized/","excerpt":"首先我们来考虑一个问题？ 在Lock出现之前我们一直使用synchronized来实现同步访问。那为什么还要提供lock呢？ 我们知道如果一个代码块被synchronized修饰了，当有一个线程获取了对应的锁，并执行该代码快时，其他线程就只能在一旁等待。 而获取锁的线程只有在线程正常执行完该代码或者线程执行过程中发生异常才会释放对锁的占有。 在synchronized这种机制下，程序就有可能出现如下问题： 获取锁的线程由于要等待IO或其他原因被阻塞住了，但是又没有释放锁，其他线程只能等待。影响效率 相互没有冲突的多个线程不能并发执行 解决死锁的一个方案是破坏不可抢占条件synchronized 没有办法解决。原因是 synchronized 申请资源的时候，如果申请不到，线程直接进入阻塞状态了，而线程进入阻塞状态，啥都干不了，也释放不了线程已经占有的资源。 正是由于这些因素的限制，需要开发出一种满足如下条件的同步机制。 能够响应中断。synchronized 的问题是，持有锁 A 后，如果尝试获取锁 B 失败，那么线程就进入阻塞状态，一旦发生死锁，就没有任何机会来唤醒阻塞的线程。但如果阻塞状态的线程能够响应中断信号，也就是说当我们给阻塞的线程发送中断信号的时候，能够唤醒它，那它就有机会释放曾经持有的锁 A。 支持超时。如果线程在一段时间之内没有获取到锁，不是进入阻塞状态，而是返回一个错误，那这个线程也有机会释放曾经持有的锁。 非阻塞地获取锁。如果尝试获取锁失败，并不进入阻塞状态，而是直接返回，那这个线程也有机会释放曾经持有的锁。","text":"首先我们来考虑一个问题？ 在Lock出现之前我们一直使用synchronized来实现同步访问。那为什么还要提供lock呢？ 我们知道如果一个代码块被synchronized修饰了，当有一个线程获取了对应的锁，并执行该代码快时，其他线程就只能在一旁等待。 而获取锁的线程只有在线程正常执行完该代码或者线程执行过程中发生异常才会释放对锁的占有。 在synchronized这种机制下，程序就有可能出现如下问题： 获取锁的线程由于要等待IO或其他原因被阻塞住了，但是又没有释放锁，其他线程只能等待。影响效率 相互没有冲突的多个线程不能并发执行 解决死锁的一个方案是破坏不可抢占条件synchronized 没有办法解决。原因是 synchronized 申请资源的时候，如果申请不到，线程直接进入阻塞状态了，而线程进入阻塞状态，啥都干不了，也释放不了线程已经占有的资源。 正是由于这些因素的限制，需要开发出一种满足如下条件的同步机制。 能够响应中断。synchronized 的问题是，持有锁 A 后，如果尝试获取锁 B 失败，那么线程就进入阻塞状态，一旦发生死锁，就没有任何机会来唤醒阻塞的线程。但如果阻塞状态的线程能够响应中断信号，也就是说当我们给阻塞的线程发送中断信号的时候，能够唤醒它，那它就有机会释放曾经持有的锁 A。 支持超时。如果线程在一段时间之内没有获取到锁，不是进入阻塞状态，而是返回一个错误，那这个线程也有机会释放曾经持有的锁。 非阻塞地获取锁。如果尝试获取锁失败，并不进入阻塞状态，而是直接返回，那这个线程也有机会释放曾经持有的锁。 Lock下面我们看下lock接口的API // 支持中断的 API void lockInterruptibly() throws InterruptedException; // 支持超时的 API boolean tryLock(long time, TimeUnit unit) throws InterruptedException; // 支持非阻塞获取锁的 API boolean tryLock(); void lock(); void unlock(); Condition newCondition(); Java SDK 里面 Lock 的使用，有一个经典的范例，就是try{}finally{}，需要重点关注的是在 finally 里面释放锁。下面我们具体看下上面那几个方法的使用。 **lcok()**方法是平时比较常用的一个方法，用来获取锁。如果锁被其他线程获取，则进行等待。 由于在前面讲到如果采用Lock，必须主动去释放锁，并且在发生异常时，不会自动释放锁。因此一般来说，使用Lock必须在try{}catch{}块中进行，并且将释放锁的操作放在finally块中进行，以保证锁一定被被释放，防止死锁的发生。通常使用Lock来进行同步的话，是以下面这种形式去使用的： Lock lock = ...; lock.lock(); try&#123; //处理任务 &#125;catch(Exception e)&#123; &#125;finally&#123; lock.unlock(); //释放锁 &#125; tryLock()方法是有返回值的，它表示用来尝试获取锁，如果获取成功，则返回true，如果获取失败（即锁已被其他线程获取），则返回false，也就说这个方法无论如何都会立即返回。在拿不到锁时不会一直在那等待。 tryLock(long time, TimeUnit unit)方法和tryLock()方法是类似的，只不过区别在于这个方法在拿不到锁时会等待一定的时间，在时间期限之内如果还拿不到锁，就返回false。如果如果一开始拿到锁或者在等待期间内拿到了锁，则返回true。 所以，一般情况下通过tryLock来获取锁时是这样使用的： Lock lock = ...; if(lock.tryLock()) &#123; try&#123; //处理任务 &#125;catch(Exception ex)&#123; &#125;finally&#123; lock.unlock(); //释放锁 &#125; &#125;else &#123; //如果不能获取锁，则直接做其他事情 &#125; lockInterruptibly()方法比较特殊，当通过这个方法去获取锁时，如果线程正在等待获取锁，则这个线程能够响应中断，即中断线程的等待状态。也就使说，当两个线程同时通过lock.lockInterruptibly()想获取某个锁时，假若此时线程A获取到了锁，而线程B只有在等待，那么对线程B调用threadB.interrupt()方法能够中断线程B的等待过程。 由于lockInterruptibly()的声明中抛出了异常，所以lock.lockInterruptibly()必须放在try块中或者在调用lockInterruptibly()的方法外声明抛出InterruptedException。 因此lockInterruptibly()一般的使用形式如下： public void method() throws InterruptedException &#123; lock.lockInterruptibly(); try &#123; //..... &#125; finally &#123; lock.unlock(); &#125; &#125; 注意，当一个线程获取了锁之后，是不会被interrupt()方法中断的。因为本身在前面的文章中讲过单独调用interrupt()方法不能中断正在运行过程中的线程，只能中断阻塞过程中的线程。 因此当通过lockInterruptibly()方法获取某个锁时，如果不能获取到，只有进行等待的情况下，是可以响应中断的。 而用synchronized修饰的话，当一个线程处于等待某个锁的状态，是无法被中断的，只有一直等待下去。 Lock如果保证可见性synchronized 之所以能够保证可见性，也是因为有一条synchronized 相关的规则：synchronized 的解锁 Happens-Before 于后续对这个锁的加锁。那 Lock 靠什么保证可见性呢？例如在下面的代码中，线程 T1 对 value 进行了 +=1 操作，那后续的线程 T2 能够看到 value 的正确结果吗？ class X &#123; private final Lock rtl = new ReentrantLock(); int value; public void addOne() &#123; // 获取锁 rtl.lock(); try &#123; value+=1; &#125; finally &#123; // 保证锁能释放 rtl.unlock(); &#125; &#125; &#125; Lock利用volatile相关的Happens-Before规则来保证可见性。 Java SDK 里面的 ReentrantLock，内部持有一个volatile的成员变量state，获取锁的时候，会读写state的值；解锁的时候也会读写state的值。也就是说，在执行 value+=1 之前，程序先读写了一次 volatile变量state，在执行value+=1之后，又读写一次olatile变量state。相关的Happens-Before规则如下： 顺序性规则：对于线程 T1，value+=1 Happens-Before释放锁的操作unlock（）； volatile 变量规则：由于 state = 1 会先读取 state，所以线程 T1 的 unlock() 操作 Happens-Before 线程 T2 的 lock() 操作； 传递性规则：线程 T1 的 value+=1 Happen-Before线程T2的lock（）操作。 class SampleLock &#123; volatile int state; // 加锁 lock() &#123; // 省略代码无数 state = 1; &#125; // 解锁 unlock() &#123; // 省略代码无数 state = 0; &#125; &#125; 总结来说，Lock和synchronized有以下几点不同： 1）Lock是一个接口，而synchronized是Java中的关键字，synchronized是内置的语言实现； 2）synchronized在发生异常时，会自动释放线程占有的锁，因此不会导致死锁现象发生；而Lock在发生异常时，如果没有主动通过unLock()去释放锁，则很可能造成死锁现象，因此使用Lock时需要在finally块中释放锁； 3）Lock可以让等待锁的线程响应中断，而synchronized却不行，使用synchronized时，等待的线程会一直等待下去，不能够响应中断； 4）通过Lock可以知道有没有成功获取锁，而synchronized却无法办到。 5）Lock可以提高多个线程进行读操作的效率。 在性能上来说，如果竞争资源不激烈，两者的性能是差不多的，而当竞争资源非常激烈时（即有大量线程同时竞争），此时Lock的性能要远远优于synchronized。所以说，在具体使用时要根据适当情况选择。 Happens-Before相关规则介绍可看下王宝令大神的Java并发课程","categories":[{"name":"并发","slug":"并发","permalink":"https://gyl-coder.top/categories/%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://gyl-coder.top/tags/Java/"},{"name":"并发","slug":"并发","permalink":"https://gyl-coder.top/tags/%E5%B9%B6%E5%8F%91/"}],"author":{"name":"yanliang","avatar":"https://cdn.jsdelivr.net/gh/gyl-coder/blogImgs/images/touxiang.webp","url":"https://gyl-coder.top"}},{"title":"Why Concurrency ?","slug":"concurrency/why_concurrency","date":"2019-11-05T00:00:00.000Z","updated":"2020-12-27T04:47:15.974Z","comments":true,"path":"concurrency/why_concurrency/","link":"","permalink":"https://gyl-coder.top/concurrency/why_concurrency/","excerpt":"一门技术的出现必然有其出现的道理，后来需要了解它出现的时代环境和因素，扩充自己的视野，发掘技术发展的经过。分析其优缺点，以便更好的运用。 一直以来，硬件的发展极其迅速，也有一个很著名的 摩尔定律 ,然而事实证明摩尔定律的有效性超过半个世纪就失效了。为了进一步提升计算速度，放弃了一味追求单独的计算单元，将多个计算单元整合到一起，也就是形成了多核CPU。在多核CPU的环境下，并发编程的形式可以将多核CPU的计算能力发挥到极致，性能得到提升。","text":"一门技术的出现必然有其出现的道理，后来需要了解它出现的时代环境和因素，扩充自己的视野，发掘技术发展的经过。分析其优缺点，以便更好的运用。 一直以来，硬件的发展极其迅速，也有一个很著名的 摩尔定律 ,然而事实证明摩尔定律的有效性超过半个世纪就失效了。为了进一步提升计算速度，放弃了一味追求单独的计算单元，将多个计算单元整合到一起，也就是形成了多核CPU。在多核CPU的环境下，并发编程的形式可以将多核CPU的计算能力发挥到极致，性能得到提升。 并发带来的好处性能上的提升提升多核CPU的利用率：一般来说一台主机上的会有多个CPU核心，我们可以创建多个线程，操作系统可以将多个线程分配给不同的CPU去执行，每个CPU执行一个线程，这样就提高了CPU的使用效率，如果使用单线程就只能有一个CPU核心被使用。 提升访问I/O时CPU的利用率：当一个线程要在网上下载一些东西的时候，这个线程将处于阻塞状态，这时CPU就不会再为这个线程分配CPU时间了，而其他进程可以不受任何影响地获得CPU时间。反过来如果没有使用并发，当前面的指令申请I/O资源的时候，整个进程就被挂起了，即使CPU处于空闲状态后面的指令也不能被执行。 降低系统的响应时间：当有多个请求需要被处理时。在单线程的环境中，所有请求需要排队进行处理。这样处在后面的请求将会等待较长的时间。多线程处理的话可以回避响应时间长的问题，所有请求轮流使用CPU资源，所有请求都可以较快的得到响应。 提升系统容错能力一个线程可以不受其他线程的干扰独立运行，如果某个线程的代码里出现了Bug，这个线程可能抛出异常退出了，这时候其他线程可以不受任何影响继续执行，不至于导致整个系统都崩溃。 并发的弊端频繁的上下文切换时间片是CPU分配给各个线程的时间，因为时间非常短，所以CPU不断通过切换线程，让我们觉得多个线程是同时执行的，时间片一般是几十毫秒。而每次切换时，需要保存当前的状态起来，以便能够进行恢复先前状态，而这个切换时非常损耗性能，过于频繁反而无法发挥出多线程编程的优势。 通常减少上下文切换可以采用无锁并发编程，CAS算法，使用最少的线程和使用协程。 无锁并发编程：可以参照concurrentHashMap锁分段的思想，不同的线程处理不同段的数据，这样在多线程竞争的条件下，可以减少上下文切换的时间。 CAS算法，利用Atomic下使用CAS算法来更新数据，使用了乐观锁，可以有效的减少一部分不必要的锁竞争带来的上下文切换 使用最少线程：避免创建不需要的线程，比如任务很少，但是创建了很多的线程，这样会造成大量的线程都处于等待状态 协程：在单线程里实现多任务的调度，并在单线程里维持多个任务间的切换 并发问题多易出错编写并发代码容易出错，多线程并发运行给执行过程带来了很多不确定性，因为只有同一个线程内部代码的执行顺序是固定的，而不同线程之间的代码执行顺序无法确定。当多个线程之间互相干扰时，问题就会接踵而至。编写多线程代码时，如果没有考虑全面很容易产生概率性的、难以复现的Bug 是否采用并发编程的模式，最终还是要结合自己的场景权衡利弊。","categories":[{"name":"并发","slug":"并发","permalink":"https://gyl-coder.top/categories/%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://gyl-coder.top/tags/Java/"},{"name":"并发","slug":"并发","permalink":"https://gyl-coder.top/tags/%E5%B9%B6%E5%8F%91/"},{"name":"Concurrency","slug":"Concurrency","permalink":"https://gyl-coder.top/tags/Concurrency/"}],"author":{"name":"yanliang","avatar":"https://cdn.jsdelivr.net/gh/gyl-coder/blogImgs/images/touxiang.webp","url":"https://gyl-coder.top"}},{"title":"并发设计模式：Immutability 模式","slug":"design-pattern/immutability","date":"2019-10-25T00:00:00.000Z","updated":"2020-12-27T04:47:15.974Z","comments":true,"path":"design-pattern/immutability/","link":"","permalink":"https://gyl-coder.top/design-pattern/immutability/","excerpt":"多个线程同时读写同一共享变量存在并发问题**，其中的必要条件之一就是 读写 ，如果没有写，只存在读，是不会存在并发问题的。 如果让一个共享变量只有读操作，没有写操作，如此则可以解决并发问题。该理论的具体实现就是 不变性（Immutability）模式 。所谓不变性，简单来讲，就是对象一旦被创建之后，状态就不再发生变化。换句话说，就是变量一旦被赋值，就不允许修改了（没有写操作）；没有修改操作，也就是保持了不变性。","text":"多个线程同时读写同一共享变量存在并发问题**，其中的必要条件之一就是 读写 ，如果没有写，只存在读，是不会存在并发问题的。 如果让一个共享变量只有读操作，没有写操作，如此则可以解决并发问题。该理论的具体实现就是 不变性（Immutability）模式 。所谓不变性，简单来讲，就是对象一旦被创建之后，状态就不再发生变化。换句话说，就是变量一旦被赋值，就不允许修改了（没有写操作）；没有修改操作，也就是保持了不变性。 实现具备不可变性的类将一个类所有的属性都设置成 final 的，并且只允许存在只读方法，那么这个类基本上就具备不可变性了。更严格的做法是这个类本身也是 final 的，也就是不允许继承。因为子类可以覆盖父类的方法，有可能改变不可变性。 Java SDK 里很多类都具备不可变性，只是由于它们的使用太简单，最后反而被忽略了。例如经常用到的 String 和 Long、Integer、Double 等基础类型的包装类都具备不可变性，这些对象的线程安全性都是靠不可变性来保证的。如果你仔细翻看这些类的声明、属性和方法，你会发现它们都严格遵守不可变类的三点要求：类和属性都是 final 的，所有方法均是只读的。 看到这里你可能会疑惑，Java 的 String 方法也有类似字符替换操作，怎么能说所有方法都是只读的呢？下面通过 String 的源代码来看一哈。 下面的示例代码源自 Java 1.8 SDK。String 这个类以及它的属性 value[] 都是 final 的；而 replace() 方法的实现，就的确没有修改 value[]，而是将替换后的字符串作为返回值返回了。 public final class String &#123; private final char value[]; // 字符替换 String replace(char oldChar, char newChar) &#123; // 无需替换，直接返回 this if (oldChar == newChar)&#123; return this; &#125; int len = value.length; int i = -1; /* avoid getfield opcode */ char[] val = value; // 定位到需要替换的字符位置 while (++i &lt; len) &#123; if (val[i] == oldChar) &#123; break; &#125; &#125; // 未找到 oldChar，无需替换 if (i &gt;= len) &#123; return this; &#125; // 创建一个 buf[]，这是关键 // 用来保存替换后的字符串 char buf[] = new char[len]; for (int j = 0; j &lt; i; j++) &#123; buf[j] = val[j]; &#125; while (i &lt; len) &#123; char c = val[i]; buf[i] = (c == oldChar) ? newChar : c; i++; &#125; // 创建一个新的字符串返回 // 原字符串不会发生任何变化 return new String(buf, true); &#125; &#125; 由上面的代码可以发现，String 是通过创建一个新的不可变对象 来实现 修改 的功能。如果 所有的修改操作都创建一个新的不可变对象，你可能会有这种担心：是不是创建的对象太多了，有点太浪费内存呢？是的，这样做的确有些浪费，那如何解决呢？ 利用享元模式避免创建重复对象利用享元模式可以减少创建对象的数量，从而减少内存占用。Java 语言里面 Long、Integer、Short、Byte 等这些基本数据类型的包装类都用到了享元模式。 下面以 Long 这个类作为例子，看看它是如何利用享元模式来优化对象的创建的。 享元模式本质上其实就是一个对象池，利用享元模式创建对象的逻辑也很简单：创建之前，首先去对象池里看看是不是存在；如果已经存在，就利用对象池里的对象；如果不存在，就会新创建一个对象，并且把这个新创建出来的对象放进对象池里。 Long 这个类并没有照搬享元模式，Long 内部维护了一个静态的对象池，仅缓存了 [-128,127] 之间的数字，这个对象池在 JVM 启动的时候就创建好了，而且这个对象池一直都不会变化，也就是说它是静态的。之所以采用这样的设计，是因为 Long 这个对象的状态共有 2 的 64 次方 种，实在太多，并不适合全部缓存，而 [-128,127] 之间的数字利用率最高。下面的示例代码出自 Java 1.8，valueOf() 方法就用到了 LongCache 这个缓存。 Long valueOf(long l) &#123; final int offset = 128; // [-128,127] 直接的数字做了缓存 if (l &gt;= -128 &amp;&amp; l &lt;= 127) &#123; return LongCache .cache[(int)l + offset]; &#125; return new Long(l); &#125; // 缓存，等价于对象池 // 仅缓存 [-128,127] 直接的数字 static class LongCache &#123; static final Long cache[] = new Long[-(-128) + 127 + 1]; static &#123; for(int i=0; i&lt;cache.length; i++) cache[i] = new Long(i-128); &#125; &#125; 注意：** “Integer 和 String 类型的对象不适合做锁”，其实基本上所有的基础类型的包装类都不适合做锁，因为它们内部用到了享元模式，这会导致看上去私有的锁，其实是共有的。例如在下面代码中，本意是 A 用锁 al，B 用锁 bl，各自管理各自的，互不影响。但实际上 al 和 bl 是一个对象，结果 A 和 B 共用的是一把锁。 class A &#123; Long al=Long.valueOf(1); public void setAX()&#123; synchronized (al) &#123; // 省略代码无数 &#125; &#125; &#125; class B &#123; Long bl=Long.valueOf(1); public void setBY()&#123; synchronized (bl) &#123; // 省略代码无数 &#125; &#125; &#125; 使用 Immutability 模式的注意事项在使用 Immutability 模式的时候，需要注意以下两点： 对象的所有属性都是 final 的，并不能保证不可变性； 不可变对象也需要正确发布。 在 Java 语言中，final 修饰的属性一旦被赋值，就不可以再修改，但是如果属性的类型是普通对象，那么这个普通对象的属性是可以被修改的。例如下面的代码中，Bar 的属性 foo 虽然是 final 的，依然可以通过 setAge() 方法来设置 foo 的属性 age。所以，在使用 Immutability 模式的时候一定要确认保持不变性的边界在哪里，是否要求属性对象也具备不可变性。 class Foo&#123; int age=0; int ; &#125; final class Bar &#123; final Foo foo; void setAge(int a)&#123; foo.age=a; &#125; &#125; 下面我们再看看如何正确地发布不可变对象。不可变对象虽然是线程安全的，但是并不意味着引用这些不可变对象的对象就是线程安全的。例如在下面的代码中，Foo 具备不可变性，线程安全，但是类 Bar 并不是线程安全的，类 Bar 中持有对 Foo 的引用 foo，对 foo 这个引用的修改在多线程中并不能保证可见性和原子性。 //Foo 线程安全 final class Foo&#123; final int age=0; final int ; &#125; //Bar 线程不安全 class Bar &#123; Foo foo; void setFoo(Foo f)&#123; this.foo=f; &#125; &#125; 如果你的程序仅仅需要 foo 保持可见性，无需保证原子性，那么可以将 foo 声明为 volatile 变量，这样就能保证可见性。如果你的程序需要保证原子性，那么可以通过原子类来实现。下面的示例代码是合理库存的原子化实现，你应该很熟悉了，其中就是用原子类解决了不可变对象引用的原子性问题。 public class SafeWM &#123; class WMRange&#123; final int upper; final int lower; WMRange(int upper,int lower)&#123; // 省略构造函数实现 &#125; &#125; final AtomicReference&lt;WMRange&gt; rf = new AtomicReference&lt;&gt;( new WMRange(0,0) ); // 设置库存上限 void setUpper(int v)&#123; while(true)&#123; WMRange or = rf.get(); // 检查参数合法性 if(v &lt; or.lower)&#123; throw new IllegalArgumentException(); &#125; WMRange nr = new WMRange(v, or.lower); if(rf.compareAndSet(or, nr))&#123; return; &#125; &#125; &#125; &#125; 总结具备不变性的对象，只有一种状态，这个状态由对象内部所有的不变属性共同决定。其实还有一种更简单的不变性对象，那就是无状态。无状态对象内部没有属性，只有方法。除了无状态的对象，你可能还听说过无状态的服务、无状态的协议等等。无状态有很多好处，最核心的一点就是性能。在多线程领域，无状态对象没有线程安全问题，无需同步处理，自然性能很好；在分布式领域，无状态意味着可以无限地水平扩展，所以分布式领域里面性能的瓶颈一定不是出在无状态的服务节点上。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://gyl-coder.top/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"并发","slug":"并发","permalink":"https://gyl-coder.top/tags/%E5%B9%B6%E5%8F%91/"},{"name":"设计模式","slug":"设计模式","permalink":"https://gyl-coder.top/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"Immutability模式","slug":"Immutability模式","permalink":"https://gyl-coder.top/tags/Immutability%E6%A8%A1%E5%BC%8F/"}],"author":{"name":"yanliang","avatar":"https://cdn.jsdelivr.net/gh/gyl-coder/blogImgs/images/touxiang.webp","url":"https://gyl-coder.top"}},{"title":"单例模式","slug":"design-pattern/single-pattern","date":"2019-10-24T00:00:00.000Z","updated":"2020-12-27T04:47:15.974Z","comments":true,"path":"design-pattern/single-pattern/","link":"","permalink":"https://gyl-coder.top/design-pattern/single-pattern/","excerpt":"在介绍单例模式之前，我们先了解一下，什么是设计模式？设计模式（Design Pattern）：是一套被反复使用，多数人知晓的，经过分类编目的，代码设计经验的总结。目的：使用设计模式是为了可重用性代码，让代码更容易被他人理解，保证代码可靠性。 本文将会用到的关键词： 单例：Singleton 实例：instance 同步：synchronized 类装载器：ClassLoader 单例模式：单例，顾名思义就是只能有一个、不能再出现第二个。就如同地球上没有两片一模一样的树叶一样。 在这里就是说：一个类只能有一个实例，并且整个项目系统都能访问该实例。 单例模式共分为两大类： 懒汉模式：实例在第一次使用时创建 饿汉模式：实例在类装载时创建","text":"在介绍单例模式之前，我们先了解一下，什么是设计模式？设计模式（Design Pattern）：是一套被反复使用，多数人知晓的，经过分类编目的，代码设计经验的总结。目的：使用设计模式是为了可重用性代码，让代码更容易被他人理解，保证代码可靠性。 本文将会用到的关键词： 单例：Singleton 实例：instance 同步：synchronized 类装载器：ClassLoader 单例模式：单例，顾名思义就是只能有一个、不能再出现第二个。就如同地球上没有两片一模一样的树叶一样。 在这里就是说：一个类只能有一个实例，并且整个项目系统都能访问该实例。 单例模式共分为两大类： 懒汉模式：实例在第一次使用时创建 饿汉模式：实例在类装载时创建 单例模式UML图 饿汉模式按照定义我们可以写出一个基本代码： public class Singleton &#123; // 使用private将构造方法私有化，以防外界通过该构造方法创建多个实例 private Singleton() &#123; &#125; // 由于不能使用构造方法创建实例，所以需要在类的内部创建该类的唯一实例 // 使用static修饰singleton 在外界可以通过类名调用该实例 类名.成员名 static Singleton singleton = new Singleton(); // 1 // 如果使用private封装该实例，则需要添加get方法实现对外界的开放 private static Singleton instance = new Singleton(); // 2 // 添加static，将该方法变成类所有 通过类名访问 public static Singleton getInstance()&#123; return instance; &#125; //1和2选一种即可，推荐2 &#125; 对于饿汉模式来说，这种写法已经很‘perfect’了，唯一的缺点就是，由于instance的初始化是在类加载时进行的，类加载是由ClassLoader来实现的，如果初始化太早，就会造成资源浪费。当然，如果所需的单例占用的资源很少，并且也不依赖于其他数据，那么这种实现方式也是很好的。 类装载的时机： new一个对象时 使用反射创建它的实例时 子类被加载时，如果父类还没有加载，就先加载父类 JVM启动时执行主类 会先被加载 懒汉模式懒汉模式的代码如下 // 代码一 public class Singleton &#123; private static Singleton instance = null; private Singleton()&#123; &#125; public static Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125; &#125; 每次获取instance之前先进行判断，如果instance为空就new一个出来，否则就直接返回已存在的instance。 这种写法在单线程的时候是没问题的。但是，当有多个线程一起工作的时候，如果有两个线程同时运行到 if (instance == null)，都判断为null（第一个线程判断为空之后，并没有继续向下执行，当第二个线程判断的时候instance依然为空），最终两个线程就各自会创建一个实例出来。这样就破环了单例模式 实例的唯一性 要想保证实例的唯一性就需要使用synchronized，加上一个同步锁 // 代码二 public class Singleton &#123; private static Singleton instance = null; private Singleton() &#123;&#125; public static Singleton getInstance() &#123; synchronized(Singleton.class)&#123; if (instance == null) instance = new Singleton(); &#125; return instance; &#125; &#125; 加上synchronized关键字之后，getInstance方法就会锁上了。如果有两个线程（T1、T2）同时执行到这个方法时，会有其中一个线程T1获得同步锁，得以继续执行，而另一个线程T2则需要等待，当第T1执行完毕getInstance之后（完成了null判断、对象创建、获得返回值之后），T2线程才会执行执行。 所以这段代码也就避免了代码一中，可能出现因为多线程导致多个实例的情况。但是，这种写法也有一个问题：给getInstance方法加锁，虽然避免了可能会出现的多个实例问题，但是会强制除T1之外的所有线程等待，实际上会对程序的执行效率造成负面影响。 双重检查（Double-Check）代码二相对于代码一的效率问题，其实是为了解决1%几率的问题，而使用了一个100%出现的防护盾。那有一个优化的思路，就是把100%出现的防护盾，也改为1%的几率出现，使之只出现在可能会导致多个实例出现的地方。代码如下： // 代码三 public class Singleton &#123; private static Singleton instance = null; private Singleton() &#123;&#125; public static Singleton getInstance() &#123; if (instance == null)&#123; synchronized(Singleton.class)&#123; if (instance == null) instance = new Singleton(); &#125; &#125; return instance; &#125; &#125; 这段代码看起来有点复杂，注意其中有两次if(instance==null)的判断，这个叫做『双重检查 Double-Check』。 第一个 if(instance==null)，其实是为了解决代码二中的效率问题，只有instance为null的时候，才进入synchronized的代码段大大减少了几率。 第二个if(instance==null)，则是跟代码二一样，是为了防止可能出现多个实例的情况。 这段代码看起来已经完美无瑕了。当然，只是『看起来』，还是有小概率出现问题的。想要充分理解需要先弄清楚以下几个概念：原子操作、指令重排。 原子操作简单来说，原子操作（atomic）就是不可分割的操作，在计算机中，就是指不会因为线程调度被打断的操作。比如，简单的赋值是一个原子操作： m = 6; // 这是个原子操作 假如m原先的值为0，那么对于这个操作，要么执行成功m变成了6，要么是没执行 m还是0，而不会出现诸如m=3这种中间态——即使是在并发的线程中。 但是，声明并赋值就不是一个原子操作： int n=6;//这不是一个原子操作 对于这个语句，至少有两个操作：①声明一个变量n ②给n赋值为6——这样就会有一个中间状态：变量n已经被声明了但是还没有被赋值的状态。这样，在多线程中，由于线程执行顺序的不确定性，如果两个线程都使用m，就可能会导致不稳定的结果出现。 指令重排简单来说，就是计算机为了提高执行效率，会做的一些优化，在不影响最终结果的情况下，可能会对一些语句的执行顺序进行调整。比如，这一段代码： int a ; // 语句1 a = 8 ; // 语句2 int b = 9 ; // 语句3 int c = a + b ; // 语句4 正常来说，对于顺序结构，执行的顺序是自上到下，也即1234。但是，由于指令重排的原因，因为不影响最终的结果，所以，实际执行的顺序可能会变成3124或者1324。 由于语句3和4没有原子性的问题，语句3和语句4也可能会拆分成原子操作，再重排。——也就是说，对于非原子性的操作，在不影响最终结果的情况下，其拆分成的原子操作可能会被重新排列执行顺序。 OK，了解了原子操作和指令重排的概念之后，我们再继续看代码三的问题。 主要在于singleton = new Singleton()这句，这并非是一个原子操作，事实上在 JVM 中这句话大概做了下面 3 件事情。 1. 给 singleton 分配内存 2. 调用 Singleton 的构造函数来初始化成员变量，形成实例 3. 将singleton对象指向分配的内存空间（执行完这步 singleton才是非 null了） 在JVM的即时编译器中存在指令重排序的优化。 也就是说上面的第二步和第三步的顺序是不能保证的，最终的执行顺序可能是 1-2-3 也可能是 1-3-2。如果是后者，则在 3 执行完毕、2 未执行之前，被线程二抢占了，这时 instance 已经是非 null 了（但却没有初始化），所以线程二会直接返回 instance，然后使用，然后顺理成章地报错。 再稍微解释一下，就是说，由于有一个『instance已经不为null但是仍没有完成初始化』的中间状态，而这个时候，如果有其他线程刚好运行到第一层if (instance ==null)这里，这里读取到的instance已经不为null了，所以就直接把这个中间状态的instance拿去用了，就会产生问题。这里的关键在于线程T1对instance的写操作没有完成，线程T2就执行了读操作。 对于代码三出现的问题，解决方案为：给instance的声明加上volatile关键字代码如下： public class Singleton &#123; private static volatile Singleton instance = null; private Singleton() &#123;&#125; public static Singleton getInstance() &#123; if (instance == null)&#123; synchronized(Singleton.class)&#123; if (instance == null) instance = new Singleton(); &#125; &#125; return instance; &#125; &#125; volatile关键字的一个作用是禁止指令重排，把instance声明为volatile之后，对它的写操作就会有一个内存屏障，这样，在它的赋值完成之前，就不用会调用读操作。 注意：volatile阻止的不是singleton = new Singleton()这句话内部[1-2-3]的指令重排，而是保证了在一个写操作（[1-2-3]）完成之前，不会调用读操作（if (instance == null)）。 其它方法静态内部类public class Singleton &#123; private static class SingletonHolder &#123; private static final Singleton INSTANCE = new Singleton(); &#125; private Singleton ()&#123;&#125; public static final Singleton getInstance() &#123; return SingletonHolder.INSTANCE; &#125; &#125; 这种写法的巧妙之处在于：对于内部类SingletonHolder，它是一个饿汉式的单例实现，在SingletonHolder初始化的时候会由ClassLoader来保证同步，使INSTANCE是一个真单例。 同时，由于SingletonHolder是一个内部类，只在外部类的Singleton的getInstance()中被使用，所以它被加载的时机也就是在getInstance()方法第一次被调用的时候。 它利用了ClassLoader来保证了同步，同时又能让开发者控制类加载的时机。从内部看是一个饿汉式的单例，但是从外部看来，又的确是懒汉式的实现 枚举public enum SingleInstance &#123; INSTANCE; public void fun1() &#123; // do something &#125; &#125;// 使用SingleInstance.INSTANCE.fun1(); 是不是很简单？而且因为自动序列化机制，保证了线程的绝对安全。三个词概括该方式：简单、高效、安全 这种写法在功能上与共有域方法相近，但是它更简洁，无偿地提供了序列化机制，绝对防止对此实例化，即使是在面对复杂的序列化或者反射攻击的时候。虽然这中方法还没有广泛采用，但是单元素的枚举类型已经成为实现Singleton的最佳方法。 视频讲解https://pan.baidu.com/s/1pWotEu9Z883hqWUM_Xu7vQ 密码： lt0t","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://gyl-coder.top/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://gyl-coder.top/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"单例模式","slug":"单例模式","permalink":"https://gyl-coder.top/tags/%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/"}],"author":{"name":"yanliang","avatar":"https://cdn.jsdelivr.net/gh/gyl-coder/blogImgs/images/touxiang.webp","url":"https://gyl-coder.top"}},{"title":"撸一个JSON解析器","slug":"JSONParser","date":"2019-10-24T00:00:00.000Z","updated":"2020-12-27T04:47:15.974Z","comments":true,"path":"JSONParser/","link":"","permalink":"https://gyl-coder.top/JSONParser/","excerpt":"JSON(JavaScript Object Notation, JS 对象简谱) 是一种轻量级的数据交换格式。易于人阅读和编写。同时也易于机器解析和生成。采用完全独立于语言的文本格式，但是也使用了类似于 C 语言家族的习惯（包括 C, C++, C#, Java, JavaScript, Perl, Python 等）。这些特性使 JSON 成为理想的数据交换语言。 JSON 与 JS 的区别以及和 XML 的区别具体请参考百度百科","text":"JSON(JavaScript Object Notation, JS 对象简谱) 是一种轻量级的数据交换格式。易于人阅读和编写。同时也易于机器解析和生成。采用完全独立于语言的文本格式，但是也使用了类似于 C 语言家族的习惯（包括 C, C++, C#, Java, JavaScript, Perl, Python 等）。这些特性使 JSON 成为理想的数据交换语言。 JSON 与 JS 的区别以及和 XML 的区别具体请参考百度百科 JSON 有两种结构： 第一种：对象 “名称 / 值” 对的集合不同的语言中，它被理解为对象（object），纪录（record），结构（struct），字典（dictionary），哈希表（hash table），有键列表（keyed list），或者关联数组 （associative array）。 对象是一个无序的 “‘名称 / 值’对” 集合。一个对象以 “{”（左括号）开始，“}”（右括号）结束。每个“名称” 后跟一个 “:”（冒号）；“‘名称 / 值’ 对” 之间使用“,”（逗号）分隔。 &#123;&quot;姓名&quot;: &quot;张三&quot;, &quot;年龄&quot;: &quot;18&quot;&#125; 第二种：数组 值的有序列表（An ordered list of values）。在大部分语言中，它被理解为数组（array）。 数组是值（value）的有序集合。一个数组以 “[”（左中括号）开始，“]”（右中括号）结束。值之间使用 “,”（逗号）分隔。 值（value）可以是双引号括起来的字符串（string）、数值 (number)、true、false、 null、对象（object）或者数组（array）。这些结构可以嵌套。 [ &#123; &quot;姓名&quot;: &quot;张三&quot;, &quot;年龄&quot;:&quot;18&quot; &#125;, &#123; &quot;姓名&quot;: &quot;里斯&quot;, &quot;年龄&quot;:&quot;19&quot; &#125; ] 通过上面的了解可以看出，JSON 存在以下几种数据类型（以 Java 做类比）： json java string Java 中的 String number Java 中的 Long 或 Double true/false Java 中的 Boolean null Java 中的 null [array] Java 中的 List 或 Object[] {“key”:”value”} Java 中的 Map&lt;String, Object&gt; 解析 JSONJSON 解析器的基本原理输入一串 JSON 字符串，输出一个 JSON 对象。 步骤JSON 解析的过程主要分以下两步： 第一步：对于输入的一串 JSON 字符串我们需要将其解析成一组 token 流。 例如 JSON 字符串 {“姓名”: “张三”, “年龄”: “18”} 我们需要将它解析成 &#123;、 姓名、 :、 张三、 ,、 年龄、 :、 18、 &#125; 这样一组 token 流 第二步：根据得到的 token 流将其解析成对应的 JSON 对象（JSONObject）或者 JSON 数组（JSONArray） 下面我们来详细分析下这两个步骤： 获取 token 流根据 JSON 格式的定义，token 可以分为以下几种类型 token 含义 NULL null NUMBER 数字 STRING 字符串 BOOLEAN true/false SEP_COLON : SEP_COMMA , BEGIN_OBJECT { END_OBJECT } BEGIN_ARRAY [ END_ARRAY ] END_DOCUMENT 表示 JSON 数据结束 根据以上的 JSON 类型，我们可以将其封装成 enum 类型的 TokenType package com.json.demo.tokenizer; /** BEGIN_OBJECT（&#123;） END_OBJECT（&#125;） BEGIN_ARRAY（[） END_ARRAY（]） NULL（null） NUMBER（数字） STRING（字符串） BOOLEAN（true/false） SEP_COLON（:） SEP_COMMA（,） END_DOCUMENT（表示JSON文档结束） */ public enum TokenType &#123; BEGIN_OBJECT(1), END_OBJECT(2), BEGIN_ARRAY(4), END_ARRAY(8), NULL(16), NUMBER(32), STRING(64), BOOLEAN(128), SEP_COLON(256), SEP_COMMA(512), END_DOCUMENT(1024); private int code; // 每个类型的编号 TokenType(int code) &#123; this.code = code; &#125; public int getTokenCode() &#123; return code; &#125; &#125; 在 TokenType 中我们为每一种类型都赋一个数字，目的是在 Parser 做一些优化操作（通过位运算来判断是否是期望出现的类型） 在进行第一步之前 JSON 串对计算机来说只是一串没有意义的字符而已。第一步的作用就是把这些无意义的字符串变成一个一个的 token，上面我们已经为每一种 token 定义了相应的类型和值。所以计算机能够区分不同的 token，并能以 token 为单位解读 JSON 数据。 下面我们封装一个 token 类来存储每一个 token 对应的值 package com.json.demo.tokenizer; /** * 存储对应类型的字面量 */ public class Token &#123; private TokenType tokenType; private String value; public Token(TokenType tokenType, String value) &#123; this.tokenType = tokenType; this.value = value; &#125; public TokenType getTokenType() &#123; return tokenType; &#125; public void setTokenType(TokenType tokenType) &#123; this.tokenType = tokenType; &#125; public String getValue() &#123; return value; &#125; public void setValue(String value) &#123; this.value = value; &#125; @Override public String toString() &#123; return &quot;Token&#123;&quot; + &quot;tokenType=&quot; + tokenType + &quot;, value=&#39;&quot; + value + &#39;\\&#39;&#39; + &#39;&#125;&#39;; &#125; &#125; 在解析的过程中我们通过字符流来不断的读取字符，并且需要经常根据相应的字符来判断状态的跳转。所以我们需要自己封装一个 ReaderChar 类，以便我们更好的操作字符流。 package com.json.demo.tokenizer; import java.io.IOException; import java.io.Reader; public class ReaderChar &#123; private static final int BUFFER_SIZE = 1024; private Reader reader; private char[] buffer; private int index; // 下标 private int size; public ReaderChar(Reader reader) &#123; this.reader = reader; buffer = new char[BUFFER_SIZE]; &#125; /** * 返回 pos 下标处的字符，并返回 * @return */ public char peek() &#123; if (index - 1 &gt;= size) &#123; return (char) -1; &#125; return buffer[Math.max(0, index - 1)]; &#125; /** * 返回 pos 下标处的字符，并将 pos + 1，最后返回字符 * @return * @throws IOException */ public char next() throws IOException &#123; if (!hasMore()) &#123; return (char) -1; &#125; return buffer[index++]; &#125; /** * 下标回退 */ public void back() &#123; index = Math.max(0, --index); &#125; /** * 判断流是否结束 */ public boolean hasMore() throws IOException &#123; if (index &lt; size) &#123; return true; &#125; fillBuffer(); return index &lt; size; &#125; /** * 填充buffer数组 * @throws IOException */ void fillBuffer() throws IOException &#123; int n = reader.read(buffer); if (n == -1) &#123; return; &#125; index = 0; size = n; &#125; &#125; 另外我们还需要一个 TokenList 来存储解析出来的 token 流 package com.json.demo.tokenizer; import java.util.ArrayList; import java.util.List; /** * 存储词法解析所得的token流 */ public class TokenList &#123; private List&lt;Token&gt; tokens = new ArrayList&lt;Token&gt;(); private int index = 0; public void add(Token token) &#123; tokens.add(token); &#125; public Token peek() &#123; return index &lt; tokens.size() ? tokens.get(index) : null; &#125; public Token peekPrevious() &#123; return index - 1 &lt; 0 ? null : tokens.get(index - 2); &#125; public Token next() &#123; return tokens.get(index++); &#125; public boolean hasMore() &#123; return index &lt; tokens.size(); &#125; @Override public String toString() &#123; return &quot;TokenList&#123;&quot; + &quot;tokens=&quot; + tokens + &#39;&#125;&#39;; &#125; &#125; JSON 解析比其他文本解析要简单的地方在于，我们只需要根据下一个字符就可知道接下来它所期望读取的到的内容是什么样的。如果满足期望了，则返回 Token，否则返回错误。 为了方便程序出错时更好的 debug，程序中自定义了两个 exception 类来处理错误信息。（具体实现参考 exception 包） 下面就是第一步中的重头戏（核心代码）： public TokenList getTokenStream(ReaderChar readerChar) throws IOException &#123; this.readerChar = readerChar; tokenList = new TokenList(); // 词法解析，获取token流 tokenizer(); return tokenList; &#125; /** * 将JSON文件解析成token流 * @throws IOException */ private void tokenizer() throws IOException &#123; Token token; do &#123; token = start(); tokenList.add(token); &#125; while (token.getTokenType() != TokenType.END_DOCUMENT); &#125; /** * 解析过程的具体实现方法 * @return * @throws IOException * @throws JsonParseException */ private Token start() throws IOException, JsonParseException &#123; char ch; while (true)&#123; //先读一个字符，若为空白符（ASCII码在[0, 20H]上）则接着读，直到刚读的字符非空白符 if (!readerChar.hasMore()) &#123; return new Token(TokenType.END_DOCUMENT, null); &#125; ch = readerChar.next(); if (!isWhiteSpace(ch)) &#123; break; &#125; &#125; switch (ch) &#123; case &#39;&#123;&#39;: return new Token(TokenType.BEGIN_OBJECT, String.valueOf(ch)); case &#39;&#125;&#39;: return new Token(TokenType.END_OBJECT, String.valueOf(ch)); case &#39;[&#39;: return new Token(TokenType.BEGIN_ARRAY, String.valueOf(ch)); case &#39;]&#39;: return new Token(TokenType.END_ARRAY, String.valueOf(ch)); case &#39;,&#39;: return new Token(TokenType.SEP_COMMA, String.valueOf(ch)); case &#39;:&#39;: return new Token(TokenType.SEP_COLON, String.valueOf(ch)); case &#39;n&#39;: return readNull(); case &#39;t&#39;: case &#39;f&#39;: return readBoolean(); case &#39;&quot;&#39;: return readString(); case &#39;-&#39;: return readNumber(); &#125; if (isDigit(ch)) &#123; return readNumber(); &#125; throw new JsonParseException(&quot;Illegal character&quot;); &#125; 在 start 方法中，我们将每个处理方法都封装成了单独的函数。主要思想就是通过一个死循环不停的读取字符，然后再根据字符的期待值，执行不同的处理函数。 下面我们详解分析几个处理函数： private Token readString() throws IOException &#123; StringBuilder sb = new StringBuilder(); while(true) &#123; char ch = readerChar.next(); if (ch == &#39;\\\\&#39;) &#123; // 处理转义字符 if (!isEscape()) &#123; throw new JsonParseException(&quot;Invalid escape character&quot;); &#125; sb.append(&#39;\\\\&#39;); ch = readerChar.peek(); sb.append(ch); if (ch == &#39;u&#39;) &#123; // 处理 Unicode 编码，形如 \\u4e2d。且只支持 \\u0000 ~ \\uFFFF 范围内的编码 for (int i = 0; i &lt; 4; i++) &#123; ch = readerChar.next(); if (isHex(ch)) &#123; sb.append(ch); &#125; else &#123; throw new JsonParseException(&quot;Invalid character&quot;); &#125; &#125; &#125; &#125; else if (ch == &#39;&quot;&#39;) &#123; // 碰到另一个双引号，则认为字符串解析结束，返回 Token return new Token(TokenType.STRING, sb.toString()); &#125; else if (ch == &#39;\\r&#39; || ch == &#39;\\n&#39;) &#123; // 传入的 JSON 字符串不允许换行 throw new JsonParseException(&quot;Invalid character&quot;); &#125; else &#123; sb.append(ch); &#125; &#125; &#125; 该方法也是通过一个死循环来读取字符，首先判断的是 JSON 中的转义字符。 JSON 中允许出现的有以下几种 \\&quot; \\\\ \\b \\f \\n \\r \\t \\u four-hex-digits \\/ 具体的处理方法封装在了 isEscape() 方法中，处理 Unicode 编码时要特别注意一下 u 的后面会出现四位十六进制数。当读取到一个双引号或者读取到了非法字符（’r’或’、’n’）循环退出。 判断数字的时候也要特别小心，注意负数，frac，exp 等等情况。 通过上面的解析，我们可以得到一组 token，接下来我们需要以这组 token 作为输入，解析出相应的 JSON 对象 解析出 JSON 对象解析之前我们需要定义出 JSON 对象（JSONObject）和 JSON 数组 (JSONArray) 的实体类。 package com.json.demo.jsonstyle; import com.json.demo.exception.JsonTypeException; import com.json.demo.util.FormatUtil; import java.util.ArrayList; import java.util.HashMap; import java.util.List; import java.util.Map; /** * JSON的对象形式 * 对象是一个无序的“‘名称/值’对”集合。一个对象以“&#123;”（左括号）开始，“&#125;”（右括号）结束。每个“名称”后跟一个“:”（冒号）；“‘名称/值’ 对”之间使用“,”（逗号）分隔。 */ public class JsonObject &#123; private Map&lt;String, Object&gt; map = new HashMap&lt;String, Object&gt;(); public void put(String key, Object value) &#123; map.put(key, value); &#125; public Object get(String key) &#123; return map.get(key); &#125; ... &#125; package com.json.demo.jsonstyle; import com.json.demo.exception.JsonTypeException; import com.json.demo.util.FormatUtil; import java.util.ArrayList; import java.util.Iterator; import java.util.List; /** * JSON的数组形式 * 数组是值（value）的有序集合。一个数组以“[”（左中括号）开始，“]”（右中括号）结束。值之间使用“,”（逗号）分隔。 */ public class JsonArray &#123; private List list = new ArrayList(); public void add(Object obj) &#123; list.add(obj); &#125; public Object get(int index) &#123; return list.get(index); &#125; public int size() &#123; return list.size(); &#125; ... &#125; 之后我们就可以写解析类了，由于代码较长，这里就不展示了。有兴趣的可以去 GitHub 上下载。实现逻辑比较简单，也易于理解。 解析类中的 parse 方法首先根据第一个 token 的类型选择调用 parseJsonObject（）或者 parseJsonArray（），进而返回 JSON 对象或者 JSON 数组。上面的解析方法中利用位运算来判断字符的期待值既提高了程序的执行效率也有助于提高代码的 ke’du’xi 完成之后我们可以写一个测试类来验证下我们的解析器的运行情况。我们可以自己定义一组 JSON 串也可以通过 HttpUtil 工具类从网上获取。最后通过 FormatUtil 类来规范我们输出。 具体效果如下图所示： 参考文章 http://www.cnblogs.com/absfre… https://www.liaoxuefeng.com/a… https://segmentfault.com/a/11… http://json.org/json-zh.html","categories":[{"name":"Java","slug":"Java","permalink":"https://gyl-coder.top/categories/Java/"}],"tags":[{"name":"json","slug":"json","permalink":"https://gyl-coder.top/tags/json/"},{"name":"json 解析器","slug":"json-解析器","permalink":"https://gyl-coder.top/tags/json-%E8%A7%A3%E6%9E%90%E5%99%A8/"}],"author":{"name":"yanliang","avatar":"https://cdn.jsdelivr.net/gh/gyl-coder/blogImgs/images/touxiang.webp","url":"https://gyl-coder.github.io"}}],"categories":[{"name":"Java","slug":"Java","permalink":"https://gyl-coder.top/categories/Java/"},{"name":"Spring","slug":"Spring","permalink":"https://gyl-coder.top/categories/Spring/"},{"name":"Mybatis","slug":"Mybatis","permalink":"https://gyl-coder.top/categories/Mybatis/"},{"name":"设计","slug":"设计","permalink":"https://gyl-coder.top/categories/%E8%AE%BE%E8%AE%A1/"},{"name":"kafka","slug":"kafka","permalink":"https://gyl-coder.top/categories/kafka/"},{"name":"并发","slug":"并发","permalink":"https://gyl-coder.top/categories/%E5%B9%B6%E5%8F%91/"},{"name":"设计模式","slug":"设计模式","permalink":"https://gyl-coder.top/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://gyl-coder.top/tags/Java/"},{"name":"接口","slug":"接口","permalink":"https://gyl-coder.top/tags/%E6%8E%A5%E5%8F%A3/"},{"name":"抽象","slug":"抽象","permalink":"https://gyl-coder.top/tags/%E6%8A%BD%E8%B1%A1/"},{"name":"集合","slug":"集合","permalink":"https://gyl-coder.top/tags/%E9%9B%86%E5%90%88/"},{"name":"LinkedHashMap","slug":"LinkedHashMap","permalink":"https://gyl-coder.top/tags/LinkedHashMap/"},{"name":"LinkedList","slug":"LinkedList","permalink":"https://gyl-coder.top/tags/LinkedList/"},{"name":"Vector","slug":"Vector","permalink":"https://gyl-coder.top/tags/Vector/"},{"name":"ArrayList","slug":"ArrayList","permalink":"https://gyl-coder.top/tags/ArrayList/"},{"name":"动态代理","slug":"动态代理","permalink":"https://gyl-coder.top/tags/%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86/"},{"name":"Spring","slug":"Spring","permalink":"https://gyl-coder.top/tags/Spring/"},{"name":"IOC","slug":"IOC","permalink":"https://gyl-coder.top/tags/IOC/"},{"name":"AOP","slug":"AOP","permalink":"https://gyl-coder.top/tags/AOP/"},{"name":"try-catch","slug":"try-catch","permalink":"https://gyl-coder.top/tags/try-catch/"},{"name":"Settings","slug":"Settings","permalink":"https://gyl-coder.top/tags/Settings/"},{"name":"Mybatis","slug":"Mybatis","permalink":"https://gyl-coder.top/tags/Mybatis/"},{"name":"HashMap","slug":"HashMap","permalink":"https://gyl-coder.top/tags/HashMap/"},{"name":"Int","slug":"Int","permalink":"https://gyl-coder.top/tags/Int/"},{"name":"Integer","slug":"Integer","permalink":"https://gyl-coder.top/tags/Integer/"},{"name":"限流算法","slug":"限流算法","permalink":"https://gyl-coder.top/tags/%E9%99%90%E6%B5%81%E7%AE%97%E6%B3%95/"},{"name":"设计","slug":"设计","permalink":"https://gyl-coder.top/tags/%E8%AE%BE%E8%AE%A1/"},{"name":"kafka","slug":"kafka","permalink":"https://gyl-coder.top/tags/kafka/"},{"name":"topic","slug":"topic","permalink":"https://gyl-coder.top/tags/topic/"},{"name":"可靠性","slug":"可靠性","permalink":"https://gyl-coder.top/tags/%E5%8F%AF%E9%9D%A0%E6%80%A7/"},{"name":"一致性","slug":"一致性","permalink":"https://gyl-coder.top/tags/%E4%B8%80%E8%87%B4%E6%80%A7/"},{"name":"线程阻塞","slug":"线程阻塞","permalink":"https://gyl-coder.top/tags/%E7%BA%BF%E7%A8%8B%E9%98%BB%E5%A1%9E/"},{"name":"并发","slug":"并发","permalink":"https://gyl-coder.top/tags/%E5%B9%B6%E5%8F%91/"},{"name":"等待通知机制","slug":"等待通知机制","permalink":"https://gyl-coder.top/tags/%E7%AD%89%E5%BE%85%E9%80%9A%E7%9F%A5%E6%9C%BA%E5%88%B6/"},{"name":"Concurrency","slug":"Concurrency","permalink":"https://gyl-coder.top/tags/Concurrency/"},{"name":"设计模式","slug":"设计模式","permalink":"https://gyl-coder.top/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"Immutability模式","slug":"Immutability模式","permalink":"https://gyl-coder.top/tags/Immutability%E6%A8%A1%E5%BC%8F/"},{"name":"单例模式","slug":"单例模式","permalink":"https://gyl-coder.top/tags/%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/"},{"name":"json","slug":"json","permalink":"https://gyl-coder.top/tags/json/"},{"name":"json 解析器","slug":"json-解析器","permalink":"https://gyl-coder.top/tags/json-%E8%A7%A3%E6%9E%90%E5%99%A8/"}]}